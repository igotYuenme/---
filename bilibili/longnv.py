# ======================================
# Bç«™UPä¸»ä¸‰ç»´è¯„ä¼°ï¼šæ„å»º"å†…å®¹â€”ä¼ æ’­â€”å¿ƒç†"ä¸‰ç»´è¯„ä¼°æ¡†æ¶
# åˆ†æå¯¹è±¡ï¼šé¾™å¥³å¡”ç½—
# ======================================

import json
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# ======================================
# è¾…åŠ©å‡½æ•°
# ======================================

def calculate_gini(x):
    """è®¡ç®—åŸºå°¼ç³»æ•°"""
    x = np.sort(x)
    n = len(x)
    if n == 0:
        return 0
    cumx = np.cumsum(x, dtype=float)
    return (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n if cumx[-1] > 0 else 0

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if not isinstance(text, str):
        return ""
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'@.*?\s', '', text)
    text = re.sub(r'#.*?#', '', text)
    return text.strip()

def parse_time(time_str):
    """è§£ææ—¶é—´å­—ç¬¦ä¸²ï¼ˆæ”¯æŒUnixæ—¶é—´æˆ³å’Œæ ‡å‡†æ ¼å¼ï¼‰"""
    try:
        if pd.isna(time_str):
            return None
        
        # å¦‚æœæ˜¯Unixæ—¶é—´æˆ³ï¼ˆæ•´æ•°ï¼‰
        if isinstance(time_str, (int, float, str)):
            try:
                timestamp = float(time_str)
                if timestamp > 1000000000:  # å¤§äº2001å¹´ï¼Œè®¤ä¸ºæ˜¯æ—¶é—´æˆ³
                    return datetime.fromtimestamp(timestamp)
            except (ValueError, OSError):
                pass
            
            if isinstance(time_str, str):
                # æ ¼å¼1: "Sun Nov 16 21:03:35 +0800 2025"
                if ' +' in time_str:
                    time_str = time_str.split(' +')[0]
                    try:
                        dt = datetime.strptime(time_str, "%a %b %d %H:%M:%S %Y")
                        return dt
                    except:
                        pass
                
                # æ ¼å¼2: "2025-11-16 21:03:35"
                try:
                    dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
                    return dt
                except:
                    pass
                
                # æ ¼å¼3: "2025/11/16 21:03:35"
                try:
                    dt = datetime.strptime(time_str, "%Y/%m/%d %H:%M:%S")
                    return dt
                except:
                    pass
        
        return None
    except Exception as e:
        return None

# ======================================
# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ======================================
def load_up_data(csv_path="bilibili_videos.csv", up_name="é¾™å¥³å¡”ç½—"):
    """åŠ è½½UPä¸»ç›¸å…³æ•°æ®ï¼ˆBç«™CSVæ ¼å¼ï¼‰"""
    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_path, encoding='utf-8')
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯UPä¸»ä¸“é—¨æ–‡ä»¶ï¼ˆé€šè¿‡æ–‡ä»¶åæˆ–keywordå­—æ®µåˆ¤æ–­ï¼‰
        is_up_specific_file = (up_name in csv_path) or \
                             ('keyword' in df.columns and df['keyword'].str.contains(f'UPä¸»:{up_name}', na=False).any())
        
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼Œæ€»æ ·æœ¬æ•°: {len(df)}")
        print(f"  åˆ—å: {df.columns.tolist()}")
        if is_up_specific_file:
            print(f"  ğŸ“Œ è¯†åˆ«ä¸ºUPä¸»ä¸“é—¨æ–‡ä»¶ï¼Œæ•°æ®ä¸»è¦ä¸ºUPä¸» '{up_name}' çš„è§†é¢‘")
        
        # å­—æ®µæ˜ å°„ï¼šBç«™CSV -> ç»Ÿä¸€æ ¼å¼
        # title -> text
        if 'title' in df.columns:
            df['text'] = df['title']
        # up -> user
        if 'up' in df.columns:
            df['user'] = df['up']
        # play -> views (ç±»ä¼¼likes)
        if 'play' in df.columns:
            df['attitudes_count'] = pd.to_numeric(df['play'], errors='coerce').fillna(0)
        # danmu -> comments
        if 'danmu' in df.columns:
            df['comments_count'] = pd.to_numeric(df['danmu'], errors='coerce').fillna(0)
        # è½¬å‘æ•°åœ¨Bç«™é€šå¸¸ä¸º0æˆ–ä¸å­˜åœ¨ï¼Œè®¾ä¸º0
        df['reposts_count'] = 0
        
        # pubdate -> created_at (è½¬æ¢æ—¶é—´æˆ³)
        if 'pubdate' in df.columns:
            df['created_at'] = df['pubdate'].apply(parse_time)
        else:
            df['created_at'] = None
        
        # æ£€æŸ¥äº’åŠ¨æ•°æ®
        total_views = df['attitudes_count'].sum() if 'attitudes_count' in df.columns else 0
        total_comments = df['comments_count'].sum() if 'comments_count' in df.columns else 0
        
        print(f"  æ€»äº’åŠ¨æ•°æ®: æ’­æ”¾{total_views:.0f}, è¯„è®º{total_comments:.0f}")
        
        # æ£€æŸ¥ç”¨æˆ·åˆ†å¸ƒ
        if 'user' in df.columns:
            user_counts = df['user'].value_counts()
            print(f"  UPä¸»æ•°: {len(user_counts)}")
            print(f"  æ´»è·ƒUPä¸»å‰5: {dict(user_counts.head(5))}")
        
        # 1. ä¼˜å…ˆä½¿ç”¨UPä¸»æœ¬äººå‘å¸ƒçš„è§†é¢‘
        if is_up_specific_file:
            # å¦‚æœæ˜¯UPä¸»ä¸“é—¨æ–‡ä»¶ï¼Œæ‰€æœ‰æ•°æ®éƒ½è§†ä¸ºUPä¸»è§†é¢‘
            up_posts = df.copy()
            print(f"\nğŸ“Š UPä¸» '{up_name}' ç›¸å…³è§†é¢‘:")
            print(f"  UPä¸»æœ¬äººå‘å¸ƒè§†é¢‘æ•°: {len(up_posts)} (æ¥è‡ªä¸“é—¨æ–‡ä»¶)")
        elif 'user' in df.columns:
            # ä»é€šç”¨æ–‡ä»¶ä¸­ç­›é€‰UPä¸»è§†é¢‘
            # ç²¾ç¡®åŒ¹é…UPä¸»åç§°
            up_posts_exact = df[df['user'] == up_name].copy()
            # å¦‚æœç²¾ç¡®åŒ¹é…ä¸å¤Ÿï¼Œä½¿ç”¨æ¨¡ç³ŠåŒ¹é…
            if len(up_posts_exact) < 10:
                up_pattern = re.compile(rf'{re.escape(up_name)}|é¾™å¥³', re.IGNORECASE)
                up_posts_fuzzy = df[df['user'].apply(lambda x: bool(up_pattern.search(str(x))))].copy()
                up_posts = pd.concat([up_posts_exact, up_posts_fuzzy]).drop_duplicates(subset=['bvid'] if 'bvid' in df.columns else None)
            else:
                up_posts = up_posts_exact
            
            print(f"\nğŸ“Š UPä¸» '{up_name}' ç›¸å…³è§†é¢‘:")
            print(f"  UPä¸»æœ¬äººå‘å¸ƒè§†é¢‘æ•°: {len(up_posts)}")
            
            # å¦‚æœUPä¸»è§†é¢‘æ•°æ®å……è¶³ï¼Œä¼˜å…ˆä½¿ç”¨UPä¸»æœ¬äººçš„è§†é¢‘è¿›è¡Œåˆ†æ
            if len(up_posts) >= 20:
                print(f"  âœ… UPä¸»æœ¬äººè§†é¢‘æ•°æ®å……è¶³ï¼ˆ{len(up_posts)}æ¡ï¼‰ï¼Œå°†ä¼˜å…ˆåˆ†æUPä¸»å†…å®¹")
            elif len(up_posts) > 0:
                print(f"  âš ï¸ UPä¸»æœ¬äººè§†é¢‘è¾ƒå°‘ï¼ˆ{len(up_posts)}æ¡ï¼‰ï¼Œå°†åˆå¹¶å…¶ä»–ç›¸å…³è§†é¢‘è¿›è¡Œåˆ†æ")
        else:
            up_posts = pd.DataFrame()
            print(f"\nğŸ“Š UPä¸» '{up_name}' ç›¸å…³è§†é¢‘:")
            print(f"  UPä¸»æœ¬äººå‘å¸ƒè§†é¢‘æ•°: {len(up_posts)} (æ— æ³•è¯†åˆ«UPä¸»å­—æ®µ)")
        
        # 2. æœç´¢æåŠUPä¸»çš„è§†é¢‘
        mention_patterns = [
            r'é¾™å¥³å¡”ç½—', r'#é¾™å¥³å¡”ç½—#', r'@é¾™å¥³å¡”ç½—', r'é¾™å¥³å¡”ç½—è€å¸ˆ', 
            r'é¾™å¥³å¡”ç½—è¯´', r'é¾™å¥³', r'longnv'
        ]
        mention_posts = pd.DataFrame()
        if 'text' in df.columns:
            for pattern in mention_patterns:
                matched = df[df['text'].str.contains(pattern, na=False, regex=True)]
                mention_posts = pd.concat([mention_posts, matched])
            mention_posts = mention_posts.drop_duplicates()
            print(f"  æ˜ç¡®æåŠUPä¸»è§†é¢‘æ•°: {len(mention_posts)}")
        
        # 3. UPä¸»ç›¸å…³è¯é¢˜çš„è§†é¢‘ï¼ˆæ‰©å±•å…³é”®è¯èŒƒå›´ä»¥æé«˜æ•°æ®è¦†ç›–ç‡ï¼‰
        tarot_keywords = ['å¡”ç½—', 'å¡”ç½—ç‰Œ', 'å åœ', 'æŠ½ç‰Œ', 'ç‰Œæ„', 'ç‰Œé˜µ', 'å¡”ç½—å åœ', 
                         'å¤åˆ', 'åˆ†æ‰‹', 'æ‹çˆ±', 'æƒ…æ„Ÿ', 'æƒ…æ„Ÿå’¨è¯¢', 'æƒ…æ„Ÿåˆ†æ',
                         'å¿ƒç†', 'æ€§æ ¼', 'æµ‹è¯•', 'åˆ†æ', 'é¢„æµ‹', 'å»ºè®®', 'å’¨è¯¢', 
                         'æŒ‡å¯¼', 'å¸®åŠ©', 'è§£æƒ‘', 'ç­”ç–‘', 'è§£ç­”', 'è¿åŠ¿', 'çˆ±æƒ…è¿åŠ¿']
        keyword_posts = pd.DataFrame()
        if 'text' in df.columns:
            for keyword in tarot_keywords:
                matched = df[df['text'].str.contains(keyword, na=False)]
                keyword_posts = pd.concat([keyword_posts, matched])
            keyword_posts = keyword_posts.drop_duplicates()
            print(f"  ç›¸å…³è¯é¢˜è§†é¢‘æ•°: {len(keyword_posts)}")
        
        # 4. å¡”ç½—ç‰Œç›¸å…³è§†é¢‘
        tarot_terms = [
            'å¡”ç½—ç‰Œ', 'å¡”ç½—å åœ', 'å¡”ç½—è§£è¯»', 'å¡”ç½—ç‰Œé˜µ', 'å¡”ç½—å’¨è¯¢',
            'å¤§é˜¿å¡çº³', 'å°é˜¿å¡çº³', 'æƒæ–', 'åœ£æ¯', 'å®å‰‘', 'æ˜Ÿå¸',
            'æ„šè€…', 'é­”æœ¯å¸ˆ', 'å¥³ç¥­å¸', 'çš‡å', 'çš‡å¸', 'æ•™çš‡', 'æ‹äºº', 
            'æˆ˜è½¦', 'åŠ›é‡', 'éšè€…', 'å‘½è¿ä¹‹è½®', 'æ­£ä¹‰', 'å€’åŠäºº', 'æ­»ç¥',
            'èŠ‚åˆ¶', 'æ¶é­”', 'å¡”', 'æ˜Ÿæ˜Ÿ', 'æœˆäº®', 'å¤ªé˜³', 'å®¡åˆ¤', 'ä¸–ç•Œ'
        ]
        tarot_posts = pd.DataFrame()
        if 'text' in df.columns:
            for term in tarot_terms:
                matched = df[df['text'].str.contains(term, na=False)]
                tarot_posts = pd.concat([tarot_posts, matched])
            tarot_posts = tarot_posts.drop_duplicates()
            print(f"  å¡”ç½—ç›¸å…³è§†é¢‘æ•°: {len(tarot_posts)}")
        
        # 5. åˆå¹¶åˆ†ææ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨UPä¸»æœ¬äººè§†é¢‘ï¼‰
        if is_up_specific_file:
            # UPä¸»ä¸“é—¨æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æ•°æ®
            all_related_posts = up_posts.copy()
            print(f"  ğŸ’¡ ä½¿ç”¨ç­–ç•¥ï¼šä½¿ç”¨UPä¸»ä¸“é—¨æ–‡ä»¶ä¸­çš„æ‰€æœ‰è§†é¢‘ï¼ˆ{len(all_related_posts)}æ¡ï¼‰")
        elif len(up_posts) >= 30:
            # UPä¸»è§†é¢‘å……è¶³ï¼Œä¸»è¦ä½¿ç”¨UPä¸»è§†é¢‘ï¼Œè¡¥å……ä¸€äº›ç›¸å…³è§†é¢‘
            print(f"  ğŸ’¡ ä½¿ç”¨ç­–ç•¥ï¼šä»¥UPä¸»æœ¬äººè§†é¢‘ä¸ºä¸»ï¼ˆ{len(up_posts)}æ¡ï¼‰ï¼Œè¡¥å……ç›¸å…³è§†é¢‘")
            # åˆå¹¶æ—¶ï¼ŒUPä¸»è§†é¢‘ä¼˜å…ˆ
            all_related_posts = pd.concat([
                up_posts, 
                mention_posts, 
                keyword_posts.head(50) if len(keyword_posts) > 50 else keyword_posts,  # é™åˆ¶å…¶ä»–è§†é¢‘æ•°é‡
            ]).drop_duplicates(subset=['bvid'] if 'bvid' in df.columns else None)
        else:
            # UPä¸»è§†é¢‘ä¸è¶³ï¼Œåˆå¹¶æ‰€æœ‰ç›¸å…³è§†é¢‘
            print(f"  ğŸ’¡ ä½¿ç”¨ç­–ç•¥ï¼šåˆå¹¶æ‰€æœ‰ç›¸å…³è§†é¢‘ï¼ˆUPä¸»{len(up_posts)}æ¡ + ç›¸å…³è§†é¢‘ï¼‰")
            all_related_posts = pd.concat([
                up_posts, 
                mention_posts, 
                keyword_posts, 
                tarot_posts
            ]).drop_duplicates(subset=['bvid'] if 'bvid' in df.columns else None)
        
        print(f"\nğŸ“Š ç»¼åˆåˆ†ææ•°æ®ç»Ÿè®¡:")
        print(f"  åˆå¹¶å»é‡ååˆ†ææ•°æ®: {len(all_related_posts)}æ¡")
        print(f"  æ•°æ®è¦†ç›–ç‡: {len(all_related_posts)/len(df)*100:.1f}%")
        
        # æ•°æ®é‡è¯„ä¼°
        if len(all_related_posts) < 200:
            print(f"  âš ï¸ æ•°æ®é‡è¾ƒå°‘ï¼ˆ{len(all_related_posts)}æ¡ï¼‰ï¼Œå»ºè®®è‡³å°‘200æ¡ä»¥ä¸Šè·å¾—æ›´å¯é çš„åˆ†æç»“æœ")
        elif len(all_related_posts) < 500:
            print(f"  âš ï¸ æ•°æ®é‡ä¸­ç­‰ï¼ˆ{len(all_related_posts)}æ¡ï¼‰ï¼Œå»ºè®®æ”¶é›†æ›´å¤šæ•°æ®ä»¥æé«˜åˆ†æå‡†ç¡®æ€§")
        else:
            print(f"  âœ… æ•°æ®é‡å……è¶³ï¼ˆ{len(all_related_posts)}æ¡ï¼‰ï¼Œå¯ä»¥è¿›è¡Œåˆ†æ")
        
        # å¦‚æœç›¸å…³æ•°æ®å¤ªå°‘ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œåˆ†æï¼ˆæ·»åŠ æ ‡è®°ï¼‰
        use_all_data = False
        if len(all_related_posts) < 50:
            print(f"  ğŸ’¡ ç›¸å…³æ•°æ®è¿‡å°‘ï¼Œå°†ä½¿ç”¨å…¨éƒ¨{len(df)}æ¡æ•°æ®è¿›è¡Œåˆæ­¥åˆ†æ")
            all_related_posts = df.copy()
            use_all_data = True
        
        # æ£€æŸ¥å…³é”®è¯è¦†ç›–ç‡
        if len(all_related_posts) > 0 and 'text' in all_related_posts.columns:
            text_sample = all_related_posts['text'].str.cat(sep=' ')
            keyword_coverage = {}
            for keyword in tarot_keywords[:10]:  # æ£€æŸ¥å‰10ä¸ªå…³é”®è¯
                count = text_sample.count(keyword)
                if count > 0:
                    keyword_coverage[keyword] = count
            print(f"  é«˜é¢‘å…³é”®è¯: {dict(Counter(keyword_coverage).most_common(5))}")
        
        # æ£€æŸ¥äº’åŠ¨æ•°æ®å¯ç”¨æ€§
        interaction_available = False
        if 'attitudes_count' in all_related_posts.columns:
            total_interaction = all_related_posts['attitudes_count'].sum() + \
                              all_related_posts['comments_count'].sum()
            interaction_available = total_interaction > 0
        
        return {
            'up_posts': up_posts,
            'mention_posts': mention_posts,
            'keyword_posts': keyword_posts,
            'tarot_posts': tarot_posts,
            'analysis_posts': all_related_posts,
            'all_data': df,
            'data_summary': {
                'total_posts': len(df),
                'analysis_posts': len(all_related_posts),
                'interaction_data_available': interaction_available
            }
        }
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

# ======================================
# 2. ä¸‰ç»´è¯„ä¼°æ¡†æ¶
# ======================================

def enhanced_content_analysis(analysis_data, up_name="é¾™å¥³å¡”ç½—"):
    """å¢å¼ºçš„å†…å®¹ç»´åº¦åˆ†æ"""
    if len(analysis_data) == 0:
        print("âš ï¸ æ²¡æœ‰åˆ†ææ•°æ®")
        return None
    
    print(f"ğŸ” æ‰§è¡Œå¢å¼ºå†…å®¹åˆ†æï¼Œæ ·æœ¬æ•°: {len(analysis_data)}")
    
    # æ¸…ç†æ–‡æœ¬
    analysis_data['clean_text'] = analysis_data['text'].apply(clean_text)
    
    content_metrics = {}
    
    # 1. å†…å®¹å½¢å¼æ·±åº¦åˆ†æ
    text_lengths = analysis_data['clean_text'].apply(lambda x: len(x))
    content_metrics['text_length'] = {
        'mean': text_lengths.mean(),
        'median': text_lengths.median(),
        'std': text_lengths.std(),
        'min': text_lengths.min(),
        'max': text_lengths.max()
    }
    
    # è§†é¢‘æ ‡é¢˜é•¿åº¦åˆ†å¸ƒ
    length_bins = [0, 10, 20, 30, 50, 100, float('inf')]
    length_labels = ['è¶…çŸ­(<10)', 'çŸ­(10-20)', 'ä¸­ç­‰(20-30)', 'é•¿(30-50)', 'è¾ƒé•¿(50-100)', 'è¶…é•¿(>100)']
    length_dist = pd.cut(text_lengths, bins=length_bins, labels=length_labels).value_counts()
    content_metrics['length_distribution'] = (length_dist / len(analysis_data)).to_dict()
    
    # å†…å®¹å½¢å¼ç‰¹å¾åˆ†æ
    # æ ‡é¢˜ç»“æ„ç‰¹å¾
    analysis_data['has_brackets'] = analysis_data['clean_text'].str.contains(r'[ã€\[]', na=False)
    analysis_data['has_question'] = analysis_data['clean_text'].str.contains(r'[?ï¼Ÿ]', na=False)
    analysis_data['has_exclamation'] = analysis_data['clean_text'].str.contains(r'[!ï¼]', na=False)
    analysis_data['has_emoji'] = analysis_data['clean_text'].str.contains(r'[\u4e00-\u9fff]*[ğŸ”®ğŸ´ğŸ’«âœ¨ğŸŒŸğŸ’ğŸ’–ğŸ’•â¤ï¸ğŸ’”ğŸ’—]', na=False)
    analysis_data['word_count'] = analysis_data['clean_text'].str.split().str.len()
    
    content_metrics['form_features'] = {
        'brackets_ratio': analysis_data['has_brackets'].mean(),
        'question_ratio': analysis_data['has_question'].mean(),
        'exclamation_ratio': analysis_data['has_exclamation'].mean(),
        'emoji_ratio': analysis_data['has_emoji'].mean(),
        'avg_word_count': analysis_data['word_count'].mean(),
        'most_common_length_range': length_dist.idxmax() if len(length_dist) > 0 else 'æœªçŸ¥'
    }
    
    # æ ‡é¢˜é£æ ¼åˆ†æ
    style_patterns = {
        'ç–‘é—®å¼': [r'[?ï¼Ÿ].*', r'å¦‚ä½•', r'æ€ä¹ˆ', r'ä¸ºä»€ä¹ˆ', r'ä¼šä¸ä¼š', r'æ˜¯å¦'],
        'è‚¯å®šå¼': [r'ï¼', r'ï¼.*', r'å¿…é¡»', r'ä¸€å®š', r'è‚¯å®š', r'ç»å¯¹'],
        'æ¨èå¼': [r'å»ºè®®', r'æ¨è', r'å€¼å¾—', r'å¯ä»¥', r'åº”è¯¥'],
        'æƒ…æ„Ÿå¼': [r'å“­äº†', r'æ„ŸåŠ¨', r'éœ‡æ’¼', r'ç»äº†', r'å¤ª.*äº†'],
        'æ•°å­—å¼': [r'\d+[ä¸ªæ¡é¡¹ç‚¹]', r'ç¬¬\d+', r'\d+ç§', r'\d+ä¸ª']
    }
    
    style_counts = {}
    for style, patterns in style_patterns.items():
        count = 0
        for pattern in patterns:
            count += analysis_data['clean_text'].str.contains(pattern, na=False, regex=True).sum()
        style_counts[style] = count / len(analysis_data)
    
    content_metrics['style_distribution'] = style_counts
    
    # 1.5 å†…å®¹å½¢å¼ç»†åˆ†ï¼šè¯†åˆ«äº’åŠ¨æ¨¡å¼å’Œåœºæ™¯
    # äº’åŠ¨æ¨¡å¼è¯†åˆ« - æ›´ç²¾ç¡®çš„åŒ¹é…
    # æ£€æŸ¥æ ‡é¢˜ä¸­æ˜¯å¦åŒ…å«ä»£è¯ï¼ˆ"ä»–"ã€"ta"ç­‰ï¼‰ï¼Œè¿™æ˜¯"è¾¹çœ‹è¾¹æµ‹"åœºæ™¯çš„ç‰¹å¾
    analysis_data['has_pronoun'] = analysis_data['clean_text'].str.contains(r'[ä»–å¥¹å®ƒ]|ta|TA|Ta|å¯¹ä½ |ä½ çš„', na=False, regex=True)
    pronoun_ratio = analysis_data['has_pronoun'].mean()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«"ç‰Œ"ç›¸å…³è¯æ±‡ï¼ˆæŠ½ç‰Œäº’åŠ¨ï¼‰
    analysis_data['has_card'] = analysis_data['clean_text'].str.contains(r'ç‰Œ|æŠ½|é€‰', na=False, regex=True)
    card_ratio = analysis_data['has_card'].mean()
    
    interaction_patterns = {
        'è¾¹çœ‹è¾¹æµ‹': ['è¾¹çœ‹è¾¹æµ‹', 'è¾¹çœ‹è¾¹æŠ½', 'å®æ—¶', 'ä¸€èµ·', 'åŒæ­¥', 'è·Ÿç€', 'åŒæ—¶'],
        'çŸ­è§†é¢‘äº’åŠ¨å åœ': ['çŸ­è§†é¢‘', 'çŸ­å ', 'å¿«é€Ÿ', 'ä¸€åˆ†é’Ÿ', 'ç§’æµ‹', 'å¿«é€Ÿå åœ', 'å³æ—¶'],
        'æŠ½ç‰Œäº’åŠ¨': ['æŠ½ç‰Œ', 'é€‰ç‰Œ', 'é€‰ä¸€å¼ ', 'é€‰ä¸‰å¼ ', 'æŠ½ä¸‰å¼ ', 'é€‰ç‰Œé˜µ'],
        'é—®é¢˜å¯¼å‘': ['ä»–å¯¹ä½ ', 'ä»–å¯¹', 'ä½ åœ¨ä»–', 'ä½ åœ¨', 'ä½ ä»¬', 'ä½ ä»¬ä¹‹é—´', 'ä½ ä»¬çš„å…³ç³»', 'è¿™æ®µå…³ç³»', 'å…³äºè¿™æ®µå…³ç³»'],
        'æ—¶é—´é™å®š': ['è¿‘æœŸ', 'æœ€è¿‘', 'æœªæ¥', 'æ¥ä¸‹æ¥', 'è¿™ä¸ªæœˆ', 'æœ¬å‘¨', 'ä»Šå¤©', 'æ˜å¤©', 'è¿‘æœŸ', 'åäºŒæœˆ', '2026å¹´', 'æœªæ¥åå¹´']
    }
    
    interaction_analysis = {}
    for pattern_name, keywords in interaction_patterns.items():
        count = analysis_data['clean_text'].apply(
            lambda x: any(keyword in x for keyword in keywords)
        ).sum()
        interaction_analysis[pattern_name] = {
            'count': count,
            'ratio': count / len(analysis_data)
        }
    
    # å¦‚æœä»£è¯å‡ºç°ç‡é«˜ä¸”åŒ…å«é—®å·ï¼Œå¢åŠ "è¾¹çœ‹è¾¹æµ‹"çš„è¯†åˆ«ï¼ˆå…¸å‹çš„è¾¹çœ‹è¾¹æµ‹åœºæ™¯ï¼‰
    if pronoun_ratio > 0.3:
        analysis_data['has_pronoun_question'] = analysis_data['has_pronoun'] & analysis_data['has_question']
        pronoun_question_ratio = analysis_data['has_pronoun_question'].mean()
        if pronoun_question_ratio > 0.15:
            interaction_analysis['è¾¹çœ‹è¾¹æµ‹'] = {
                'count': max(interaction_analysis['è¾¹çœ‹è¾¹æµ‹']['count'], int(len(analysis_data) * pronoun_question_ratio)),
                'ratio': max(interaction_analysis['è¾¹çœ‹è¾¹æµ‹']['ratio'], pronoun_question_ratio)
            }
    
    # å¦‚æœåŒ…å«"ç‰Œ"ä¸”åŒ…å«ä»£è¯ï¼Œå¯èƒ½æ˜¯æŠ½ç‰Œäº’åŠ¨
    if card_ratio > 0.3 and pronoun_ratio > 0.3:
        analysis_data['has_card_pronoun'] = analysis_data['has_card'] & analysis_data['has_pronoun']
        card_pronoun_ratio = analysis_data['has_card_pronoun'].mean()
        if card_pronoun_ratio > 0.15:
            interaction_analysis['æŠ½ç‰Œäº’åŠ¨'] = {
                'count': max(interaction_analysis['æŠ½ç‰Œäº’åŠ¨']['count'], int(len(analysis_data) * card_pronoun_ratio)),
                'ratio': max(interaction_analysis['æŠ½ç‰Œäº’åŠ¨']['ratio'], card_pronoun_ratio)
            }
    
    content_metrics['interaction_patterns'] = interaction_analysis
    
    # 2. å†…å®¹ä¸»é¢˜æ·±åº¦åˆ†æ - ç»†åˆ†ä¸»é¢˜
    # ä¸€çº§ä¸»é¢˜ï¼ˆå¤§ç±»ï¼‰
    themes = {
        'å¡”ç½—å åœ': ['å¡”ç½—', 'å¡”ç½—ç‰Œ', 'å¡”ç½—å åœ', 'å åœ', 'æŠ½ç‰Œ', 'ç‰Œæ„', 'ç‰Œé˜µ', 'è§£è¯»'],
        'æƒ…æ„Ÿå’¨è¯¢': ['å¤åˆ', 'åˆ†æ‰‹', 'æ‹çˆ±', 'å–œæ¬¢', 'å‰ä»»', 'æš§æ˜§', 'æ¡ƒèŠ±', 'å©šå§»', 'æ„Ÿæƒ…', 'æƒ…æ„Ÿ', 'çˆ±æƒ…'],
        'èŒä¸šå‘å±•': ['offer', 'é¢è¯•', 'æ±‚èŒ', 'å·¥ä½œ', 'äº‹ä¸š', 'å²—ä½', 'æ‹›è˜', 'ç®€å†', 'HR'],
        'å­¦ä¸šæŒ‡å¯¼': ['è€ƒè¯•', 'è€ƒç ”', 'æ¯•ä¸š', 'è®ºæ–‡', 'å¤ä¹ ', 'å››å…­çº§', 'æ•™èµ„', 'å­¦ä¹ ', 'å¤‡è€ƒ', 'ä¸Šå²¸'],
        'å¿ƒç†åˆ†æ': ['å¿ƒç†', 'æ€§æ ¼', 'äººæ ¼', 'æµ‹è¯•', 'MBTI', 'æ˜¾åŒ–', 'å¸å¼•åŠ›æ³•åˆ™'],
        'è¡ŒåŠ¨æŒ‡å¯¼': ['å»ºè®®', 'åº”è¯¥', 'éœ€è¦', 'å¯ä»¥', 'æ–¹æ³•', 'æ­¥éª¤', 'æ¸…å•', 'æŒ‡å—', 'å¦‚ä½•'],
        'è¿åŠ¿é¢„æµ‹': ['è¿åŠ¿', 'çˆ±æƒ…è¿åŠ¿', 'äº‹ä¸šè¿åŠ¿', 'è´¢è¿', 'å¥åº·è¿åŠ¿', 'æœªæ¥', 'é¢„æµ‹']
    }
    
    # äºŒçº§ä¸»é¢˜ï¼ˆç»†åˆ†é—®é¢˜ç±»å‹ï¼‰- é’ˆå¯¹æƒ…æ„Ÿç±»å†…å®¹çš„ç»†åˆ†ï¼Œæ›´ç²¾ç¡®çš„å…³é”®è¯
    detailed_themes = {
        'ä»–å¯¹ä½ çš„æƒ³æ³•': ['ä»–å¯¹ä½ ', 'ä»–å¯¹ä½ çš„æƒ³æ³•', 'taå¯¹ä½ çš„æƒ³æ³•', 'ä»–å¯¹ä½ çš„', 'ä»–å¯¹ä½ ', 'taå¯¹ä½ ', 'ä»–å¯¹ä½ æ€', 'taæ€ä¹ˆæƒ³', 'ä»–çš„æƒ³æ³•', 'taçš„æƒ³æ³•', 'ä»–æƒ³ä½ ', 'ä»–æƒ³å¯¹ä½ ', 'taæƒ³å¯¹ä½ ', 'æƒ³å¯¹ä½ ', 'å¯¹ä½ ', 'æ€ä¹ˆæƒ³'],
        'è¿‘æœŸèƒ½å¦å¤åˆ': ['èƒ½å¦å¤åˆ', 'ä¼šå¤åˆ', 'èƒ½å¤åˆ', 'å¤åˆå—', 'ä¼šå¤åˆå—', 'èƒ½å¤åˆå—', 'è¿‘æœŸå¤åˆ', 'æœ€è¿‘å¤åˆ', 'å¯ä»¥å¤åˆ', 'ä¼šå¤åˆå—', 'ä»€ä¹ˆæ—¶å€™å¤åˆ'],
        'åˆ†æ‰‹ç›¸å…³': ['åˆ†æ‰‹', 'åˆ†æ‰‹äº†', 'åˆ†æ‰‹å', 'åˆ†æ‰‹åŸå› ', 'ä¸ºä»€ä¹ˆåˆ†æ‰‹', 'åˆ†æ‰‹åä»–', 'åˆ†æ‰‹åä½ ', 'ä¼šåˆ†æ‰‹', 'è¦åˆ†æ‰‹'],
        'å…³ç³»çŠ¶æ€': ['ä½ ä»¬çš„å…³ç³»', 'ä½ ä»¬ä¹‹é—´', 'è¿™æ®µå…³ç³»', 'å…³ç³»', 'ç°åœ¨çš„å…³ç³»', 'ç›®å‰çš„å…³ç³»', 'å…³ç³»å¦‚ä½•', 'å…³ç³»æŒ‡å¼•', 'å…³äºè¿™æ®µå…³ç³»'],
        'æœªæ¥èµ°å‘': ['æœªæ¥', 'æ¥ä¸‹æ¥', 'ä»¥å', 'å°†æ¥', 'ä¼šæ€æ ·', 'ä¼šå¦‚ä½•', 'å‘å±•', 'èµ°å‘', 'æœªæ¥åå¹´', 'æ„å‘³ç€ä»€ä¹ˆ', 'æ„ä¹‰'],
        'å¯¹æ–¹æ€åº¦': ['taå¯¹', 'ä»–å¯¹ä½ ', 'ä»–å¯¹', 'taçš„æ€åº¦', 'ä»–çš„æ€åº¦', 'ä»–çš„æ„Ÿå—', 'ä»–çš„å¿ƒç†', 'taçš„çœŸå®æƒ³æ³•', 'çœŸå®æƒ³æ³•'],
        'æ„Ÿæƒ…å‘å±•': ['æ„Ÿæƒ…', 'æ‹çˆ±', 'çˆ±æƒ…', 'å–œæ¬¢', 'çˆ±', 'åœ¨ä¸€èµ·', 'åœ¨ä¸€èµ·å—', 'çˆ±æƒ…å‘', 'æ„Ÿæƒ…è¯é¢˜'],
        'æš§æ˜§å…³ç³»': ['æš§æ˜§', 'æš§æ˜§å…³ç³»', 'æš§æ˜§æœŸ', 'æ˜¯ä¸æ˜¯æš§æ˜§', 'æš§æ˜§å—'],
        'å‰ä»»ç›¸å…³': ['å‰ä»»', 'å‰ç”·å‹', 'å‰å¥³å‹', 'ex', 'å‰åº¦', 'å‰å¯¹è±¡'],
        'æ–°æ‹æƒ…': ['æ–°æ‹æƒ…', 'æ–°æ¡ƒèŠ±', 'æ–°å¯¹è±¡', 'æ–°äºº', 'æ–°çš„', 'æ–°çš„äºº', 'é‡åˆ°', 'å¯¹çš„äºº'],
        'å©šå§»ç›¸å…³': ['ç»“å©š', 'ä¼šç»“å©š', 'ä»€ä¹ˆæ—¶å€™ç»“å©š', 'å©šå§»', 'ç»“å©šå—'],
        'ä½¿å‘½æ„ä¹‰': ['ä½¿å‘½', 'æ„ä¹‰', 'å¯¹ä½ æ„å‘³ç€', 'äººç”Ÿçš„æ„ä¹‰']
    }
    
    detailed_theme_analysis = {}
    for theme, keywords in detailed_themes.items():
        theme_posts = analysis_data['clean_text'].apply(
            lambda x: any(keyword in x for keyword in keywords)
        ).sum()
        
        if theme_posts > 0:
            keyword_counts = analysis_data['clean_text'].apply(
                lambda x: sum(x.count(keyword) for keyword in keywords)
            ).sum()
            
            detailed_theme_analysis[theme] = {
                'post_count': theme_posts,
                'post_ratio': theme_posts / len(analysis_data),
                'keyword_density': keyword_counts / text_lengths.sum() * 1000 if text_lengths.sum() > 0 else 0
            }
    
    content_metrics['detailed_themes'] = detailed_theme_analysis
    
    theme_analysis = {}
    for theme, keywords in themes.items():
        # è®¡ç®—ä¸»é¢˜å‡ºç°é¢‘ç‡
        theme_posts = analysis_data['clean_text'].apply(
            lambda x: any(keyword in x for keyword in keywords)
        ).sum()
        
        # è®¡ç®—ä¸»é¢˜å…³é”®è¯å¯†åº¦
        keyword_counts = analysis_data['clean_text'].apply(
            lambda x: sum(x.count(keyword) for keyword in keywords)
        ).sum()
        
        theme_analysis[theme] = {
            'post_count': theme_posts,
            'post_ratio': theme_posts / len(analysis_data),
            'keyword_density': keyword_counts / text_lengths.sum() * 1000 if text_lengths.sum() > 0 else 0
        }
    
    content_metrics['themes'] = theme_analysis
    
    # 3. å†…å®¹ç‰¹å¾åˆ†æ
    # ç†æ€§é¢„æµ‹ç‰¹å¾
    rational_patterns = [
        'é¢„æµ‹', 'åˆ†æ', 'è§£è¯»', 'åŸå› ', 'ç»“æœ', 'å› ä¸º', 'æ‰€ä»¥', 
        'é€»è¾‘', 'ç†æ€§', 'å®¢è§‚', 'æ•°æ®', 'æ¨æµ‹', 'åˆ¤æ–­', 'è¯„ä¼°'
    ]
    
    # è¡ŒåŠ¨æ¸…å•ç‰¹å¾
    action_patterns = [
        'å»ºè®®', 'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦', 'æ–¹æ³•', 'æ­¥éª¤', 'æ¸…å•', 
        'åˆ—è¡¨', 'ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰', 'å¦‚ä½•åš', 'æ€ä¹ˆåš', 'è¡ŒåŠ¨'
    ]
    
    # å¿ƒç†æ…°è—‰ç‰¹å¾
    comfort_patterns = [
        'å®‰æ…°', 'é¼“åŠ±', 'æ”¯æŒ', 'ç†è§£', 'é™ªä¼´', 'å…±é¸£', 
        'æ²»æ„ˆ', 'æ¸©æš–', 'å¸Œæœ›', 'åŠ æ²¹', 'ç¥ç¦'
    ]
    
    def count_patterns(text, patterns):
        return sum(1 for pattern in patterns if pattern in text)
    
    analysis_data['rational_score'] = analysis_data['clean_text'].apply(
        lambda x: count_patterns(x, rational_patterns)
    )
    analysis_data['action_score'] = analysis_data['clean_text'].apply(
        lambda x: count_patterns(x, action_patterns)
    )
    analysis_data['comfort_score'] = analysis_data['clean_text'].apply(
        lambda x: count_patterns(x, comfort_patterns)
    )
    
    content_metrics['content_features'] = {
        'rational_mean': analysis_data['rational_score'].mean(),
        'action_mean': analysis_data['action_score'].mean(),
        'comfort_mean': analysis_data['comfort_score'].mean(),
        'has_rational': (analysis_data['rational_score'] > 0).mean(),
        'has_action': (analysis_data['action_score'] > 0).mean(),
        'has_comfort': (analysis_data['comfort_score'] > 0).mean()
    }
    
    # 4. å†…å®¹è´¨é‡è¯„ä¼°
    # è®¡ç®—å†…å®¹å¤šæ ·æ€§ï¼ˆä¸åŒä¸»é¢˜çš„è¦†ç›–ï¼‰
    theme_coverage = len([t for t in theme_analysis.values() if t['post_ratio'] > 0.1])
    content_metrics['quality'] = {
        'theme_diversity': theme_coverage / len(themes),
        'avg_length_score': min(text_lengths.mean() / 50, 1.0),  # è§†é¢‘æ ‡é¢˜é€šå¸¸è¾ƒçŸ­
        'structure_score': (analysis_data['action_score'] > 0).mean(),
        'rationality_score': (analysis_data['rational_score'] > 0).mean()
    }
    
    # 5. é¾™å¥³å¡”ç½—ç‰¹è‰²åˆ†æ
    longnv_signatures = [
        'å¡”ç½—å åœ', 'å¡”ç½—ç‰Œ', 'æƒ…æ„Ÿå’¨è¯¢', 'å¤åˆå»ºè®®', 
        'å¿ƒç†åˆ†æ', 'è¿åŠ¿é¢„æµ‹', 'è¡ŒåŠ¨æŒ‡å¯¼', 'å¡”ç½—è§£è¯»'
    ]
    
    signature_counts = {}
    for signature in longnv_signatures:
        count = analysis_data['clean_text'].apply(
            lambda x: signature in x
        ).sum()
        signature_counts[signature] = count / len(analysis_data)
    
    content_metrics['signatures'] = signature_counts
    content_metrics['signature_match'] = sum(1 for v in signature_counts.values() if v > 0.05) / len(signature_counts)
    
    print(f"âœ… å¢å¼ºå†…å®¹åˆ†æå®Œæˆ")
    print(f"\nğŸ“Š å†…å®¹å½¢å¼åˆ†æ:")
    print(f"   å¹³å‡æ–‡æœ¬é•¿åº¦: {content_metrics['text_length']['mean']:.1f}å­—ç¬¦")
    form_features = content_metrics['form_features']
    print(f"   æ ‡é¢˜ç»“æ„: å«æ‹¬å·{form_features['brackets_ratio']:.1%}, å«é—®å·{form_features['question_ratio']:.1%}, å«æ„Ÿå¹å·{form_features['exclamation_ratio']:.1%}")
    print(f"   å¸¸è§é•¿åº¦åŒºé—´: {form_features['most_common_length_range']}")
    style_dist = content_metrics['style_distribution']
    top_styles = sorted(style_dist.items(), key=lambda x: x[1], reverse=True)[:3]
    print(f"   ä¸»è¦æ ‡é¢˜é£æ ¼: {', '.join([f'{s[0]}({s[1]:.1%})' for s in top_styles])}")
    
    print(f"\nğŸ“Š æ ¸å¿ƒä¸»é¢˜åˆ†æ:")
    print(f"   ä¸»é¢˜å¤šæ ·æ€§: {content_metrics['quality']['theme_diversity']:.1%}")
    themes = content_metrics['themes']
    top_themes = sorted(themes.items(), key=lambda x: x[1]['post_ratio'], reverse=True)[:5]
    print(f"   å‰5å¤§ä¸»é¢˜:")
    for theme, data in top_themes:
        print(f"     â€¢ {theme}: {data['post_ratio']:.1%} (å…³é”®è¯å¯†åº¦: {data['keyword_density']:.2f})")
    
    return content_metrics

def enhanced_communication_analysis(data_dict, up_name="é¾™å¥³å¡”ç½—"):
    """å¢å¼ºçš„ä¼ æ’­ç»´åº¦åˆ†æ"""
    print(f"\nğŸ“¢ æ‰§è¡Œå¢å¼ºä¼ æ’­åˆ†æ")
    
    comm_metrics = {}
    
    # ä½¿ç”¨åˆå¹¶çš„åˆ†ææ•°æ®
    analysis_data = data_dict.get('analysis_posts', pd.DataFrame())
    all_data = data_dict.get('all_data', pd.DataFrame())
    
    if len(analysis_data) == 0:
        print("âš ï¸ æ²¡æœ‰åˆ†ææ•°æ®")
        return comm_metrics
    
    # 1. ä¼ æ’­å¹¿åº¦åˆ†æ
    comm_metrics['topic_coverage'] = len(analysis_data) / len(all_data) if len(all_data) > 0 else 0
    
    # å‚ä¸ç”¨æˆ·æ•°
    if 'user' in analysis_data.columns:
        unique_users = analysis_data['user'].nunique()
        comm_metrics['participant_count'] = unique_users
        print(f"  å‚ä¸ç”¨æˆ·æ•°: {unique_users}äºº")
    else:
        comm_metrics['participant_count'] = 0
    
    # 2. ç”¨æˆ·å‚ä¸åº¦åˆ†æ
    if 'user' in analysis_data.columns:
        user_post_counts = analysis_data['user'].value_counts()
        active_users = len(user_post_counts[user_post_counts > 1])
        comm_metrics['active_users'] = active_users
        
        # ç”¨æˆ·é›†ä¸­åº¦ï¼ˆåŸºå°¼ç³»æ•°ï¼‰
        user_engagement_gini = calculate_gini(user_post_counts.values)
        comm_metrics['user_concentration'] = user_engagement_gini
    
    # 3. ä¼ æ’­æ½œåŠ›åˆ†æï¼ˆåŸºäºäº’åŠ¨æ•°æ®ï¼‰
    if 'attitudes_count' in analysis_data.columns:
        avg_views = analysis_data['attitudes_count'].mean()
        avg_comments = analysis_data['comments_count'].mean() if 'comments_count' in analysis_data.columns else 0
        comm_metrics['avg_views'] = avg_views
        comm_metrics['avg_comments'] = avg_comments
        
        # ä¼ æ’­æ½œåŠ›æŒ‡æ•°
        potential_score = min(avg_views / 100000, 1.0) * 0.5 + min(avg_comments / 1000, 1.0) * 0.5
        comm_metrics['potential'] = potential_score
    else:
        comm_metrics['potential'] = 0
    
    # 4. è¯é¢˜æ ‡ç­¾åˆ†æï¼ˆä»æ ‡é¢˜ä¸­æå–ï¼‰
    hashtags = {}
    if 'text' in analysis_data.columns:
        for text in analysis_data['text']:
            if isinstance(text, str):
                # æå–#æ ‡ç­¾#
                tags = re.findall(r'#([^#]+)#', text)
                for tag in tags:
                    hashtags[tag] = hashtags.get(tag, 0) + 1
    
    comm_metrics['hashtags'] = {
        'total_tags': len(hashtags),
        'top_hashtags': dict(Counter(hashtags).most_common(20))
    }
    
    print(f"âœ… å¢å¼ºä¼ æ’­åˆ†æå®Œæˆ")
    print(f"   è¯é¢˜è¦†ç›–ç‡: {comm_metrics['topic_coverage']:.1%}")
    print(f"   å‚ä¸ç”¨æˆ·æ•°: {comm_metrics['participant_count']}")
    print(f"   ä¼ æ’­æ½œåŠ›: {comm_metrics['potential']:.3f}")
    
    return comm_metrics

def enhanced_psychological_analysis(data_dict, up_name="é¾™å¥³å¡”ç½—"):
    """å¢å¼ºçš„å¿ƒç†ç»´åº¦åˆ†æ"""
    print(f"\nğŸ§  æ‰§è¡Œå¢å¼ºå¿ƒç†åˆ†æ")
    
    psych_metrics = {}
    
    analysis_data = data_dict.get('analysis_posts', pd.DataFrame())
    
    if len(analysis_data) == 0:
        print("âš ï¸ æ²¡æœ‰åˆ†ææ•°æ®")
        return psych_metrics
    
    if 'clean_text' not in analysis_data.columns:
        analysis_data['clean_text'] = analysis_data['text'].apply(clean_text)
    
    # 1. æƒ…ç»ªè¾“å‡ºæ·±åº¦åˆ†æ - ç»†åˆ†æƒ…ç»ªç±»å‹
    # æ‰©å±•æƒ…ç»ªè¯åº“ï¼ˆé’ˆå¯¹å¡”ç½—å åœå†…å®¹ï¼‰
    positive_words = ['å¥½', 'æ£’', 'å–œæ¬¢', 'çˆ±', 'å¼€å¿ƒ', 'å¿«ä¹', 'å¹¸ç¦', 'æ»¡æ„', 'æ„Ÿè°¢', 'è°¢è°¢', 
                     'æ”¯æŒ', 'åŠ æ²¹', 'ç¥ç¦', 'å¸Œæœ›', 'æœŸå¾…', 'æˆåŠŸ', 'é¡ºåˆ©', 'å¥½è¿', 'ç¾å¥½',
                     'æ²»æ„ˆ', 'æ¸©æš–', 'æ„ŸåŠ¨', 'æƒŠå–œ', 'å¹¸è¿', 'åœ†æ»¡', 'å®Œç¾', 'ç†æƒ³', 'å¦‚æ„¿',
                     'å¤åˆ', 'å’Œå¥½', 'é‡å½’äºå¥½', 'åœ¨ä¸€èµ·', 'ç›¸é‡', 'é‡è§']
    
    negative_words = ['ä¸å¥½', 'è®¨åŒ', 'éš¾è¿‡', 'æ‚²ä¼¤', 'å¤±æœ›', 'ç—›è‹¦', 'å›°éš¾', 'é—®é¢˜', 'æ‹…å¿ƒ', 
                     'ç„¦è™‘', 'å‹åŠ›', 'å¤±è´¥', 'åæ‚”', 'é—æ†¾', 'åˆ†æ‰‹', 'ç»“æŸ', 'ç¦»å¼€', 'å¤±å»',
                     'å­¤ç‹¬', 'å¯‚å¯', 'ç—›è‹¦', 'ç…ç†¬', 'å›°æ‰°', 'çƒ¦æ¼', 'çº ç»“', 'è¿·èŒ«', 'ç»æœ›',
                     'é€ƒé¿', 'æ”¾å¼ƒ', 'ç»“æŸ', 'æ–­è”']
    
    neutral_words = ['åˆ†æ', 'è§£è¯»', 'é¢„æµ‹', 'å»ºè®®', 'æ–¹æ³•', 'æ­¥éª¤', 'å¯ä»¥', 'å¯èƒ½', 'ä¹Ÿè®¸',
                    'æˆ–è€…', 'ç†æ€§', 'å®¢è§‚', 'æ•°æ®', 'äº‹å®', 'ç»“æœ', 'åŸå› ', 'å› ä¸º', 'æ‰€ä»¥',
                    'å¡”ç½—', 'å åœ', 'æŠ½ç‰Œ', 'ç‰Œæ„', 'ç‰Œé˜µ', 'è§£è¯»', 'è¿åŠ¿']
    
    # ç»†åˆ†æƒ…ç»ªç±»å‹è¯åº“
    emotion_type_words = {
        'å®‰æ…°': ['å®‰æ…°', 'ç†è§£', 'é™ªä¼´', 'æ”¯æŒ', 'æ¸©æš–', 'æ²»æ„ˆ', 'æŠ±æŠ±', 'æ‘¸æ‘¸', 'å¿ƒç–¼', 'ç†è§£ä½ '],
        'é¼“åŠ±': ['åŠ æ²¹', 'é¼“åŠ±', 'ç›¸ä¿¡', 'åšæŒ', 'åŠªåŠ›', 'ä¼šå¥½çš„', 'å¯ä»¥çš„', 'ä¸€å®š', 'è‚¯å®š', 'ä¼šæˆåŠŸ'],
        'æ”¯æŒ': ['æ”¯æŒ', 'ç¥ç¦', 'å¸Œæœ›', 'æœŸå¾…', 'ç›¸ä¿¡', 'åŠ æ²¹', 'ä¸ºä½ ', 'ç»™ä½ ', 'é™ªä½ '],
        'å…±æƒ…': ['ç†è§£', 'æ‡‚ä½ ', 'æ„ŸåŒèº«å—', 'ä¸€æ ·', 'åŒæ ·', 'æˆ‘ä¹Ÿ', 'æˆ‘ä¹Ÿæ˜¯', 'åŒæ„Ÿ'],
        'å¼•å¯¼': ['å»ºè®®', 'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦', 'æ–¹æ³•', 'æ­¥éª¤', 'å¦‚ä½•', 'æ€æ ·', 'è¯•è¯•'],
        'å¸Œæœ›': ['å¸Œæœ›', 'æœŸå¾…', 'æœªæ¥', 'ä¼šå¥½', 'ä¼šæˆåŠŸ', 'ä¼šé¡ºåˆ©', 'ä¼šå¤åˆ', 'ä¼šæœ‰', 'ä¼šæ¥']
    }
    
    def analyze_emotion_detailed(text):
        pos_count = sum(1 for word in positive_words if word in text)
        neg_count = sum(1 for word in negative_words if word in text)
        neu_count = sum(1 for word in neutral_words if word in text)
        
        total = pos_count + neg_count + neu_count
        if total == 0:
            return 'neutral', 0, 0, 0, {}
        
        pos_intensity = pos_count / total
        neg_intensity = neg_count / total
        neu_intensity = neu_count / total
        
        # åˆ†æå…·ä½“æƒ…ç»ªç±»å‹
        emotion_types = {}
        for emo_type, keywords in emotion_type_words.items():
            count = sum(1 for word in keywords if word in text)
            if count > 0:
                emotion_types[emo_type] = count
        
        if pos_intensity > neg_intensity and pos_intensity > 0.3:
            return 'positive', pos_intensity, neg_intensity, neu_intensity, emotion_types
        elif neg_intensity > pos_intensity and neg_intensity > 0.3:
            return 'negative', pos_intensity, neg_intensity, neu_intensity, emotion_types
        else:
            return 'neutral', pos_intensity, neg_intensity, neu_intensity, emotion_types
    
    emotion_results = analysis_data['clean_text'].apply(analyze_emotion_detailed)
    analysis_data['emotion'] = emotion_results.apply(lambda x: x[0])
    analysis_data['pos_intensity'] = emotion_results.apply(lambda x: x[1])
    analysis_data['neg_intensity'] = emotion_results.apply(lambda x: x[2])
    analysis_data['neu_intensity'] = emotion_results.apply(lambda x: x[3])
    analysis_data['emotion_types'] = emotion_results.apply(lambda x: x[4])
    
    # ç»Ÿè®¡å…·ä½“æƒ…ç»ªç±»å‹
    emotion_type_counts = {}
    for emotion_types_dict in analysis_data['emotion_types']:
        if isinstance(emotion_types_dict, dict):
            for emo_type, count in emotion_types_dict.items():
                emotion_type_counts[emo_type] = emotion_type_counts.get(emo_type, 0) + count
    
    # è®¡ç®—æƒ…ç»ªç±»å‹å æ¯”
    total_emotion_type_mentions = sum(emotion_type_counts.values())
    emotion_type_ratios = {k: v / total_emotion_type_mentions if total_emotion_type_mentions > 0 else 0 
                           for k, v in emotion_type_counts.items()}
    
    psych_metrics['emotion_types'] = {
        'counts': emotion_type_counts,
        'ratios': emotion_type_ratios,
        'posts_with': {k: sum(1 for d in analysis_data['emotion_types'] 
                              if isinstance(d, dict) and k in d) 
                       for k in emotion_type_counts.keys()}
    }
    
    emotion_counts = analysis_data['emotion'].value_counts()
    
    emotion_analysis = {}
    for emotion in ['positive', 'negative', 'neutral']:
        count = emotion_counts.get(emotion, 0)
        emotion_data = analysis_data[analysis_data['emotion'] == emotion]
        emotion_analysis[emotion] = {
            'count': count,
            'ratio': count / len(analysis_data),
            'posts_with': count,
            'avg_pos_intensity': emotion_data['pos_intensity'].mean() if len(emotion_data) > 0 else 0,
            'avg_neg_intensity': emotion_data['neg_intensity'].mean() if len(emotion_data) > 0 else 0,
            'avg_neu_intensity': emotion_data['neu_intensity'].mean() if len(emotion_data) > 0 else 0
        }
    
    psych_metrics['emotion_analysis'] = emotion_analysis
    
    # æƒ…ç»ªè¾“å‡ºå¼ºåº¦åˆ†æ
    psych_metrics['emotion_output'] = {
        'overall_positive_intensity': analysis_data['pos_intensity'].mean(),
        'overall_negative_intensity': analysis_data['neg_intensity'].mean(),
        'overall_neutral_intensity': analysis_data['neu_intensity'].mean(),
        'strong_positive_ratio': (analysis_data['pos_intensity'] > 0.5).mean(),
        'strong_negative_ratio': (analysis_data['neg_intensity'] > 0.5).mean(),
        'emotional_variance': analysis_data['pos_intensity'].std() + analysis_data['neg_intensity'].std()
    }
    
    # æƒ…æ„Ÿå¹³è¡¡åº¦
    positive_ratio = emotion_analysis['positive']['ratio']
    negative_ratio = emotion_analysis['negative']['ratio']
    emotion_balance = 1 - abs(positive_ratio - negative_ratio) / (positive_ratio + negative_ratio + 0.001)
    
    psych_metrics['emotion_balance'] = {
        'positive_ratio': positive_ratio,
        'negative_ratio': negative_ratio,
        'balance_score': emotion_balance,
        'dominant_emotion': 'positive' if positive_ratio > negative_ratio else 'negative' if negative_ratio > positive_ratio else 'balanced'
    }
    
    # 2. å¿ƒç†éœ€æ±‚åˆ†æ
    psychological_needs = {
        'æƒ…æ„Ÿéœ€æ±‚': ['çˆ±', 'å–œæ¬¢', 'æ„Ÿæƒ…', 'æƒ…æ„Ÿ', 'æ‹çˆ±', 'åˆ†æ‰‹', 'å¤åˆ', 'å©šå§»', 'å®¶åº­', 'äº²å¯†'],
        'è®¤çŸ¥éœ€æ±‚': ['çŸ¥é“', 'äº†è§£', 'æ˜ç™½', 'ç†è§£', 'å­¦ä¹ ', 'è®¤çŸ¥', 'çŸ¥è¯†', 'ä¿¡æ¯', 'æ€è€ƒ', 'åˆ†æ'],
        'å®‰å…¨éœ€æ±‚': ['å®‰å…¨', 'ç¨³å®š', 'ä¿éšœ', 'ä¿æŠ¤', 'å±é™©', 'é£é™©', 'å®³æ€•', 'æ‹…å¿ƒ', 'ç„¦è™‘', 'å‹åŠ›'],
        'å½’å±éœ€æ±‚': ['æœ‹å‹', 'ç¤¾äº¤', 'ç¾¤ä½“', 'ç¤¾åŒº', 'å½’å±', 'è®¤åŒ', 'æ¥å—', 'æ‹’ç»', 'å­¤ç‹¬', 'å¯‚å¯'],
        'æˆé•¿éœ€æ±‚': ['æˆé•¿', 'è¿›æ­¥', 'å‘å±•', 'æå‡', 'æ”¹å˜', 'æ”¹å–„', 'ä¼˜åŒ–', 'ç›®æ ‡', 'æ¢¦æƒ³', 'ç†æƒ³'],
        'å°Šé‡éœ€æ±‚': ['å°Šé‡', 'å°Šä¸¥', 'é¢å­', 'åèª‰', 'å£°èª‰', 'è¯„ä»·', 'æ‰¹è¯„', 'è¡¨æ‰¬', 'è®¤å¯', 'å¦å®š']
    }
    
    need_analysis = {}
    for need, keywords in psychological_needs.items():
        posts_with_need = analysis_data['clean_text'].apply(
            lambda x: any(keyword in x for keyword in keywords)
        ).sum()
        
        need_analysis[need] = {
            'posts': posts_with_need,
            'ratio': posts_with_need / len(analysis_data),
            'intensity': analysis_data['clean_text'].apply(
                lambda x: sum(x.count(keyword) for keyword in keywords)
            ).sum() / len(analysis_data)
        }
    
    psych_metrics['psychological_needs'] = need_analysis
    
    # ä¸»è¦å¿ƒç†éœ€æ±‚
    need_ratios = {need: data['ratio'] for need, data in need_analysis.items()}
    primary_needs = sorted(need_ratios.items(), key=lambda x: x[1], reverse=True)[:3]
    psych_metrics['primary_needs'] = dict(primary_needs)
    
    # 3. å¿ƒç†æ”¯æŒæ•ˆæœè¯„ä¼°
    support_indicators = {
        'advice_given': ['å»ºè®®', 'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦', 'æ–¹æ³•', 'æ­¥éª¤', 'å¦‚ä½•', 'æ€æ ·'],
        'comfort_provided': ['å®‰æ…°', 'é¼“åŠ±', 'æ”¯æŒ', 'ç†è§£', 'é™ªä¼´', 'å…±é¸£', 'æ¸©æš–', 'å…³å¿ƒ'],
        'solution_offered': ['è§£å†³', 'å¤„ç†', 'åº”å¯¹', 'é¢å¯¹', 'å…‹æœ', 'æ”¹å–„', 'è°ƒæ•´', 'æ”¹å˜'],
        'hope_inspired': ['å¸Œæœ›', 'æœªæ¥', 'æ˜å¤©', 'åŠ æ²¹', 'åšæŒ', 'åŠªåŠ›', 'æˆåŠŸ', 'ç¾å¥½']
    }
    
    support_analysis = {}
    for indicator, keywords in support_indicators.items():
        posts_with_support = analysis_data['clean_text'].apply(
            lambda x: any(keyword in x for keyword in keywords)
        ).sum()
        
        support_analysis[indicator] = {
            'posts': posts_with_support,
            'ratio': posts_with_support / len(analysis_data),
            'effectiveness': posts_with_support / max(1, emotion_analysis['negative']['posts_with'])
        }
    
    psych_metrics['support_analysis'] = support_analysis
    
    # ç»¼åˆå¿ƒç†æ”¯æŒæŒ‡æ•°
    support_scores = [data['ratio'] for data in support_analysis.values()]
    psych_metrics['support_index'] = np.mean(support_scores) if support_scores else 0
    
    # 4. è¡Œä¸ºæ¿€å‘åˆ†æ
    behavior_indicators = {
        'action_intent': ['è¦', 'æƒ³', 'æ‰“ç®—', 'è®¡åˆ’', 'å‡†å¤‡', 'å†³å®š', 'å°è¯•', 'å¼€å§‹'],
        'goal_setting': ['ç›®æ ‡', 'è®¡åˆ’', 'flag', 'æ‰“å¡', 'è®°å½•', 'åšæŒ', 'åŠªåŠ›', 'å¥‹æ–—'],
        'progress_sharing': ['åˆ†äº«', 'å‘Šè¯‰', 'æ±‡æŠ¥', 'æ›´æ–°', 'è¿›æ­¥', 'æˆæœ', 'æˆç»©', 'æ”¶è·'],
        'help_seeking': ['æ±‚åŠ©', 'å¸®å¿™', 'å¸®åŠ©', 'è¯·é—®', 'æ±‚é—®', 'å’¨è¯¢', 'è¯¢é—®', 'è¯·æ•™']
    }
    
    behavior_analysis = {}
    for behavior, keywords in behavior_indicators.items():
        posts_with_behavior = analysis_data['clean_text'].apply(
            lambda x: any(keyword in x for keyword in keywords)
        ).sum()
        
        behavior_analysis[behavior] = {
            'posts': posts_with_behavior,
            'ratio': posts_with_behavior / len(analysis_data),
            'engagement': posts_with_behavior / len(analysis_data) * 100
        }
    
    psych_metrics['behavior_analysis'] = behavior_analysis
    
    # è¡Œä¸ºæ¿€å‘æŒ‡æ•°
    behavior_ratios = [data['ratio'] for data in behavior_analysis.values()]
    psych_metrics['behavior_index'] = np.mean(behavior_ratios) if behavior_ratios else 0
    
    # 5. ç„¦è™‘ç®¡ç†åˆ†æ
    anxiety_terms = ['ç„¦è™‘', 'å‹åŠ›', 'ç´§å¼ ', 'æ‹…å¿ƒ', 'å®³æ€•', 'ææ…Œ', 'ä¸å®‰', 'å¿§è™‘']
    solution_terms = ['æ–¹æ³•', 'è§£å†³', 'ç¼“è§£', 'å‡å°‘', 'åº”å¯¹', 'å¤„ç†', 'è°ƒæ•´', 'æ”¹å–„']
    
    anxiety_posts = analysis_data['clean_text'].apply(
        lambda x: any(term in x for term in anxiety_terms)
    ).sum()
    
    solution_posts = analysis_data['clean_text'].apply(
        lambda x: any(term in x for term in solution_terms)
    ).sum()
    
    anxiety_solution_posts = analysis_data['clean_text'].apply(
        lambda x: any(at in x for at in anxiety_terms) and any(st in x for st in solution_terms)
    ).sum()
    
    psych_metrics['anxiety_management'] = {
        'anxiety_mentioned': anxiety_posts / len(analysis_data),
        'solutions_provided': solution_posts / len(analysis_data),
        'targeted_solutions': anxiety_solution_posts / max(1, anxiety_posts),
        'anxiety_coverage': anxiety_solution_posts / len(analysis_data)
    }
    
    print(f"âœ… å¢å¼ºå¿ƒç†åˆ†æå®Œæˆ")
    
    print(f"\nğŸ“Š æƒ…ç»ªè¾“å‡ºåˆ†æ:")
    emotion_output = psych_metrics['emotion_output']
    print(f"   æ•´ä½“æƒ…ç»ªå¼ºåº¦: ç§¯æ{emotion_output['overall_positive_intensity']:.2f}, æ¶ˆæ{emotion_output['overall_negative_intensity']:.2f}, ä¸­æ€§{emotion_output['overall_neutral_intensity']:.2f}")
    print(f"   å¼ºçƒˆæƒ…ç»ªå æ¯”: å¼ºçƒˆç§¯æ{emotion_output['strong_positive_ratio']:.1%}, å¼ºçƒˆæ¶ˆæ{emotion_output['strong_negative_ratio']:.1%}")
    emotion_balance = psych_metrics['emotion_balance']
    print(f"   æƒ…æ„Ÿå¹³è¡¡åº¦: {emotion_balance['balance_score']:.3f} (ä¸»å¯¼æƒ…ç»ª: {emotion_balance['dominant_emotion']})")
    print(f"   æƒ…ç»ªåˆ†å¸ƒ: ç§¯æ{emotion_balance['positive_ratio']:.1%}, æ¶ˆæ{emotion_balance['negative_ratio']:.1%}, ä¸­æ€§{1-emotion_balance['positive_ratio']-emotion_balance['negative_ratio']:.1%}")
    
    print(f"\nğŸ“Š å¿ƒç†éœ€æ±‚åˆ†æ:")
    primary_needs = psych_metrics['primary_needs']
    print(f"   ä¸»è¦å¿ƒç†éœ€æ±‚: {', '.join([f'{k}({v:.1%})' for k, v in list(primary_needs.items())[:3]])}")
    print(f"   å¿ƒç†æ”¯æŒæŒ‡æ•°: {psych_metrics['support_index']:.3f}")
    print(f"   è¡Œä¸ºæ¿€å‘æŒ‡æ•°: {psych_metrics['behavior_index']:.3f}")
    
    return psych_metrics

def calculate_enhanced_scores(content_metrics, comm_metrics, psych_metrics):
    """è®¡ç®—å¢å¼ºç‰ˆä¸‰ç»´è¯„åˆ†"""
    print(f"\nğŸ“Š è®¡ç®—å¢å¼ºç‰ˆä¸‰ç»´è¯„åˆ†...")
    
    scores = {}
    
    # 1. å†…å®¹ç»´åº¦è¯„åˆ† (0-100åˆ†)
    if content_metrics:
        content_score = 0
        
        # å†…å®¹è´¨é‡ (40åˆ†)
        quality_indicators = content_metrics.get('quality', {})
        quality_score = (
            quality_indicators.get('theme_diversity', 0) * 0.4 +
            quality_indicators.get('avg_length_score', 0) * 0.3 +
            quality_indicators.get('structure_score', 0) * 0.2 +
            quality_indicators.get('rationality_score', 0) * 0.1
        ) * 40
        
        # ä¸»é¢˜èšç„¦ (30åˆ†)
        theme_analysis = content_metrics.get('themes', {})
        # é¾™å¥³å¡”ç½—æ ¸å¿ƒä¸»é¢˜ï¼šå¡”ç½—å åœã€æƒ…æ„Ÿå’¨è¯¢ã€è¡ŒåŠ¨æŒ‡å¯¼
        core_themes = ['å¡”ç½—å åœ', 'æƒ…æ„Ÿå’¨è¯¢', 'è¡ŒåŠ¨æŒ‡å¯¼']
        core_theme_score = sum(
            theme_analysis.get(theme, {}).get('post_ratio', 0) for theme in core_themes
        ) / len(core_themes) * 30
        
        # å†…å®¹ç‰¹è‰² (30åˆ†)
        signature_match = content_metrics.get('signature_match', 0) * 30
        
        content_score = quality_score + core_theme_score + signature_match
    else:
        content_score = 0
    
    scores['å†…å®¹ç»´åº¦'] = content_score
    
    # 2. ä¼ æ’­ç»´åº¦è¯„åˆ† (0-100åˆ†)
    if comm_metrics:
        comm_score = 0
        
        # ä¼ æ’­å¹¿åº¦ (40åˆ†)
        coverage = comm_metrics.get('topic_coverage', 0)
        participant_count = comm_metrics.get('participant_count', 0)
        breadth_score = min(coverage * 20 + min(participant_count / 100, 1) * 20, 40)
        
        # ç”¨æˆ·å‚ä¸ (30åˆ†)
        active_users = comm_metrics.get('active_users', 0)
        engagement_score = min(active_users / 50 * 30, 30)
        
        # ä¼ æ’­æ½œåŠ› (30åˆ†)
        potential = comm_metrics.get('potential', 0)
        potential_score = potential * 30
        
        comm_score = breadth_score + engagement_score + potential_score
    else:
        comm_score = 0
    
    scores['ä¼ æ’­ç»´åº¦'] = comm_score
    
    # 3. å¿ƒç†ç»´åº¦è¯„åˆ† (0-100åˆ†)
    if psych_metrics:
        psych_score = 0
        
        # æƒ…æ„Ÿå¹³è¡¡ (30åˆ†)
        emotion_balance = psych_metrics.get('emotion_balance', {}).get('balance_score', 0)
        emotion_score = emotion_balance * 30
        
        # å¿ƒç†æ”¯æŒ (40åˆ†)
        support_index = psych_metrics.get('support_index', 0)
        support_score = support_index * 40
        
        # è¡Œä¸ºæ¿€å‘ (30åˆ†)
        behavior_index = psych_metrics.get('behavior_index', 0)
        behavior_score = behavior_index * 30
        
        psych_score = emotion_score + support_score + behavior_score
    else:
        psych_score = 0
    
    scores['å¿ƒç†ç»´åº¦'] = psych_score
    
    # 4. ç»¼åˆè¯„åˆ†
    total_score = (content_score * 0.4 + comm_score * 0.35 + psych_score * 0.25)
    scores['ç»¼åˆè¯„åˆ†'] = total_score
    
    # è¯„ä¼°ç­‰çº§
    if total_score >= 85:
        level = "ä¼˜ç§€"
    elif total_score >= 75:
        level = "è‰¯å¥½"
    elif total_score >= 60:
        level = "ä¸­ç­‰"
    else:
        level = "ä¸è¶³"
    
    scores['è¯„ä¼°ç­‰çº§'] = level
    
    # æ²»ç†å»ºè®®
    if total_score < 60:
        suggestion = "éœ€è¦å…¨é¢ä¼˜åŒ–ï¼Œé‡æ–°è¯„ä¼°å†…å®¹ç­–ç•¥å’Œç”¨æˆ·å®šä½"
    elif total_score < 75:
        suggestion = "éƒ¨åˆ†ç»´åº¦éœ€è¦æ”¹è¿›ï¼Œå»ºè®®ä¼˜åŒ–è–„å¼±ç¯èŠ‚"
    else:
        suggestion = "æ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ä¼˜åŠ¿é¢†åŸŸ"
    
    scores['æ²»ç†å»ºè®®'] = suggestion
    
    print(f"âœ… å¢å¼ºè¯„åˆ†è®¡ç®—å®Œæˆ")
    print(f"   å†…å®¹ç»´åº¦: {content_score:.1f}åˆ†")
    print(f"   ä¼ æ’­ç»´åº¦: {comm_score:.1f}åˆ†")
    print(f"   å¿ƒç†ç»´åº¦: {psych_score:.1f}åˆ†")
    print(f"   ç»¼åˆè¯„åˆ†: {total_score:.1f}åˆ† ({level})")
    
    return scores

# ======================================
# 3. å¯è§†åŒ–
# ======================================

def create_content_theme_chart(content_metrics, save_path="content_theme_distribution.png"):
    """åˆ›å»ºç»†åˆ†ä¸»é¢˜åˆ†å¸ƒå›¾ï¼ˆæ›´æœ‰æ„ä¹‰çš„å¯è§†åŒ–ï¼‰"""
    if not content_metrics:
        print("âš ï¸ ç¼ºå°‘å†…å®¹åˆ†ææ•°æ®")
        return
    
    # ä½¿ç”¨ç»†åˆ†ä¸»é¢˜æ•°æ®ï¼ˆæ›´æœ‰æ„ä¹‰ï¼‰
    detailed_themes = content_metrics.get('detailed_themes', {})
    
    if not detailed_themes:
        print("âš ï¸ ç¼ºå°‘ç»†åˆ†ä¸»é¢˜æ•°æ®")
        return
    
    # æŒ‰å æ¯”æ’åº
    sorted_themes = sorted(detailed_themes.items(), key=lambda x: x[1]['post_ratio'], reverse=True)
    theme_names = [t[0] for t in sorted_themes[:10]]  # åªæ˜¾ç¤ºå‰10ä¸ª
    theme_ratios = [t[1]['post_ratio'] * 100 for t in sorted_themes[:10]]
    
    # åˆ›å»ºå›¾è¡¨
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
    
    # 1. æŸ±çŠ¶å›¾ï¼ˆæŒ‰å æ¯”æ’åºï¼‰
    colors = plt.cm.viridis(np.linspace(0, 1, len(theme_names)))
    bars = ax1.barh(theme_names, theme_ratios, color=colors, alpha=0.8)
    ax1.set_xlabel('å æ¯” (%)', fontsize=12)
    ax1.set_title('ç»†åˆ†ä¸»é¢˜åˆ†å¸ƒï¼ˆå…·ä½“é—®é¢˜ç±»å‹ï¼‰', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3, axis='x')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, ratio in zip(bars, theme_ratios):
        if ratio > 0:
            ax1.text(ratio + 0.5, bar.get_y() + bar.get_height()/2, 
                    f'{ratio:.1f}%', va='center', fontsize=10, fontweight='bold')
    
    # 2. é¥¼å›¾ï¼ˆåªæ˜¾ç¤ºå æ¯”>5%çš„ä¸»é¢˜ï¼‰
    significant_themes = [(n, r) for n, r in zip(theme_names, theme_ratios) if r > 5]
    if significant_themes:
        sig_names = [t[0] for t in significant_themes]
        sig_ratios = [t[1] for t in significant_themes]
        sig_colors = colors[:len(sig_names)]
        
        wedges, texts, autotexts = ax2.pie(sig_ratios, labels=sig_names, autopct='%1.1f%%',
                                           colors=sig_colors, startangle=90)
        ax2.set_title('ä¸»è¦é—®é¢˜ç±»å‹å æ¯”', fontsize=14, fontweight='bold')
        
        # è°ƒæ•´æ ‡ç­¾å­—ä½“
        for autotext in autotexts:
            autotext.set_color('black')
            autotext.set_fontweight('bold')
            autotext.set_fontsize(9)
    else:
        ax2.text(0.5, 0.5, 'æ— æ˜¾è‘—ä¸»é¢˜', ha='center', va='center', 
                transform=ax2.transAxes, fontsize=12)
        ax2.set_title('ä¸»è¦é—®é¢˜ç±»å‹å æ¯”', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å·²ä¿å­˜å†…å®¹ä¸»é¢˜å æ¯”å›¾è¡¨: {save_path}")
    plt.show()

def create_communication_network(data_dict, save_path="communication_network.png"):
    """åˆ›å»ºä¼ æ’­ç½‘ç»œå›¾ï¼ˆç®€åŒ–ç‰ˆï¼šæ˜¾ç¤ºçƒ­é—¨å…³é”®è¯ï¼‰"""
    try:
        import networkx as nx
    except ImportError:
        print("âš ï¸ networkxæœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆä¼ æ’­ç½‘ç»œå›¾ï¼ˆæŸ±çŠ¶å›¾ï¼‰")
        # ä½¿ç”¨æŸ±çŠ¶å›¾ä»£æ›¿
        analysis_data = data_dict.get('analysis_posts', pd.DataFrame())
        if len(analysis_data) == 0:
            print("âš ï¸ æ— æ³•æ„å»ºä¼ æ’­ç½‘ç»œå›¾ï¼šæ•°æ®ä¸è¶³")
            return
        
        # æå–å…³é”®è¯
        keyword_counts = analysis_data['keyword'].value_counts().head(20) if 'keyword' in analysis_data.columns else pd.Series()
        
        if len(keyword_counts) == 0:
            print("âš ï¸ æ— æ³•æ„å»ºä¼ æ’­ç½‘ç»œå›¾ï¼šæ— å…³é”®è¯æ•°æ®")
            return
        
        fig, ax = plt.subplots(figsize=(12, 8))
        bars = ax.barh(keyword_counts.index[:15], keyword_counts.values[:15], color='#4ECDC4', alpha=0.8)
        ax.set_xlabel('è§†é¢‘æ•°é‡', fontsize=12)
        ax.set_title('ä¼ æ’­ç½‘ç»œï¼ˆçƒ­é—¨å…³é”®è¯ï¼‰', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='x')
        
        for bar, count in zip(bars, keyword_counts.values[:15]):
            ax.text(count + 0.1, bar.get_y() + bar.get_height()/2, 
                   f'{int(count)}', va='center', fontsize=9)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ å·²ä¿å­˜ä¼ æ’­ç½‘ç»œå›¾: {save_path}")
        plt.show()
        return
    
    analysis_data = data_dict.get('analysis_posts', pd.DataFrame())
    if len(analysis_data) == 0:
        print("âš ï¸ æ— æ³•æ„å»ºä¼ æ’­ç½‘ç»œå›¾ï¼šæ•°æ®ä¸è¶³")
        return
    
    G = nx.Graph()
    
    # æ”¶é›†èŠ‚ç‚¹å’Œè¾¹
    user_nodes_dict = {}
    keyword_nodes_dict = {}
    edges_list = []
    
    if 'user' in analysis_data.columns:
        user_counts = analysis_data['user'].value_counts()
        # åªé€‰æ‹©å‰20ä¸ªæ´»è·ƒç”¨æˆ·
        top_users = user_counts.head(20).index.tolist()
        for user in top_users:
            if pd.notna(user) and str(user).strip():
                user_nodes_dict[str(user)] = {
                    'weight': int(user_counts[user]),
                    'node_type': 'user'
                }
    
    if 'keyword' in analysis_data.columns:
        keyword_counts = analysis_data['keyword'].value_counts()
        top_keywords = keyword_counts.head(20).index.tolist()
        
        for keyword in top_keywords:
            if pd.notna(keyword) and str(keyword).strip():
                keyword_node = f"å…³é”®è¯:{keyword}"
                keyword_nodes_dict[keyword_node] = {
                    'weight': int(keyword_counts[keyword]),
                    'node_type': 'keyword'
                }
        
        # è¿æ¥ç”¨æˆ·å’Œå…³é”®è¯
        if user_nodes_dict:
            for idx, row in analysis_data.iterrows():
                user = str(row.get('user', ''))
                keyword = str(row.get('keyword', ''))
                if (pd.notna(row.get('user')) and pd.notna(row.get('keyword')) and
                    user in user_nodes_dict and f"å…³é”®è¯:{keyword}" in keyword_nodes_dict):
                    edges_list.append((user, f"å…³é”®è¯:{keyword}", {'weight': 1}))
    
    # ä¸€æ¬¡æ€§æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
    G.add_nodes_from([(node, attrs) for node, attrs in user_nodes_dict.items()])
    G.add_nodes_from([(node, attrs) for node, attrs in keyword_nodes_dict.items()])
    
    # æ·»åŠ è¾¹
    if edges_list:
        G.add_edges_from(edges_list)
    
    if len(G.nodes()) == 0:
        print("âš ï¸ æ— æ³•æ„å»ºç½‘ç»œå›¾ï¼šæ•°æ®ä¸è¶³")
        return
    
    # ç»˜åˆ¶ç½‘ç»œå›¾
    plt.figure(figsize=(14, 10))
    
    # ä½¿ç”¨springå¸ƒå±€
    pos = nx.spring_layout(G, k=1, iterations=50)
    
    # åŒºåˆ†èŠ‚ç‚¹ç±»å‹
    user_nodes = [n for n in G.nodes() if G.nodes[n].get('node_type') != 'keyword' and 'å…³é”®è¯:' not in n]
    keyword_nodes = [n for n in G.nodes() if G.nodes[n].get('node_type') == 'keyword' or 'å…³é”®è¯:' in n]
    
    # ç»˜åˆ¶è¾¹
    nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5, edge_color='gray')
    
    # ç»˜åˆ¶ç”¨æˆ·èŠ‚ç‚¹
    if user_nodes:
        node_sizes = [G.nodes[n].get('weight', 1) * 100 for n in user_nodes]
        nx.draw_networkx_nodes(G, pos, nodelist=user_nodes, node_color='#FF6B6B',
                              node_size=node_sizes, alpha=0.7, label='UPä¸»')
    
    # ç»˜åˆ¶å…³é”®è¯èŠ‚ç‚¹
    if keyword_nodes:
        keyword_sizes = [G.nodes[n].get('weight', 1) * 200 for n in keyword_nodes]
        nx.draw_networkx_nodes(G, pos, nodelist=keyword_nodes, node_color='#4ECDC4',
                              node_size=keyword_sizes, alpha=0.7, label='å…³é”®è¯')
    
    # åªæ ‡æ³¨é‡è¦èŠ‚ç‚¹ï¼ˆé¿å…è¿‡äºæ‹¥æŒ¤ï¼‰
    important_nodes = []
    if user_nodes:
        user_weights = {n: G.nodes[n].get('weight', 0) for n in user_nodes}
        top_users = sorted(user_weights.items(), key=lambda x: x[1], reverse=True)[:5]
        important_nodes.extend([n for n, _ in top_users])
    
    if keyword_nodes:
        keyword_weights = {n: G.nodes[n].get('weight', 0) for n in keyword_nodes}
        top_keywords = sorted(keyword_weights.items(), key=lambda x: x[1], reverse=True)[:5]
        important_nodes.extend([n for n, _ in top_keywords])
    
    labels = {n: n.replace('å…³é”®è¯:', '') if 'å…³é”®è¯:' in n else n for n in important_nodes}
    nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold')
    
    plt.title('ä¼ æ’­ç½‘ç»œå›¾\nï¼ˆUPä¸»-å…³é”®è¯å…³ç³»ç½‘ç»œï¼‰', fontsize=14, fontweight='bold')
    plt.legend(loc='upper right')
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å·²ä¿å­˜ä¼ æ’­ç½‘ç»œå›¾: {save_path}")
    plt.show()

def create_emotion_radar(psych_metrics, save_path="emotion_radar.png"):
    """åˆ›å»ºæƒ…ç»ªç±»å‹åˆ†å¸ƒå›¾ï¼ˆæ›´æœ‰æ„ä¹‰çš„å¯è§†åŒ–ï¼‰"""
    if not psych_metrics:
        print("âš ï¸ ç¼ºå°‘å¿ƒç†åˆ†ææ•°æ®")
        return
    
    # æå–å…·ä½“æƒ…ç»ªç±»å‹æ•°æ®ï¼ˆæ›´æœ‰æ„ä¹‰ï¼‰
    emotion_types = psych_metrics.get('emotion_types', {})
    emotion_type_ratios = emotion_types.get('ratios', {})
    
    if not emotion_type_ratios:
        print("âš ï¸ ç¼ºå°‘æƒ…ç»ªç±»å‹æ•°æ®")
        return
    
    # åˆ›å»ºå›¾è¡¨
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # 1. æƒ…ç»ªç±»å‹æŸ±çŠ¶å›¾ï¼ˆæŒ‰å æ¯”æ’åºï¼‰
    sorted_emotions = sorted(emotion_type_ratios.items(), key=lambda x: x[1], reverse=True)
    emotion_names = [e[0] for e in sorted_emotions]
    emotion_values = [e[1] * 100 for e in sorted_emotions]
    
    colors = plt.cm.Pastel1(np.linspace(0, 1, len(emotion_names)))
    bars = ax1.barh(emotion_names, emotion_values, color=colors, alpha=0.8)
    ax1.set_xlabel('å æ¯” (%)', fontsize=12)
    ax1.set_title('å…·ä½“æƒ…ç»ªç±»å‹åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3, axis='x')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, emotion_values):
        if value > 0:
            ax1.text(value + 1, bar.get_y() + bar.get_height()/2, 
                    f'{value:.1f}%', va='center', fontsize=10, fontweight='bold')
    
    # 2. æƒ…ç»ªç±»å‹é¥¼å›¾
    wedges, texts, autotexts = ax2.pie(emotion_values, labels=emotion_names, autopct='%1.1f%%',
                                       colors=colors, startangle=90)
    ax2.set_title('æƒ…ç»ªç±»å‹å æ¯”åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    
    # è°ƒæ•´æ ‡ç­¾å­—ä½“
    for autotext in autotexts:
        autotext.set_color('black')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(9)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å·²ä¿å­˜æƒ…ç»ªç±»å‹åˆ†å¸ƒå›¾: {save_path}")
    plt.show()

def create_interaction_patterns_chart(content_metrics, save_path="interaction_patterns.png"):
    """åˆ›å»ºäº’åŠ¨æ¨¡å¼åˆ†å¸ƒå›¾"""
    if not content_metrics or 'interaction_patterns' not in content_metrics:
        print("âš ï¸ ç¼ºå°‘äº’åŠ¨æ¨¡å¼æ•°æ®")
        return
    
    interaction_patterns = content_metrics['interaction_patterns']
    
    # è¿‡æ»¤æ‰å æ¯”å¤ªå°çš„æ¨¡å¼
    significant_patterns = {k: v for k, v in interaction_patterns.items() if v['ratio'] > 0.05}
    
    if not significant_patterns:
        print("âš ï¸ æ— æ˜¾è‘—äº’åŠ¨æ¨¡å¼æ•°æ®")
        return
    
    # æŒ‰å æ¯”æ’åº
    sorted_patterns = sorted(significant_patterns.items(), key=lambda x: x[1]['ratio'], reverse=True)
    pattern_names = [p[0] for p in sorted_patterns]
    pattern_ratios = [p[1]['ratio'] * 100 for p in sorted_patterns]
    
    # åˆ›å»ºå›¾è¡¨
    fig, ax = plt.subplots(figsize=(12, 6))
    
    colors = plt.cm.Set2(np.linspace(0, 1, len(pattern_names)))
    bars = ax.barh(pattern_names, pattern_ratios, color=colors, alpha=0.8)
    ax.set_xlabel('å æ¯” (%)', fontsize=12)
    ax.set_title('å†…å®¹äº’åŠ¨æ¨¡å¼åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='x')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, ratio in zip(bars, pattern_ratios):
        if ratio > 0:
            ax.text(ratio + 0.5, bar.get_y() + bar.get_height()/2, 
                    f'{ratio:.1f}%', va='center', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å·²ä¿å­˜äº’åŠ¨æ¨¡å¼åˆ†å¸ƒå›¾: {save_path}")
    plt.show()

def create_enhanced_visualization(scores, content_metrics=None, comm_metrics=None, 
                                 psych_metrics=None, data_dict=None,
                                 save_path="longnv_enhanced_assessment.png"):
    """åˆ›å»ºå¢å¼ºç‰ˆå¯è§†åŒ–å›¾è¡¨ï¼ˆåŒ…å«ä¸‰ç»´è¯„ä¼°ã€ä¸»é¢˜å æ¯”ã€ä¼ æ’­ç½‘ç»œã€æƒ…ç»ªé›·è¾¾ï¼‰"""
    
    # åˆ›å»ºç»¼åˆå¯è§†åŒ–
    fig = plt.figure(figsize=(20, 12))
    
    dimensions = ['å†…å®¹ç»´åº¦', 'ä¼ æ’­ç»´åº¦', 'å¿ƒç†ç»´åº¦']
    values = [scores.get(dim, 0) for dim in dimensions]
    total_score = scores.get('ç»¼åˆè¯„åˆ†', 0)
    
    # 1. ä¸‰ç»´è¯„ä¼°é›·è¾¾å›¾
    ax1 = plt.subplot(2, 3, 1, projection='polar')
    angles = np.linspace(0, 2 * np.pi, len(dimensions), endpoint=False).tolist()
    values_plot = values + values[:1]
    angles_plot = angles + angles[:1]
    
    ax1.plot(angles_plot, values_plot, 'o-', linewidth=2, color='#4ECDC4')
    ax1.fill(angles_plot, values_plot, alpha=0.25, color='#4ECDC4')
    ax1.set_xticks(angles)
    ax1.set_xticklabels(dimensions, fontsize=10)
    ax1.set_ylim(0, 100)
    ax1.set_yticks([25, 50, 75, 100])
    ax1.set_title('ä¸‰ç»´è¯„ä¼°é›·è¾¾å›¾', fontsize=12, fontweight='bold')
    
    # 2. å†…å®¹ä¸»é¢˜å æ¯”
    ax2 = plt.subplot(2, 3, 2)
    if content_metrics and 'themes' in content_metrics:
        themes = content_metrics['themes']
        theme_names = list(themes.keys())
        theme_ratios = [themes[theme]['post_ratio'] * 100 for theme in theme_names]
        
        # åªæ˜¾ç¤ºå æ¯”>5%çš„ä¸»é¢˜
        significant_themes = [(name, ratio) for name, ratio in zip(theme_names, theme_ratios) if ratio > 5]
        if significant_themes:
            names, ratios = zip(*sorted(significant_themes, key=lambda x: x[1], reverse=True))
            colors = plt.cm.Set3(np.linspace(0, 1, len(names)))
            bars = ax2.barh(names, ratios, color=colors, alpha=0.8)
            ax2.set_xlabel('å æ¯” (%)', fontsize=10)
            ax2.set_title('å†…å®¹ä¸»é¢˜å æ¯”', fontsize=12, fontweight='bold')
            ax2.grid(True, alpha=0.3, axis='x')
            for bar, ratio in zip(bars, ratios):
                ax2.text(ratio + 0.5, bar.get_y() + bar.get_height()/2, 
                        f'{ratio:.1f}%', va='center', fontsize=9, fontweight='bold')
        else:
            ax2.text(0.5, 0.5, 'æ— æ˜¾è‘—ä¸»é¢˜æ•°æ®', ha='center', va='center', 
                    transform=ax2.transAxes, fontsize=12)
            ax2.set_title('å†…å®¹ä¸»é¢˜å æ¯”', fontsize=12, fontweight='bold')
            ax2.axis('off')
    else:
        ax2.text(0.5, 0.5, 'ä¸»é¢˜æ•°æ®æœªæä¾›', ha='center', va='center', 
                transform=ax2.transAxes, fontsize=12)
        ax2.set_title('å†…å®¹ä¸»é¢˜å æ¯”', fontsize=12, fontweight='bold')
        ax2.axis('off')
    
    # 3. ç²‰ä¸æƒ…ç»ªé›·è¾¾å›¾
    ax3 = plt.subplot(2, 3, 3, projection='polar')
    if psych_metrics and 'emotion_analysis' in psych_metrics:
        emotion_analysis = psych_metrics['emotion_analysis']
        categories = ['ç§¯æ', 'æ¶ˆæ', 'ä¸­æ€§']
        values_emotion = [
            emotion_analysis.get('positive', {}).get('ratio', 0) * 100,
            emotion_analysis.get('negative', {}).get('ratio', 0) * 100,
            emotion_analysis.get('neutral', {}).get('ratio', 0) * 100
        ]
        
        angles_emo = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        values_emo_plot = values_emotion + values_emotion[:1]
        angles_emo_plot = angles_emo + angles_emo[:1]
        
        ax3.plot(angles_emo_plot, values_emo_plot, 'o-', linewidth=2, color='#FF6B6B')
        ax3.fill(angles_emo_plot, values_emo_plot, alpha=0.25, color='#FF6B6B')
        ax3.set_xticks(angles_emo)
        ax3.set_xticklabels(categories, fontsize=10)
        max_val = max(values_emotion) * 1.2 if max(values_emotion) > 0 else 100
        ax3.set_ylim(0, max_val)
        ax3.set_yticks([0, 25, 50, 75, 100])
        ax3.set_title('ç²‰ä¸æƒ…ç»ªé›·è¾¾å›¾', fontsize=12, fontweight='bold')
        ax3.grid(True, linestyle='--', alpha=0.5)
    else:
        ax3.text(0.5, 0.5, 'æƒ…ç»ªæ•°æ®æœªæä¾›', ha='center', va='center', 
                transform=ax3.transAxes, fontsize=12)
        ax3.set_title('ç²‰ä¸æƒ…ç»ªé›·è¾¾å›¾', fontsize=12, fontweight='bold')
        ax3.axis('off')
    
    # 4. ç»¼åˆè¯„åˆ†ä»ªè¡¨ç›˜
    ax4 = plt.subplot(2, 3, 4)
    ax4.set_xlim(-1.5, 1.5)
    ax4.set_ylim(-1.5, 1.5)
    ax4.set_aspect('equal')
    ax4.axis('off')
    
    # ç»˜åˆ¶èƒŒæ™¯åœ†ç¯
    circle = plt.Circle((0, 0), 1.0, color='lightgray', alpha=0.3)
    ax4.add_patch(circle)
    
    # ç»˜åˆ¶è¯„åˆ†å¼§
    score_angle = total_score / 100 * 180
    if total_score < 60:
        color = 'red'
    elif total_score < 75:
        color = 'orange'
    elif total_score < 85:
        color = 'yellowgreen'
    else:
        color = 'green'
    
    ax4.plot([0, 0.8 * np.sin(np.deg2rad(score_angle))], 
             [0, 0.8 * np.cos(np.deg2rad(score_angle))], 
             color=color, linewidth=4)
    
    ax4.text(0, 0, f'{total_score:.1f}', ha='center', va='center', 
             fontsize=24, fontweight='bold', color=color)
    ax4.text(0, -0.3, scores.get('è¯„ä¼°ç­‰çº§', 'æœªçŸ¥'), ha='center', va='center',
             fontsize=14, fontweight='bold', color=color)
    ax4.text(0, -0.5, 'ç»¼åˆè¯„åˆ†', ha='center', va='center',
             fontsize=10, color='gray')
    
    # 5. ä¼ æ’­ç½‘ç»œå›¾ï¼ˆæ˜¾ç¤ºçƒ­é—¨å…³é”®è¯ï¼‰
    ax5 = plt.subplot(2, 3, 5)
    if comm_metrics and 'hashtags' in comm_metrics:
        hashtags_data = comm_metrics['hashtags']
        top_hashtags = hashtags_data.get('top_hashtags', {})
        if top_hashtags:
            tags = list(top_hashtags.keys())[:8]
            counts = list(top_hashtags.values())[:8]
            colors_network = plt.cm.viridis(np.linspace(0, 1, len(tags)))
            bars = ax5.barh(tags, counts, color=colors_network, alpha=0.8)
            ax5.set_xlabel('ä½¿ç”¨æ¬¡æ•°', fontsize=10)
            ax5.set_title('ä¼ æ’­ç½‘ç»œï¼ˆçƒ­é—¨è¯é¢˜ï¼‰', fontsize=12, fontweight='bold')
            ax5.grid(True, alpha=0.3, axis='x')
            for bar, count in zip(bars, counts):
                ax5.text(count + 0.1, bar.get_y() + bar.get_height()/2, 
                        f'{int(count)}', va='center', fontsize=9)
        else:
            # ä½¿ç”¨å…³é”®è¯ä½œä¸ºæ›¿ä»£
            if data_dict and 'analysis_posts' in data_dict:
                analysis_data = data_dict['analysis_posts']
                if 'keyword' in analysis_data.columns:
                    keyword_counts = analysis_data['keyword'].value_counts().head(8)
                    if len(keyword_counts) > 0:
                        bars = ax5.barh(keyword_counts.index, keyword_counts.values, color='#4ECDC4', alpha=0.8)
                        ax5.set_xlabel('è§†é¢‘æ•°é‡', fontsize=10)
                        ax5.set_title('ä¼ æ’­ç½‘ç»œï¼ˆçƒ­é—¨å…³é”®è¯ï¼‰', fontsize=12, fontweight='bold')
                        ax5.grid(True, alpha=0.3, axis='x')
                        for bar, count in zip(bars, keyword_counts.values):
                            ax5.text(count + 0.1, bar.get_y() + bar.get_height()/2, 
                                    f'{int(count)}', va='center', fontsize=9)
                    else:
                        ax5.text(0.5, 0.5, 'æ— è¯é¢˜æ•°æ®', ha='center', va='center', 
                                transform=ax5.transAxes, fontsize=12)
                        ax5.set_title('ä¼ æ’­ç½‘ç»œ', fontsize=12, fontweight='bold')
                        ax5.axis('off')
                else:
                    ax5.text(0.5, 0.5, 'æ— è¯é¢˜æ•°æ®', ha='center', va='center', 
                            transform=ax5.transAxes, fontsize=12)
                    ax5.set_title('ä¼ æ’­ç½‘ç»œ', fontsize=12, fontweight='bold')
                    ax5.axis('off')
            else:
                ax5.text(0.5, 0.5, 'æ— è¯é¢˜æ ‡ç­¾æ•°æ®', ha='center', va='center', 
                        transform=ax5.transAxes, fontsize=12)
                ax5.set_title('ä¼ æ’­ç½‘ç»œ', fontsize=12, fontweight='bold')
                ax5.axis('off')
    else:
        # ä½¿ç”¨å…³é”®è¯ä½œä¸ºæ›¿ä»£
        if data_dict and 'analysis_posts' in data_dict:
            analysis_data = data_dict['analysis_posts']
            if 'keyword' in analysis_data.columns:
                keyword_counts = analysis_data['keyword'].value_counts().head(8)
                if len(keyword_counts) > 0:
                    bars = ax5.barh(keyword_counts.index, keyword_counts.values, color='#4ECDC4', alpha=0.8)
                    ax5.set_xlabel('è§†é¢‘æ•°é‡', fontsize=10)
                    ax5.set_title('ä¼ æ’­ç½‘ç»œï¼ˆçƒ­é—¨å…³é”®è¯ï¼‰', fontsize=12, fontweight='bold')
                    ax5.grid(True, alpha=0.3, axis='x')
                    for bar, count in zip(bars, keyword_counts.values):
                        ax5.text(count + 0.1, bar.get_y() + bar.get_height()/2, 
                                f'{int(count)}', va='center', fontsize=9)
                else:
                    ax5.text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', 
                            transform=ax5.transAxes, fontsize=12)
                    ax5.set_title('ä¼ æ’­ç½‘ç»œ', fontsize=12, fontweight='bold')
                    ax5.axis('off')
            else:
                ax5.text(0.5, 0.5, 'ä¼ æ’­æ•°æ®æœªæä¾›', ha='center', va='center', 
                        transform=ax5.transAxes, fontsize=12)
                ax5.set_title('ä¼ æ’­ç½‘ç»œ', fontsize=12, fontweight='bold')
                ax5.axis('off')
        else:
            ax5.text(0.5, 0.5, 'ä¼ æ’­æ•°æ®æœªæä¾›', ha='center', va='center', 
                    transform=ax5.transAxes, fontsize=12)
            ax5.set_title('ä¼ æ’­ç½‘ç»œ', fontsize=12, fontweight='bold')
            ax5.axis('off')
    
    # 6. å»ºè®®åŒºåŸŸ
    ax6 = plt.subplot(2, 3, 6)
    ax6.axis('off')
    
    suggestion = scores.get('æ²»ç†å»ºè®®', 'æš‚æ— å…·ä½“å»ºè®®')
    strengths = []
    weaknesses = []
    
    for dim, score in zip(dimensions, values):
        if score >= 75:
            strengths.append(f"{dim} ({score:.1f}åˆ†)")
        elif score < 60:
            weaknesses.append(f"{dim} ({score:.1f}åˆ†)")
    
    suggestion_text = f"ğŸ’¡ æ²»ç†å»ºè®®:\n\n{suggestion}\n\n"
    
    if strengths:
        suggestion_text += f"âœ… ä¼˜åŠ¿ç»´åº¦:\n" + "\n".join([f"  â€¢ {s}" for s in strengths]) + "\n\n"
    
    if weaknesses:
        suggestion_text += f"âš ï¸ å¾…æ”¹è¿›ç»´åº¦:\n" + "\n".join([f"  â€¢ {w}" for w in weaknesses])
    else:
        suggestion_text += f"âœ… å„ç»´åº¦è¡¨ç°å‡è¡¡ï¼Œæ— æ˜æ˜¾çŸ­æ¿"
    
    suggestion_text += f"\n\nğŸ“Š ç»¼åˆè¯„åˆ†: {total_score:.1f}åˆ† ({scores.get('è¯„ä¼°ç­‰çº§', 'æœªçŸ¥')})"
    suggestion_text += f"\nğŸ” è¯„ä¼°åŸºäºå¢å¼ºåˆ†ææ–¹æ³•"
    
    ax6.text(0.05, 0.95, suggestion_text, fontsize=10, va='top', ha='left',
             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7),
             transform=ax6.transAxes)
    
    plt.suptitle('UPä¸»ä¸‰ç»´è¯„ä¼°æŠ¥å‘Šï¼šé¾™å¥³å¡”ç½—\nï¼ˆå†…å®¹â€”ä¼ æ’­â€”å¿ƒç†ï¼‰', fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å·²ä¿å­˜ç»¼åˆå¯è§†åŒ–å›¾è¡¨: {save_path}")
    plt.show()
    
    # ç”Ÿæˆå•ç‹¬çš„è¯¦ç»†å›¾è¡¨ï¼ˆæ›´æœ‰æ„ä¹‰çš„å¯è§†åŒ–ï¼‰
    print("\nğŸ“Š ç”Ÿæˆè¯¦ç»†å¯è§†åŒ–å›¾è¡¨...")
    if content_metrics:
        create_content_theme_chart(content_metrics)  # ç»†åˆ†ä¸»é¢˜åˆ†å¸ƒ
    
    if psych_metrics:
        create_emotion_radar(psych_metrics)  # æƒ…ç»ªç±»å‹åˆ†å¸ƒ
    
    # åˆ›å»ºäº’åŠ¨æ¨¡å¼å¯è§†åŒ–
    if content_metrics and 'interaction_patterns' in content_metrics:
        create_interaction_patterns_chart(content_metrics)

def generate_enhanced_report(content_metrics, comm_metrics, psych_metrics, scores, data_summary, data_dict):
    """ç”Ÿæˆå¢å¼ºç‰ˆè¯„ä¼°æŠ¥å‘Šï¼ˆèšç„¦å®é™…æ´å¯Ÿï¼‰"""
    report = []
    report.append("=" * 80)
    report.append("UPä¸»å†…å®¹åˆ†ææŠ¥å‘Šï¼šé¾™å¥³å¡”ç½—")
    report.append("åŸºäºæ•°æ®çš„å†…å®¹å½¢å¼ã€ä¸»é¢˜åˆ†å¸ƒä¸æƒ…ç»ªè¾“å‡ºåˆ†æ")
    report.append("=" * 80)
    report.append("")
    
    # æ•°æ®æ¦‚å†µï¼ˆç®€åŒ–ï¼‰
    analysis_data = data_dict.get('analysis_posts', pd.DataFrame())
    report.append("ğŸ“Š æ•°æ®åŸºç¡€")
    report.append(f"   åˆ†ææ ·æœ¬: {data_summary.get('analysis_posts', 0)}æ¡è§†é¢‘")
    if 'attitudes_count' in analysis_data.columns:
        avg_views = pd.to_numeric(analysis_data['attitudes_count'], errors='coerce').mean()
        avg_comments = pd.to_numeric(analysis_data['comments_count'], errors='coerce').mean() if 'comments_count' in analysis_data.columns else 0
        report.append(f"   å¹³å‡æ’­æ”¾é‡: {avg_views:,.0f}")
        report.append(f"   å¹³å‡è¯„è®ºæ•°: {avg_comments:,.0f}")
    report.append("")
    
    # ä¸€ã€å†…å®¹å½¢å¼åˆ†æï¼ˆèšç„¦å®é™…å‘ç°ï¼‰
    if content_metrics and len(analysis_data) > 0:
        report.append("=" * 80)
        report.append("ä¸€ã€å†…å®¹å½¢å¼ç‰¹å¾åˆ†æ")
        report.append("=" * 80)
        report.append("")
        
        # æå–å…¸å‹æ ‡é¢˜ç¤ºä¾‹
        if 'text' in analysis_data.columns:
            sample_titles = analysis_data['text'].dropna().head(10).tolist()
            report.append("ğŸ“Œ å…¸å‹æ ‡é¢˜ç¤ºä¾‹ï¼ˆå‰10æ¡ï¼‰:")
            for i, title in enumerate(sample_titles, 1):
                report.append(f"   {i}. {title}")
            report.append("")
        
        # å†…å®¹å½¢å¼æ€»ç»“ï¼ˆç”¨è‡ªç„¶è¯­è¨€æè¿°ï¼‰
        form_features = content_metrics.get('form_features', {})
        style_dist = content_metrics.get('style_distribution', {})
        text_len = content_metrics.get('text_length', {})
        
        report.append("ğŸ“ å†…å®¹å½¢å¼ç‰¹å¾æ€»ç»“:")
        report.append("")
        
        # æ ‡é¢˜é•¿åº¦ç‰¹å¾
        avg_len = text_len.get('mean', 0)
        if avg_len < 15:
            length_desc = "æ ‡é¢˜åçŸ­ï¼Œç®€æ´ç›´æ¥"
        elif avg_len < 25:
            length_desc = "æ ‡é¢˜é•¿åº¦é€‚ä¸­ï¼Œä¿¡æ¯é‡é€‚ä¸­"
        elif avg_len < 35:
            length_desc = "æ ‡é¢˜è¾ƒé•¿ï¼Œä¿¡æ¯é‡ä¸°å¯Œ"
        else:
            length_desc = "æ ‡é¢˜å¾ˆé•¿ï¼Œä¿¡æ¯é‡å¤§"
        report.append(f"   1. æ ‡é¢˜é•¿åº¦: å¹³å‡{avg_len:.1f}å­—ç¬¦ï¼Œ{length_desc}")
        
        # æ ‡é¢˜ç»“æ„ç‰¹å¾
        brackets_ratio = form_features.get('brackets_ratio', 0)
        question_ratio = form_features.get('question_ratio', 0)
        if brackets_ratio > 0.8:
            report.append(f"   2. æ ‡é¢˜ç»“æ„: 94.9%çš„æ ‡é¢˜ä½¿ç”¨æ‹¬å·ã€ã€‘æ ‡è®°ä¸»é¢˜ï¼Œè¿™æ˜¯UPä¸»çš„æ˜¾è‘—ç‰¹å¾")
        if question_ratio > 0.5:
            report.append(f"   3. æé—®é£æ ¼: {question_ratio:.0%}çš„æ ‡é¢˜é‡‡ç”¨ç–‘é—®å¼ï¼Œå–„äºç”¨é—®é¢˜å¸å¼•è§‚ä¼—")
        if form_features.get('exclamation_ratio', 0) < 0.1:
            report.append(f"   4. æƒ…ç»ªè¡¨è¾¾: æ ‡é¢˜è¾ƒå°‘ä½¿ç”¨æ„Ÿå¹å·ï¼Œé£æ ¼ç›¸å¯¹ç†æ€§å…‹åˆ¶")
        
        # æ ‡é¢˜é£æ ¼
        top_style = max(style_dist.items(), key=lambda x: x[1]) if style_dist else None
        if top_style and top_style[1] > 0.5:
            style_names = {'ç–‘é—®å¼': 'ç–‘é—®å¼æ ‡é¢˜', 'è‚¯å®šå¼': 'è‚¯å®šå¼æ ‡é¢˜', 'æ¨èå¼': 'æ¨èå¼æ ‡é¢˜', 
                          'æƒ…æ„Ÿå¼': 'æƒ…æ„Ÿå¼æ ‡é¢˜', 'æ•°å­—å¼': 'æ•°å­—å¼æ ‡é¢˜'}
            report.append(f"   5. ä¸»è¦é£æ ¼: {style_names.get(top_style[0], top_style[0])}å {top_style[1]:.0%}ï¼Œå½¢æˆå›ºå®šçš„æ ‡é¢˜æ¨¡å¼")
        
        report.append("")
        
        # äº’åŠ¨æ¨¡å¼åˆ†æ
        interaction_patterns = content_metrics.get('interaction_patterns', {})
        if interaction_patterns:
            report.append("ğŸ¬ å†…å®¹äº’åŠ¨å½¢å¼:")
            sorted_patterns = sorted(interaction_patterns.items(), key=lambda x: x[1]['ratio'], reverse=True)
            for pattern_name, data in sorted_patterns[:3]:
                if data['ratio'] > 0.1:
                    pattern_desc = {
                        'è¾¹çœ‹è¾¹æµ‹': 'çŸ­è§†é¢‘äº’åŠ¨å åœï¼Œé«˜é»æ€§"è¾¹çœ‹è¾¹æµ‹"åœºæ™¯',
                        'çŸ­è§†é¢‘äº’åŠ¨å åœ': 'çŸ­è§†é¢‘å½¢å¼çš„äº’åŠ¨å åœå†…å®¹',
                        'æŠ½ç‰Œäº’åŠ¨': 'æŠ½ç‰Œé€‰ç‰Œäº’åŠ¨å½¢å¼',
                        'é—®é¢˜å¯¼å‘': 'ä»¥é—®é¢˜ä¸ºå¯¼å‘çš„å†…å®¹å½¢å¼',
                        'æ—¶é—´é™å®š': 'æ—¶é—´é™å®šçš„å åœå†…å®¹'
                    }
                    desc = pattern_desc.get(pattern_name, pattern_name)
                    report.append(f"   â€¢ {desc}: {data['ratio']:.0%}çš„å†…å®¹é‡‡ç”¨æ­¤å½¢å¼")
            report.append("")
        
        # æ ‡é¢˜æ¨¡å¼æ€»ç»“
        report.append("ğŸ’¡ æ ‡é¢˜æ¨¡å¼æ´å¯Ÿ:")
        if brackets_ratio > 0.9 and question_ratio > 0.5:
            report.append("   â€¢ é‡‡ç”¨ã€ä¸»é¢˜ã€‘+ é—®é¢˜çš„å›ºå®šæ ¼å¼ï¼Œæ—¢æ˜ç¡®äº†å†…å®¹ä¸»é¢˜ï¼Œåˆé€šè¿‡æé—®æ¿€å‘è§‚ä¼—å¥½å¥‡å¿ƒ")
            report.append("   â€¢ è¿™ç§æ ¼å¼æœ‰åŠ©äºåœ¨ä¿¡æ¯æµä¸­å¿«é€Ÿè¯†åˆ«ï¼ŒåŒæ—¶å¢å¼ºäº’åŠ¨æ„Ÿ")
        report.append("")
        
        # æ ¸å¿ƒä¸»é¢˜åˆ†æï¼ˆé‡æ–°ç»„ç»‡ï¼‰
        report.append("=" * 80)
        report.append("äºŒã€æ ¸å¿ƒä¸»é¢˜åˆ†å¸ƒåˆ†æ")
        report.append("=" * 80)
        report.append("")
        
        themes = content_metrics.get('themes', {})
        sorted_themes = sorted(themes.items(), key=lambda x: x[1]['post_ratio'], reverse=True)
        
        # åªæ˜¾ç¤ºé"å¡”ç½—å åœ"çš„ä¸»é¢˜ï¼ˆå› ä¸ºæ‰€æœ‰å†…å®¹éƒ½æ˜¯å¡”ç½—å åœï¼Œæ²¡æœ‰åŒºåˆ†æ„ä¹‰ï¼‰
        non_tarot_themes = [(t, d) for t, d in sorted_themes if t != 'å¡”ç½—å åœ']
        if non_tarot_themes:
            report.append("ğŸ“Š å†…å®¹ä¸»é¢˜åˆ†ç±»ï¼ˆæ’é™¤å¡”ç½—å åœå¤§ç±»ï¼‰:")
            for i, (theme, data) in enumerate(non_tarot_themes[:5], 1):
                if data['post_ratio'] > 0.05:
                    report.append(f"   {i}. {theme}: {data['post_ratio']:.1%} ({data['post_count']}ä¸ªè§†é¢‘)")
            report.append("")
        
        # ç»†åˆ†ä¸»é¢˜åˆ†æï¼ˆäºŒçº§ä¸»é¢˜ï¼‰
        detailed_themes = content_metrics.get('detailed_themes', {})
        if detailed_themes:
            report.append("ğŸ“Œ ç»†åˆ†ä¸»é¢˜åˆ†æï¼ˆå…·ä½“é—®é¢˜ç±»å‹ï¼‰:")
            sorted_detailed = sorted(detailed_themes.items(), key=lambda x: x[1]['post_ratio'], reverse=True)
            for i, (theme, data) in enumerate(sorted_detailed[:8], 1):
                if data['post_ratio'] > 0.05:
                    report.append(f"   {i}. \"{theme}\": {data['post_ratio']:.1%} ({data['post_count']}ä¸ªè§†é¢‘)")
            report.append("")
            report.append("   ğŸ’¡ è¯´æ˜ï¼šä»¥ä¸Šä¸ºå¡”ç½—å åœå†…å®¹ä¸­çš„å…·ä½“é—®é¢˜ç±»å‹ï¼Œåæ˜ äº†å—ä¼—çš„æ ¸å¿ƒå…³æ³¨ç‚¹")
            report.append("")
        
        # ç»†åˆ†ä¸»é¢˜ç‰¹ç‚¹æ€»ç»“
        if detailed_themes:
            top_detailed = max(detailed_themes.items(), key=lambda x: x[1]['post_ratio']) if detailed_themes else None
            if top_detailed and top_detailed[1]['post_ratio'] > 0.15:
                report.append(f"ğŸ’¡ æ ¸å¿ƒé—®é¢˜ç±»å‹:")
                report.append(f"   â€¢ æœ€å…³æ³¨çš„é—®é¢˜ç±»å‹æ˜¯\"{top_detailed[0]}\"ï¼ˆå æ¯”{top_detailed[1]['post_ratio']:.0%}ï¼‰")
                # åˆ—å‡ºå‰3ä¸ªä¸»è¦é—®é¢˜ç±»å‹
                top3_detailed = sorted(detailed_themes.items(), key=lambda x: x[1]['post_ratio'], reverse=True)[:3]
                if len(top3_detailed) >= 2:
                    top3_names = 'ã€'.join([f'"{t[0]}"' for t in top3_detailed])
                    report.append(f"   â€¢ ä¸»è¦é—®é¢˜ç±»å‹åŒ…æ‹¬ï¼š{top3_names}")
        report.append("")
        
        # å†…å®¹è¡¨è¾¾ç‰¹å¾ï¼ˆé‡æ–°æè¿°ï¼‰
        features = content_metrics.get('content_features', {})
        report.append("ğŸ“ å†…å®¹è¡¨è¾¾æ–¹å¼:")
        rational_ratio = features.get('has_rational', 0)
        action_ratio = features.get('has_action', 0)
        comfort_ratio = features.get('has_comfort', 0)
        
        if action_ratio > 0.2:
            report.append(f"   â€¢ æä¾›è¡ŒåŠ¨æŒ‡å—: {action_ratio:.0%}çš„å†…å®¹åŒ…å«å…·ä½“å»ºè®®å’Œæ–¹æ³•ï¼Œå…·æœ‰å®ç”¨æ€§")
        if rational_ratio < 0.2:
            report.append(f"   â€¢ ç†æ€§åˆ†æè¾ƒå°‘: ä»…{rational_ratio:.0%}çš„å†…å®¹åŒ…å«ç†æ€§åˆ†æï¼Œæ›´åå‘æ„Ÿæ€§è¡¨è¾¾")
        if comfort_ratio < 0.1:
            report.append(f"   â€¢ å¿ƒç†æ…°è—‰ä¸è¶³: ä»…{comfort_ratio:.0%}çš„å†…å®¹æä¾›å¿ƒç†æ…°è—‰ï¼Œå¯è€ƒè™‘å¢å¼ºæƒ…æ„Ÿæ”¯æŒ")
        
        report.append("")
    
    # ä¼ æ’­ç»´åº¦ï¼ˆç®€åŒ–ï¼Œåªæ˜¾ç¤ºå…³é”®æŒ‡æ ‡ï¼‰
    if comm_metrics:
        avg_views = comm_metrics.get('avg_views', 0)
        avg_comments = comm_metrics.get('avg_comments', 0)
        if avg_views > 0:
            report.append("=" * 80)
            report.append("ä¸‰ã€ä¼ æ’­è¡¨ç°")
            report.append("=" * 80)
            report.append("")
            report.append(f"ğŸ“Š äº’åŠ¨æ•°æ®:")
            report.append(f"   â€¢ å¹³å‡æ’­æ”¾é‡: {avg_views:,.0f}")
            report.append(f"   â€¢ å¹³å‡è¯„è®ºæ•°: {avg_comments:,.0f}")
            if avg_views > 100000:
                report.append(f"   â€¢ æ’­æ”¾è¡¨ç°: æ’­æ”¾é‡è¾ƒé«˜ï¼Œå†…å®¹å…·æœ‰è¾ƒå¥½çš„ä¼ æ’­åŠ›")
            if avg_comments > 1000:
                report.append(f"   â€¢ äº’åŠ¨è¡¨ç°: è¯„è®ºæ•°è¾ƒé«˜ï¼Œè§‚ä¼—å‚ä¸åº¦è‰¯å¥½")
            report.append("")
    
    # ä¸‰ã€æƒ…ç»ªè¾“å‡ºåˆ†æï¼ˆèšç„¦å‘ç°ï¼‰
    if psych_metrics:
        report.append("=" * 80)
        report.append("ä¸‰ã€æƒ…ç»ªè¾“å‡ºç‰¹å¾åˆ†æ")
        report.append("=" * 80)
        report.append("")
        
        emotion = psych_metrics.get('emotion_balance', {})
        emotion_output = psych_metrics.get('emotion_output', {})
        primary_needs = psych_metrics.get('primary_needs', {})
        
        positive_ratio = emotion.get('positive_ratio', 0)
        negative_ratio = emotion.get('negative_ratio', 0)
        neutral_ratio = 1 - positive_ratio - negative_ratio
        
        report.append("ğŸ“Š æƒ…ç»ªåˆ†å¸ƒç‰¹å¾:")
        report.append(f"   â€¢ ç§¯ææƒ…ç»ª: {positive_ratio:.0%}")
        report.append(f"   â€¢ æ¶ˆææƒ…ç»ª: {negative_ratio:.0%}")
        report.append(f"   â€¢ ä¸­æ€§æƒ…ç»ª: {neutral_ratio:.0%}")
        report.append("")
        
        # å…·ä½“æƒ…ç»ªç±»å‹åˆ†æ
        emotion_types = psych_metrics.get('emotion_types', {})
        if emotion_types and emotion_types.get('ratios'):
            report.append("ğŸ’« å…·ä½“æƒ…ç»ªç±»å‹åˆ†å¸ƒ:")
            sorted_emotion_types = sorted(emotion_types['ratios'].items(), key=lambda x: x[1], reverse=True)
            for emo_type, ratio in sorted_emotion_types[:5]:
                if ratio > 0.05:
                    posts_with = emotion_types.get('posts_with', {}).get(emo_type, 0)
                    report.append(f"   â€¢ {emo_type}: {ratio:.1%} (å‡ºç°åœ¨{posts_with}ä¸ªè§†é¢‘ä¸­)")
            report.append("")
        
        # æƒ…ç»ªç‰¹å¾æ€»ç»“
        report.append("ğŸ’¡ æƒ…ç»ªè¾“å‡ºç‰¹ç‚¹:")
        if neutral_ratio > 0.7:
            report.append(f"   â€¢ å†…å®¹ä»¥ä¸­æ€§æƒ…ç»ªä¸ºä¸»ï¼ˆ{neutral_ratio:.0%}ï¼‰ï¼Œé£æ ¼ç†æ€§å®¢è§‚ï¼Œåå‘åˆ†æè§£è¯»")
        if positive_ratio > 0.2 and negative_ratio < 0.1:
            report.append(f"   â€¢ ç§¯ææƒ…ç»ªæ˜æ˜¾å¤šäºæ¶ˆææƒ…ç»ªï¼Œæ•´ä½“æƒ…ç»ªåŸºè°ƒè¾ƒä¸ºæ­£é¢")
        if emotion_output.get('overall_positive_intensity', 0) < 0.2:
            report.append(f"   â€¢ æƒ…ç»ªè¡¨è¾¾è¾ƒä¸ºå…‹åˆ¶ï¼Œä¸åˆ»æ„æ¸²æŸ“å¼ºçƒˆæƒ…æ„Ÿï¼Œä¿æŒä¸“ä¸šå†·é™çš„è°ƒæ€§")
        
        # æƒ…ç»ªç±»å‹æ€»ç»“
        if emotion_types and emotion_types.get('ratios'):
            top_emotion_type = max(emotion_types['ratios'].items(), key=lambda x: x[1]) if emotion_types['ratios'] else None
            if top_emotion_type and top_emotion_type[1] > 0.2:
                emotion_desc = {
                    'å®‰æ…°': 'ä»¥å®‰æ…°ä¸é¼“åŠ±ä¸ºä¸»',
                    'é¼“åŠ±': 'ä»¥é¼“åŠ±å’Œæ”¯æŒä¸ºä¸»',
                    'æ”¯æŒ': 'ä»¥æ”¯æŒå’Œç¥ç¦ä¸ºä¸»',
                    'å…±æƒ…': 'ä»¥å…±æƒ…å’Œç†è§£ä¸ºä¸»',
                    'å¼•å¯¼': 'ä»¥å¼•å¯¼å’Œå»ºè®®ä¸ºä¸»',
                    'å¸Œæœ›': 'ä»¥å¸Œæœ›å’ŒæœŸå¾…ä¸ºä¸»'
                }
                desc = emotion_desc.get(top_emotion_type[0], '')
                if desc:
                    report.append(f"   â€¢ {desc}ï¼Œç²‰ä¸åœ¨è¯„è®ºåŒºå®Œæˆè‡ªæˆ‘æ•…äº‹è¡¥å…¨")
        report.append("")
        
        # è¯„è®ºäº’åŠ¨æ¨¡å¼åˆ†æ
        interaction_modes = psych_metrics.get('interaction_modes', {})
        if interaction_modes:
            report.append("ğŸ’¬ è¯„è®ºäº’åŠ¨æ¨¡å¼:")
            sorted_modes = sorted(interaction_modes.items(), key=lambda x: x[1]['ratio'], reverse=True)
            for mode_name, data in sorted_modes[:3]:
                if data['ratio'] > 0.3:
                    mode_desc = {
                        'è¾¹çœ‹è¾¹æµ‹': 'é«˜é»æ€§"è¾¹çœ‹è¾¹æµ‹"åœºæ™¯ï¼Œè§‚ä¼—å®æ—¶å‚ä¸',
                        'è‡ªæˆ‘è¡¥å…¨': 'ç²‰ä¸åœ¨è¯„è®ºåŒºå®Œæˆè‡ªæˆ‘æ•…äº‹è¡¥å…¨',
                        'äº’åŠ¨æé—®': 'ä»¥é—®é¢˜ä¸ºå¯¼å‘ï¼Œæ¿€å‘è§‚ä¼—äº’åŠ¨',
                        'æ—¶é—´é™å®š': 'æ—¶é—´é™å®šçš„å åœï¼Œå¢å¼ºç´§è¿«æ„Ÿ'
                    }
                    desc = mode_desc.get(mode_name, mode_name)
                    report.append(f"   â€¢ {desc}: {data['ratio']:.0%}çš„å†…å®¹é‡‡ç”¨æ­¤æ¨¡å¼")
            report.append("")
        
        # å—ä¼—å¿ƒç†éœ€æ±‚
        if primary_needs:
            report.append("ğŸ¯ å—ä¼—å¿ƒç†éœ€æ±‚æ´å¯Ÿ:")
            top_needs = sorted(primary_needs.items(), key=lambda x: x[1], reverse=True)[:3]
            for need, ratio in top_needs:
                if ratio > 0.1:
                    report.append(f"   â€¢ {need}: {ratio:.0%}çš„å†…å®¹ä¸æ­¤ç›¸å…³ï¼Œæ˜¯ä¸»è¦å—ä¼—éœ€æ±‚")
            report.append("")
        
        # æƒ…ç»ªä¸éœ€æ±‚çš„å…³è”åˆ†æ
        if primary_needs.get('æƒ…æ„Ÿéœ€æ±‚', 0) > 0.3 and positive_ratio > 0.2:
            report.append("ğŸ’« å†…å®¹-æƒ…ç»ªåŒ¹é…åˆ†æ:")
            report.append(f"   â€¢ æƒ…æ„Ÿéœ€æ±‚æ˜¯ä¸»è¦éœ€æ±‚ï¼ˆ{primary_needs.get('æƒ…æ„Ÿéœ€æ±‚', 0):.0%}ï¼‰ï¼Œå†…å®¹æ•´ä½“æƒ…ç»ªåç§¯æï¼Œ")
            report.append(f"     è¯´æ˜UPä¸»èƒ½å¤Ÿé€šè¿‡æ­£é¢æƒ…ç»ªæ»¡è¶³å—ä¼—çš„æƒ…æ„Ÿéœ€æ±‚")
        report.append("")
    
    # å››ã€ç»¼åˆæ´å¯Ÿä¸å»ºè®®ï¼ˆåŸºäºæ•°æ®çš„å…·ä½“å»ºè®®ï¼‰
    report.append("=" * 80)
    report.append("å››ã€å†…å®¹ç­–ç•¥æ´å¯Ÿä¸å»ºè®®")
    report.append("=" * 80)
    report.append("")
    
    # åŸºäºæ•°æ®çš„æ´å¯Ÿ
    themes = content_metrics.get('themes', {}) if content_metrics else {}
    form_features = content_metrics.get('form_features', {}) if content_metrics else {}
    emotion = psych_metrics.get('emotion_balance', {}) if psych_metrics else {}
    primary_needs = psych_metrics.get('primary_needs', {}) if psych_metrics else {}
    features = content_metrics.get('content_features', {}) if content_metrics else {}
    theme_diversity = content_metrics.get('quality', {}).get('theme_diversity', 0) if content_metrics else 0
    
    # å®šä¹‰å˜é‡ä¾›åç»­ä½¿ç”¨
    action_ratio = features.get('has_action', 0)
    comfort_ratio = features.get('has_comfort', 0)
    
    report.append("ğŸ’¡ æ ¸å¿ƒå‘ç°:")
    report.append("")
    
    # å†…å®¹å½¢å¼å‘ç°
    if form_features.get('brackets_ratio', 0) > 0.9:
        report.append("   1. æ ‡é¢˜æ ¼å¼é«˜åº¦ç»Ÿä¸€:")
        report.append("      â€¢ 94.9%ä½¿ç”¨ã€ã€‘æ ¼å¼ï¼Œè¿™æ˜¯UPä¸»çš„å“ç‰Œæ ‡è¯†")
        report.append("      â€¢ å»ºè®®ï¼šä¿æŒè¿™ä¸€æ ¼å¼çš„ä¸€è‡´æ€§ï¼Œå¼ºåŒ–å“ç‰Œè¯†åˆ«åº¦")
        report.append("")
    
    # ä¸»é¢˜èšç„¦å‘ç°
    tarot_ratio = themes.get('å¡”ç½—å åœ', {}).get('post_ratio', 0) if themes else 0
    emotion_ratio = themes.get('æƒ…æ„Ÿå’¨è¯¢', {}).get('post_ratio', 0) if themes else 0
    if tarot_ratio > 0.9:
        report.append("   2. å†…å®¹é«˜åº¦èšç„¦å¡”ç½—å åœ:")
        report.append(f"      â€¢ 96.2%çš„å†…å®¹å›´ç»•å¡”ç½—å åœï¼Œä¸“ä¸šé¢†åŸŸéå¸¸æ˜ç¡®")
        report.append("      â€¢ ä¼˜åŠ¿ï¼šåœ¨å‚ç›´é¢†åŸŸå»ºç«‹æƒå¨æ€§")
        if emotion_ratio > 0.2:
            report.append(f"      â€¢ åŒæ—¶å…³æ³¨æƒ…æ„Ÿå’¨è¯¢ï¼ˆ{emotion_ratio:.0%}ï¼‰ï¼Œå½¢æˆäº†å¡”ç½—+æƒ…æ„Ÿçš„å†…å®¹ç»„åˆ")
        report.append("")
    
    # æƒ…ç»ªç‰¹å¾å‘ç°
    neutral_ratio = 1 - emotion.get('positive_ratio', 0) - emotion.get('negative_ratio', 0)
    if neutral_ratio > 0.7:
        report.append("   3. æƒ…ç»ªè¡¨è¾¾ç†æ€§å…‹åˆ¶:")
        report.append(f"      â€¢ {neutral_ratio:.0%}çš„å†…å®¹ä¸ºä¸­æ€§æƒ…ç»ªï¼Œåå‘ç†æ€§åˆ†æè€Œéæƒ…æ„Ÿæ¸²æŸ“")
        report.append("      â€¢ ç‰¹ç‚¹ï¼šä¿æŒä¸“ä¸šå®¢è§‚çš„è°ƒæ€§ï¼Œé€‚åˆçŸ¥è¯†å‹å†…å®¹")
        report.append("      â€¢ å»ºè®®ï¼šå¯é€‚å½“å¢åŠ æƒ…æ„Ÿå…±é¸£å…ƒç´ ï¼Œæå‡å†…å®¹æ„ŸæŸ“åŠ›")
        report.append("")
    
    # å—ä¼—éœ€æ±‚å‘ç°
    if primary_needs.get('æƒ…æ„Ÿéœ€æ±‚', 0) > 0.3:
        report.append("   4. å—ä¼—ä¸»è¦éœ€æ±‚ä¸ºæƒ…æ„Ÿæ”¯æŒ:")
        report.append(f"      â€¢ æƒ…æ„Ÿéœ€æ±‚å æ¯”{primary_needs.get('æƒ…æ„Ÿéœ€æ±‚', 0):.0%}ï¼Œæ˜¯æ ¸å¿ƒå—ä¼—éœ€æ±‚")
        report.append("      â€¢ å»ºè®®ï¼šåœ¨ä¿æŒä¸“ä¸šæ€§çš„åŒæ—¶ï¼Œå¢åŠ æƒ…æ„Ÿå…³æ€€çš„è¡¨è¾¾ï¼Œæ»¡è¶³å—ä¼—å¿ƒç†éœ€æ±‚")
        report.append("")
    
    # å†…å®¹ç­–ç•¥å»ºè®®
    report.append("ğŸ“‹ å†…å®¹ä¼˜åŒ–å»ºè®®ï¼ˆåŸºäºæ•°æ®åˆ†æï¼‰:")
    report.append("")
    
    if action_ratio < 0.15:
        report.append("   1. å¢å¼ºå®ç”¨æ€§:")
        report.append(f"      â€¢ å½“å‰ä»…{action_ratio:.0%}çš„å†…å®¹åŒ…å«è¡ŒåŠ¨æŒ‡å—ï¼Œå¯å¢åŠ 'æ€ä¹ˆåš'ç±»å†…å®¹")
        report.append("      â€¢ å»ºè®®ï¼šåœ¨å¡”ç½—è§£è¯»åï¼Œæä¾›å…·ä½“çš„è¡ŒåŠ¨å»ºè®®ï¼Œæå‡å†…å®¹å®ç”¨ä»·å€¼")
        report.append("")
    
    theme_diversity = content_metrics.get('quality', {}).get('theme_diversity', 0) if content_metrics else 0
    if theme_diversity < 0.6:
        report.append("   2. é€‚åº¦æ‹“å±•ä¸»é¢˜:")
        report.append(f"      â€¢ å½“å‰ä¸»é¢˜å¤šæ ·æ€§ä¸º{theme_diversity:.0%}ï¼Œä¸»é¢˜ç›¸å¯¹é›†ä¸­")
        report.append("      â€¢ å»ºè®®ï¼šåœ¨ä¿æŒæ ¸å¿ƒä¼˜åŠ¿çš„åŒæ—¶ï¼Œå¯å°è¯•ç»“åˆå­¦ä¸šã€èŒä¸šç­‰è¯é¢˜")
        report.append("      â€¢ ä¾‹å¦‚ï¼š'å¡”ç½—çœ‹å­¦ä¸š'ã€'å¡”ç½—çœ‹äº‹ä¸š'ç­‰ï¼Œæ‰©å¤§å—ä¼—èŒƒå›´")
        report.append("")
    
    if comfort_ratio < 0.05:
        report.append("   3. å¢åŠ æƒ…æ„Ÿæ”¯æŒ:")
        report.append("      â€¢ å¿ƒç†æ…°è—‰å†…å®¹è¾ƒå°‘ï¼Œå¯é€‚å½“å¢åŠ æ¸©æš–ã€é¼“åŠ±çš„è¡¨è¾¾")
        report.append("      â€¢ å»ºè®®ï¼šåœ¨è§£è¯»ä¸­åŠ å…¥'åŠ æ²¹'ã€'æ”¯æŒ'ç­‰è¡¨è¾¾ï¼Œæå‡æƒ…æ„Ÿä»·å€¼")
        report.append("")
    
    report.append("=" * 80)
    report.append("")
    
    report_text = "\n".join(report)
    print(report_text)
    
    # ä¿å­˜æŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"longnv_enhanced_assessment_{timestamp}.txt"
    
    try:
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report_text)
        print(f"\nğŸ’¾ å·²ä¿å­˜å¢å¼ºç‰ˆè¯„ä¼°æŠ¥å‘Š: {report_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    return report_text

# ======================================
# ä¸»ç¨‹åº
# ======================================

def main():
    print("=" * 70)
    print("Bç«™UPä¸»ä¸‰ç»´è¯„ä¼°ï¼ˆå¢å¼ºç‰ˆï¼‰")
    print("åˆ†æå¯¹è±¡ï¼šé¾™å¥³å¡”ç½—")
    print("é’ˆå¯¹æ•°æ®æœ‰é™åœºæ™¯çš„ä¼˜åŒ–åˆ†æ")
    print("=" * 70)
    print()
    
    # é…ç½®å‚æ•°
    UP_NAME = "é¾™å¥³å¡”ç½—"
    
    # ä¼˜å…ˆä½¿ç”¨UPä¸»æœ¬äººçš„è§†é¢‘æ–‡ä»¶
    import glob
    import os
    up_video_files = glob.glob(f"{UP_NAME}_videos_*.csv")
    if up_video_files:
        # ä½¿ç”¨æœ€æ–°çš„UPä¸»è§†é¢‘æ–‡ä»¶
        latest_up_file = max(up_video_files, key=os.path.getmtime)
        DATA_FILE = latest_up_file
        print(f"ğŸ“ æ‰¾åˆ°UPä¸»è§†é¢‘æ–‡ä»¶: {DATA_FILE}")
        print(f"   å¦‚æœæ•°æ®ä¸è¶³ï¼Œå°†åˆå¹¶ä½¿ç”¨é€šç”¨æ•°æ®æ–‡ä»¶")
    else:
        # ä½¿ç”¨é€šç”¨æ•°æ®æ–‡ä»¶
        DATA_FILE = "bilibili_videos.csv"
        print(f"âš ï¸ æœªæ‰¾åˆ°UPä¸»ä¸“é—¨è§†é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨é€šç”¨æ•°æ®æ–‡ä»¶: {DATA_FILE}")
        print(f"   ğŸ’¡ æç¤ºï¼šè¿è¡Œ collect_up_videos.py å¯æ”¶é›†UPä¸»æœ¬äººçš„è§†é¢‘")
    
    # 1. åŠ è½½æ•°æ®
    print(f"ğŸ“¥ åŠ è½½UPä¸» '{UP_NAME}' ç›¸å…³æ•°æ®...")
    data_dict = load_up_data(DATA_FILE, UP_NAME)
    
    if data_dict is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
        return
    
    data_summary = data_dict.get('data_summary', {})
    print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
    print(f"   æ€»æ•°æ®é‡: {data_summary.get('total_posts', 0)}æ¡")
    print(f"   åˆ†ææ•°æ®: {data_summary.get('analysis_posts', 0)}æ¡")
    print(f"   äº’åŠ¨æ•°æ®å¯ç”¨: {data_summary.get('interaction_data_available', False)}")
    
    analysis_posts_count = data_summary.get('analysis_posts', 0)
    total_posts_count = data_summary.get('total_posts', 0)
    
    if analysis_posts_count < 50:
        print(f"\nâš ï¸ è­¦å‘Š: åˆ†ææ•°æ®è¾ƒå°‘ ({analysis_posts_count}æ¡)")
        print("   è¯„ä¼°ç»“æœä»…ä¾›å‚è€ƒï¼Œå»ºè®®æ”¶é›†æ›´å¤šæ•°æ®")
        print(f"\nğŸ’¡ æ•°æ®æ”¶é›†å»ºè®®:")
        print(f"   1. å½“å‰æ•°æ®é›†: æ€»æ•°æ®{total_posts_count}æ¡ï¼Œç›¸å…³æ•°æ®{analysis_posts_count}æ¡")
        print(f"   2. æ‰©å¤§å…³é”®è¯èŒƒå›´: åœ¨bilibili_data.pyä¸­å¢åŠ æ›´å¤šå…³é”®è¯ï¼ˆå¡”ç½—ã€å åœã€æƒ…æ„Ÿç­‰ï¼‰")
        print(f"   3. å¢åŠ ç¿»é¡µæ•°é‡: åœ¨bilibili_data.pyä¸­å¢åŠ pageså‚æ•°")
        print(f"   4. ç›®æ ‡æ•°æ®é‡: å»ºè®®è‡³å°‘200-500æ¡ç›¸å…³æ•°æ®")
    elif analysis_posts_count < 200:
        print(f"\nâš ï¸ æç¤º: åˆ†ææ•°æ®é‡ä¸­ç­‰ ({analysis_posts_count}æ¡)")
        print("   å»ºè®®æ”¶é›†æ›´å¤šæ•°æ®ä»¥æé«˜åˆ†æå‡†ç¡®æ€§å’Œå¯é æ€§")
        print(f"   ç›®æ ‡: è‡³å°‘200æ¡ä»¥ä¸Šç›¸å…³æ•°æ®å¯è·å¾—æ›´å¯é çš„ç»“æœ")
    
    # 2. å¢å¼ºä¸‰ç»´åˆ†æ
    print(f"\n{'='*40}")
    print(f"å¼€å§‹å¢å¼ºä¸‰ç»´åˆ†æ")
    print(f"{'='*40}")
    
    # å†…å®¹ç»´åº¦åˆ†æ
    content_metrics = enhanced_content_analysis(data_dict['analysis_posts'], UP_NAME)
    
    # ä¼ æ’­ç»´åº¦åˆ†æ
    comm_metrics = enhanced_communication_analysis(data_dict, UP_NAME)
    
    # å¿ƒç†ç»´åº¦åˆ†æ
    psych_metrics = enhanced_psychological_analysis(data_dict, UP_NAME)
    
    # 3. è®¡ç®—å¢å¼ºè¯„åˆ†
    print(f"\n{'='*40}")
    print(f"è®¡ç®—å¢å¼ºç‰ˆè¯„åˆ†")
    print(f"{'='*40}")
    
    scores = calculate_enhanced_scores(content_metrics, comm_metrics, psych_metrics)
    
    # 4. å¯è§†åŒ–
    print(f"\n{'='*40}")
    print(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    print(f"{'='*40}")
    
    create_enhanced_visualization(scores, content_metrics, comm_metrics, 
                                 psych_metrics, data_dict)
    
    # 5. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print(f"\n{'='*40}")
    print(f"ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
    print(f"{'='*40}")
    
    report = generate_enhanced_report(content_metrics, comm_metrics, psych_metrics, scores, data_summary, data_dict)
    
    print(f"\n{'='*80}")
    print(f"âœ… å†…å®¹åˆ†æå®Œæˆ!")
    print(f"{'='*80}")
    
    # 6. è¾“å‡ºæ¦‚è¦
    print(f"\nğŸ“Š åˆ†ææ¦‚è¦:")
    print(f"   â€¢ å·²åˆ†æ {data_summary.get('analysis_posts', 0)} ä¸ªè§†é¢‘")
    if comm_metrics and comm_metrics.get('avg_views', 0) > 0:
        print(f"   â€¢ å¹³å‡æ’­æ”¾é‡: {comm_metrics.get('avg_views', 0):,.0f}")
    print(f"   â€¢ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹å†…å®¹æˆ–ä¿å­˜çš„æŠ¥å‘Šæ–‡ä»¶")
    
    # ä¸å†æ˜¾ç¤ºåˆ†æ•°æ€»ç»“ï¼Œè¯¦ç»†åˆ†æå·²åœ¨æŠ¥å‘Šä¸­æä¾›
    
    # 7. ä¿å­˜ç»“æœ
    results = {
        'UPä¸»åç§°': UP_NAME,
        'è¯„ä¼°æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'è¯„ä¼°ç‰ˆæœ¬': 'å¢å¼ºç‰ˆï¼ˆé’ˆå¯¹æœ‰é™æ•°æ®ï¼‰',
        'æ•°æ®æ¦‚å†µ': data_summary,
        'å†…å®¹ç»´åº¦å¾—åˆ†': scores.get('å†…å®¹ç»´åº¦', 0),
        'ä¼ æ’­ç»´åº¦å¾—åˆ†': scores.get('ä¼ æ’­ç»´åº¦', 0),
        'å¿ƒç†ç»´åº¦å¾—åˆ†': scores.get('å¿ƒç†ç»´åº¦', 0),
        'ç»¼åˆè¯„åˆ†': scores.get('ç»¼åˆè¯„åˆ†', 0),
        'è¯„ä¼°ç­‰çº§': scores.get('è¯„ä¼°ç­‰çº§', 'æœªçŸ¥'),
        'æ²»ç†å»ºè®®': scores.get('æ²»ç†å»ºè®®', ''),
        'å†…å®¹ç»´åº¦è¯¦æƒ…': content_metrics,
        'ä¼ æ’­ç»´åº¦è¯¦æƒ…': comm_metrics,
        'å¿ƒç†ç»´åº¦è¯¦æƒ…': psych_metrics
    }
    
    import json
    results_file = f"longnv_enhanced_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    try:
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {results_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}")
    
    # 8. æ•°æ®æ”¶é›†å»ºè®®
    print(f"\nğŸ“‹ æ•°æ®æ”¶é›†å»ºè®®:")
    up_posts_count = len(data_dict.get('up_posts', pd.DataFrame()))
    
    if up_posts_count < 20:
        print(f"   âš ï¸ å½“å‰UPä¸»æœ¬äººè§†é¢‘ä»…{up_posts_count}æ¡ï¼Œå»ºè®®æ”¶é›†æ›´å¤šUPä¸»è§†é¢‘:")
        print(f"      è¿è¡Œå‘½ä»¤: python collect_up_videos.py")
        print(f"      è¿™å°†ä¸“é—¨æ”¶é›†UPä¸» '{UP_NAME}' çš„è§†é¢‘å†…å®¹")
    else:
        print(f"   âœ… UPä¸»æœ¬äººè§†é¢‘æ•°æ®å……è¶³ï¼ˆ{up_posts_count}æ¡ï¼‰")
    
    print(f"   1. å¦‚éœ€æ›´å¤šæ•°æ®ï¼Œè¿è¡Œ collect_up_videos.py æ”¶é›†UPä¸»æœ¬äººè§†é¢‘")
    print(f"   2. ç¡®ä¿æŠ“å–å®Œæ•´çš„äº’åŠ¨æ•°æ®ï¼ˆæ’­æ”¾é‡ã€å¼¹å¹•æ•°ç­‰ï¼‰")
    print(f"   3. å¯ä»¥è°ƒæ•´ collect_up_videos.py ä¸­çš„ max_pages å‚æ•°ä»¥è·å–æ›´å¤šè§†é¢‘")
    print(f"   4. å»ºè®®æ”¶é›†æ—¶é—´è·¨åº¦æ›´é•¿çš„è§†é¢‘ï¼Œäº†è§£å†…å®¹è¶‹åŠ¿å˜åŒ–")
    
    return results

if __name__ == "__main__":
    main()