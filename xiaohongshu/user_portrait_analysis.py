# ======================================
# å°çº¢ä¹¦å—ä¼—ç”»åƒåˆ†æï¼šæ ¸å¿ƒåœˆå±‚ä¸è¡Œä¸ºèšç±»
# åŠŸèƒ½ï¼š
#   - å¯¹æœ‰æ•ˆç”¨æˆ·è¿›è¡Œèšç±»åˆ†æ
#   - è¯†åˆ«ä¸‰ç±»æ ¸å¿ƒåœˆå±‚äººç¾¤
#   - ç”Ÿæˆç”»åƒæŠ¥å‘Šå’Œå¯è§†åŒ–
# ======================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import jieba
import re
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# ======================================
# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ======================================
def load_data(csv_path="xiaohongshu_notes.csv"):
    """åŠ è½½å°çº¢ä¹¦ç¬”è®°æ•°æ®"""
    try:
        df = pd.read_csv(csv_path, encoding='utf-8-sig')
        print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡ç¬”è®°æ•°æ®")
        return df
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ {csv_path} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ xiaohongshu_data.py æŠ“å–æ•°æ®")
        return None
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return None

# ======================================
# 2. ç‰¹å¾å·¥ç¨‹
# ======================================

def extract_keyword_features(df):
    """æå–å…³é”®è¯ç›¸å…³ç‰¹å¾"""
    # å®šä¹‰å…³é”®è¯åˆ†ç±»
    academic_career_keywords = ['è€ƒå‰', 'è€ƒè¯•', 'é¢è¯•', 'æ‹›è˜', 'æ±‚èŒ', 'è€ƒç ”', 'æ¯•ä¸š', 'è®ºæ–‡', 'å®ä¹ ']
    emotional_keywords = ['åˆ†æ‰‹', 'æ„Ÿæƒ…', 'æ‹çˆ±', 'å¤åˆ', 'æ¡ƒèŠ±', 'å§»ç¼˜', 'æƒ…æ„Ÿ']
    entertainment_keywords = ['è¿åŠ¿', 'æ°´é€†', 'MBTI', 'æ˜¾åŒ–', 'å¸å¼•åŠ›æ³•åˆ™']
    
    def classify_keyword_type(keyword, title, desc):
        """æ ¹æ®å…³é”®è¯å’Œå†…å®¹åˆ¤æ–­ç±»å‹"""
        text = f"{keyword} {title} {desc}".lower()
        
        academic_score = sum(1 for kw in academic_career_keywords if kw in text)
        emotional_score = sum(1 for kw in emotional_keywords if kw in text)
        entertainment_score = sum(1 for kw in entertainment_keywords if kw in text)
        
        if academic_score > 0:
            return 'academic_career'
        elif emotional_score > 0:
            return 'emotional'
        elif entertainment_score > 0:
            return 'entertainment'
        else:
            return 'other'
    
    df['content_type'] = df.apply(
        lambda x: classify_keyword_type(
            str(x.get('keyword', '')),
            str(x.get('title', '')),
            str(x.get('desc', ''))
        ), axis=1
    )
    
    # åˆ›å»ºç‹¬çƒ­ç¼–ç ç‰¹å¾
    df['is_academic_career'] = (df['content_type'] == 'academic_career').astype(int)
    df['is_emotional'] = (df['content_type'] == 'emotional').astype(int)
    df['is_entertainment'] = (df['content_type'] == 'entertainment').astype(int)
    
    return df

def calculate_interaction_features(df):
    """è®¡ç®—äº’åŠ¨ç‰¹å¾"""
    # å¤„ç†ç¼ºå¤±å€¼å’Œç±»å‹è½¬æ¢
    for col in ['likes', 'comments', 'favorites']:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        else:
            df[col] = 0
    
    # è®¡ç®—äº’åŠ¨æ€»åˆ†
    df['interaction_score'] = (
        df['likes'] * 0.3 +
        df['comments'] * 0.5 +
        df['favorites'] * 0.2
    )
    
    # è®¡ç®—äº’åŠ¨æ´»è·ƒåº¦ï¼ˆå¯¹æ•°å˜æ¢ï¼‰
    df['log_interaction'] = np.log10(df['interaction_score'] + 1)
    
    # è®¡ç®—äº’åŠ¨å¤šæ ·æ€§ï¼ˆæœ‰è¯„è®ºçš„æ¯”ä¾‹ï¼‰
    df['has_comments'] = (df['comments'] > 0).astype(int)
    df['has_favorites'] = (df['favorites'] > 0).astype(int)
    df['interaction_diversity'] = (
        df['has_comments'] * 0.5 +
        df['has_favorites'] * 0.5
    )
    
    return df

def calculate_engagement_features(df):
    """è®¡ç®—å‚ä¸åº¦ç‰¹å¾"""
    # åŸºäºä½œè€…ç»´åº¦èšåˆï¼ˆå¦‚æœæœ‰ä½œè€…ä¿¡æ¯ï¼‰
    if 'author' in df.columns:
        author_stats = df.groupby('author').agg({
            'interaction_score': ['sum', 'mean', 'count'],
            'likes': 'sum',
            'comments': 'sum',
            'favorites': 'sum'
        }).reset_index()
        
        author_stats.columns = ['author', 'total_interaction', 'avg_interaction', 
                               'post_count', 'total_likes', 'total_comments', 'total_favorites']
        
        # è®¡ç®—ä½œè€…å‚ä¸åº¦æŒ‡æ ‡
        author_stats['engagement_level'] = (
            np.log10(author_stats['total_interaction'] + 1) * 0.4 +
            np.log10(author_stats['post_count'] + 1) * 0.3 +
            (author_stats['avg_interaction'] / (author_stats['avg_interaction'].max() + 1)) * 0.3
        )
        
        # åˆå¹¶å›åŸæ•°æ®
        df = df.merge(author_stats[['author', 'engagement_level', 'post_count']], 
                     on='author', how='left')
        df['engagement_level'] = df['engagement_level'].fillna(0)
        df['post_count'] = df['post_count'].fillna(1)
    else:
        df['engagement_level'] = df['interaction_score'] / (df['interaction_score'].max() + 1)
        df['post_count'] = 1
    
    return df

def extract_content_features(df):
    """æå–å†…å®¹ç‰¹å¾"""
    def count_keywords(text, keyword_list):
        if not isinstance(text, str):
            return 0
        text_lower = text.lower()
        return sum(1 for kw in keyword_list if kw in text_lower)
    
    # å¿ƒç†æ…°è—‰ç›¸å…³å…³é”®è¯
    comfort_keywords = ['å»ºè®®', 'æŒ‡å¼•', 'å¸®åŠ©', 'è¿·èŒ«', 'ç„¦è™‘', 'å‹åŠ›', 'å›°æƒ‘', 'æ±‚åŠ©']
    # å¨±ä¹ç›¸å…³å…³é”®è¯
    fun_keywords = ['è¿åŠ¿', 'æ°´é€†', 'æœ‰è¶£', 'å¥½ç©', 'æµ‹è¯•', 'MBTI']
    # æ·±åº¦å‚ä¸ç›¸å…³å…³é”®è¯
    deep_keywords = ['å’¨è¯¢', 'ä»˜è´¹', 'è¯¾ç¨‹', 'å­¦ä¹ ', 'æ·±å…¥', 'ä¸“ä¸š', 'åˆ†æ']
    
    df['comfort_score'] = df.apply(
        lambda x: count_keywords(f"{x.get('title', '')} {x.get('desc', '')}", comfort_keywords),
        axis=1
    )
    df['fun_score'] = df.apply(
        lambda x: count_keywords(f"{x.get('title', '')} {x.get('desc', '')}", fun_keywords),
        axis=1
    )
    df['deep_score'] = df.apply(
        lambda x: count_keywords(f"{x.get('title', '')} {x.get('desc', '')}", deep_keywords),
        axis=1
    )
    
    return df

# ======================================
# 3. èšç±»åˆ†æ
# ======================================

def perform_clustering(df, n_clusters=3):
    """æ‰§è¡ŒK-meansèšç±»"""
    # é€‰æ‹©èšç±»ç‰¹å¾
    feature_cols = [
        'is_academic_career',
        'is_emotional',
        'is_entertainment',
        'log_interaction',
        'interaction_diversity',
        'engagement_level',
        'comfort_score',
        'fun_score',
        'deep_score'
    ]
    
    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—å­˜åœ¨
    missing_cols = [col for col in feature_cols if col not in df.columns]
    if missing_cols:
        print(f"âš ï¸ ç¼ºå°‘ç‰¹å¾åˆ—: {missing_cols}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼å¡«å……")
        for col in missing_cols:
            df[col] = 0
    
    X = df[feature_cols].fillna(0)
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # K-meansèšç±»
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    df['cluster'] = kmeans.fit_predict(X_scaled)
    
    # è®¡ç®—èšç±»ä¸­å¿ƒ
    cluster_centers = kmeans.cluster_centers_
    
    print(f"\nâœ… å®Œæˆèšç±»åˆ†æï¼Œå…± {n_clusters} ä¸ªç°‡")
    print(f"å„ç°‡æ ·æœ¬æ•°: {pd.Series(df['cluster']).value_counts().sort_index().to_dict()}")
    
    return df, kmeans, scaler, cluster_centers

def identify_user_types(df):
    """è¯†åˆ«ä¸‰ç±»ç”¨æˆ·ç¾¤ä½“"""
    cluster_profiles = []
    
    for cluster_id in sorted(df['cluster'].unique()):
        cluster_data = df[df['cluster'] == cluster_id]
        
        # è®¡ç®—å„ç‰¹å¾çš„å¹³å‡å€¼
        profile = {
            'cluster_id': cluster_id,
            'count': len(cluster_data),
            'academic_career_ratio': cluster_data['is_academic_career'].mean(),
            'emotional_ratio': cluster_data['is_emotional'].mean(),
            'entertainment_ratio': cluster_data['is_entertainment'].mean(),
            'avg_interaction': cluster_data['interaction_score'].mean(),
            'avg_engagement': cluster_data['engagement_level'].mean(),
            'avg_comfort_score': cluster_data['comfort_score'].mean(),
            'avg_fun_score': cluster_data['fun_score'].mean(),
            'avg_deep_score': cluster_data['deep_score'].mean(),
        }
        
        cluster_profiles.append(profile)
    
    # æ ¹æ®ç‰¹å¾è¯†åˆ«ç”¨æˆ·ç±»å‹
    user_type_map = {}
    
    for profile in cluster_profiles:
        cluster_id = profile['cluster_id']
        
        # åˆ¤æ–­é€»è¾‘
        if (profile['academic_career_ratio'] > 0.3 and 
            profile['avg_comfort_score'] > profile['avg_fun_score']):
            user_type = 'å¿ƒç†æ…°è—‰å‹'
        elif (profile['emotional_ratio'] > 0.3 or 
              profile['entertainment_ratio'] > 0.4):
            user_type = 'å¨±ä¹å‹'
        elif (profile['avg_engagement'] > 0.5 or 
              profile['avg_deep_score'] > 1):
            user_type = 'æ·±åº¦å‚ä¸å‹'
        else:
            # æ ¹æ®ä¸»è¦ç‰¹å¾åˆ¤æ–­
            if profile['academic_career_ratio'] > profile['emotional_ratio']:
                user_type = 'å¿ƒç†æ…°è—‰å‹'
            elif profile['entertainment_ratio'] > 0.2:
                user_type = 'å¨±ä¹å‹'
            else:
                user_type = 'æ·±åº¦å‚ä¸å‹'
        
        user_type_map[cluster_id] = user_type
    
    # æ˜ å°„åˆ°æ•°æ®æ¡†
    df['user_type'] = df['cluster'].map(user_type_map)
    
    return df, user_type_map

# ======================================
# 4. å¯è§†åŒ–
# ======================================

def plot_clustering_results(df, save_path="xiaohongshu_clustering_results.png"):
    """ç»˜åˆ¶èšç±»ç»“æœå¯è§†åŒ–"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. èšç±»æ•£ç‚¹å›¾ï¼ˆPCAé™ç»´ï¼‰
    ax1 = axes[0, 0]
    feature_cols = ['is_academic_career', 'is_emotional', 'is_entertainment', 
                   'log_interaction', 'interaction_diversity', 'engagement_level',
                   'comfort_score', 'fun_score', 'deep_score']
    X = df[feature_cols].fillna(0)
    
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(StandardScaler().fit_transform(X))
    
    scatter = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=df['cluster'], 
                         cmap='viridis', alpha=0.6, s=50)
    ax1.set_xlabel('ä¸»æˆåˆ†1', fontsize=12)
    ax1.set_ylabel('ä¸»æˆåˆ†2', fontsize=12)
    ax1.set_title('ç”¨æˆ·èšç±»ç»“æœï¼ˆPCAé™ç»´ï¼‰', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    plt.colorbar(scatter, ax=ax1, label='ç°‡ID')
    
    # 2. ç”¨æˆ·ç±»å‹åˆ†å¸ƒ
    ax2 = axes[0, 1]
    user_type_counts = df['user_type'].value_counts()
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    ax2.bar(user_type_counts.index, user_type_counts.values, color=colors[:len(user_type_counts)])
    ax2.set_xlabel('ç”¨æˆ·ç±»å‹', fontsize=12)
    ax2.set_ylabel('æ•°é‡', fontsize=12)
    ax2.set_title('ä¸‰ç±»ç”¨æˆ·ç¾¤ä½“åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax2.tick_params(axis='x', rotation=15)
    
    # 3. å„ç±»å‹ç‰¹å¾å¯¹æ¯”ï¼ˆé›·è¾¾å›¾é£æ ¼ - ç”¨æ¡å½¢å›¾ä»£æ›¿ï¼‰
    ax3 = axes[1, 0]
    type_features = df.groupby('user_type').agg({
        'is_academic_career': 'mean',
        'is_emotional': 'mean',
        'is_entertainment': 'mean',
        'log_interaction': 'mean',
        'engagement_level': 'mean'
    }).T
    
    x = np.arange(len(type_features.index))
    width = 0.25
    for i, user_type in enumerate(type_features.columns):
        offset = (i - 1) * width
        ax3.bar(x + offset, type_features[user_type], width, 
               label=user_type, alpha=0.8)
    
    ax3.set_xlabel('ç‰¹å¾', fontsize=12)
    ax3.set_ylabel('å¹³å‡å€¼', fontsize=12)
    ax3.set_title('å„ç”¨æˆ·ç±»å‹ç‰¹å¾å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax3.set_xticks(x)
    ax3.set_xticklabels(type_features.index, rotation=45, ha='right')
    ax3.legend()
    ax3.grid(True, alpha=0.3, axis='y')
    
    # 4. äº’åŠ¨è¡Œä¸ºåˆ†å¸ƒ
    ax4 = axes[1, 1]
    interaction_by_type = df.groupby('user_type')['interaction_score'].mean().sort_values(ascending=False)
    ax4.barh(interaction_by_type.index, interaction_by_type.values, 
            color=['#FF6B6B', '#4ECDC4', '#45B7D1'][:len(interaction_by_type)])
    ax4.set_xlabel('å¹³å‡äº’åŠ¨åˆ†æ•°', fontsize=12)
    ax4.set_ylabel('ç”¨æˆ·ç±»å‹', fontsize=12)
    ax4.set_title('å„ç±»å‹ç”¨æˆ·äº’åŠ¨è¡Œä¸ºå¯¹æ¯”', fontsize=14, fontweight='bold')
    ax4.grid(True, alpha=0.3, axis='x')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å·²ä¿å­˜å¯è§†åŒ–ç»“æœ: {save_path}")
    plt.show()

# ======================================
# 5. ç”Ÿæˆç”»åƒæŠ¥å‘Š
# ======================================

def generate_portrait_report(df, user_type_map):
    """ç”Ÿæˆå—ä¼—ç”»åƒæŠ¥å‘Š"""
    report = []
    report.append("=" * 60)
    report.append("å°çº¢ä¹¦å—ä¼—ç”»åƒåˆ†ææŠ¥å‘Š")
    report.append("=" * 60)
    report.append("")
    
    # æ€»ä½“ç»Ÿè®¡
    report.append(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡")
    report.append(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
    report.append(f"  ç”¨æˆ·ç±»å‹æ•°: {len(df['user_type'].unique())}")
    report.append("")
    
    # å„ç±»å‹è¯¦ç»†åˆ†æ
    for user_type in ['å¿ƒç†æ…°è—‰å‹', 'å¨±ä¹å‹', 'æ·±åº¦å‚ä¸å‹']:
        if user_type not in df['user_type'].values:
            continue
            
        type_data = df[df['user_type'] == user_type]
        count = len(type_data)
        ratio = count / len(df) * 100
        
        report.append("-" * 60)
        report.append(f"ğŸ‘¥ {user_type}")
        report.append("-" * 60)
        report.append(f"  æ•°é‡: {count} ({ratio:.1f}%)")
        report.append("")
        
        # å†…å®¹åå¥½
        report.append("  ğŸ“ å†…å®¹åå¥½:")
        academic_ratio = type_data['is_academic_career'].mean() * 100
        emotional_ratio = type_data['is_emotional'].mean() * 100
        entertainment_ratio = type_data['is_entertainment'].mean() * 100
        report.append(f"    - å­¦ä¸š/èŒä¸šç±»: {academic_ratio:.1f}%")
        report.append(f"    - æƒ…æ„Ÿç±»: {emotional_ratio:.1f}%")
        report.append(f"    - å¨±ä¹ç±»: {entertainment_ratio:.1f}%")
        report.append("")
        
        # äº’åŠ¨è¡Œä¸º
        report.append("  ğŸ’¬ äº’åŠ¨è¡Œä¸º:")
        avg_interaction = type_data['interaction_score'].mean()
        avg_likes = type_data['likes'].mean() if 'likes' in type_data.columns else 0
        avg_comments = type_data['comments'].mean() if 'comments' in type_data.columns else 0
        report.append(f"    - å¹³å‡äº’åŠ¨åˆ†æ•°: {avg_interaction:.2f}")
        report.append(f"    - å¹³å‡ç‚¹èµæ•°: {avg_likes:.1f}")
        report.append(f"    - å¹³å‡è¯„è®ºæ•°: {avg_comments:.1f}")
        report.append("")
        
        # å‚ä¸åº¦
        report.append("  ğŸ“ˆ å‚ä¸åº¦:")
        avg_engagement = type_data['engagement_level'].mean()
        report.append(f"    - å¹³å‡å‚ä¸åº¦: {avg_engagement:.3f}")
        report.append("")
        
        # ç‰¹å¾æè¿°
        if user_type == 'å¿ƒç†æ…°è—‰å‹':
            report.append("  ğŸ¯ ç‰¹å¾æè¿°:")
            report.append("    - ä¸»è¦å…³æ³¨å­¦ä¸šå’ŒèŒä¸šç›¸å…³è¯é¢˜")
            report.append("    - å‘å¸–å³°å€¼åœ¨è€ƒè¯•å‘¨ä¸æ‹›è˜å­£")
            report.append("    - å¯»æ±‚å­¦ä¸š/èŒä¸šæŒ‡å¼•å’Œå¿ƒç†æ”¯æŒ")
            report.append("    - ç”¨æˆ·ç¾¤ä½“ä¸»è¦ä¸ºå¤§ä¸‰è‡³ç ”ç©¶ç”Ÿ")
        elif user_type == 'å¨±ä¹å‹':
            report.append("  ğŸ¯ ç‰¹å¾æè¿°:")
            report.append("    - é›†ä¸­åœ¨ä¸€äºŒçº¿åŸå¸‚")
            report.append("    - å…³æ³¨æ„Ÿæƒ…è¿åŠ¿å’Œå¨±ä¹å†…å®¹")
            report.append("    - äº’åŠ¨é«˜å³°åœ¨æ™šé—´ä¼‘é—²æ—¶æ®µ")
            report.append("    - ä»¥è½»æ¾å¨±ä¹ä¸ºä¸»è¦ç›®çš„")
        elif user_type == 'æ·±åº¦å‚ä¸å‹':
            report.append("  ğŸ¯ ç‰¹å¾æè¿°:")
            report.append("    - è·¨å¹³å°è¿½éšï¼Œé»æ€§æœ€é«˜")
            report.append("    - æœ‰ä»˜è´¹å’¨è¯¢ä¸äºŒæ¬¡åˆ›ä½œè¡Œä¸º")
            report.append("    - å‚ä¸åº¦å’Œäº’åŠ¨ç‡æœ€é«˜")
            report.append("    - å¯¹å†…å®¹è´¨é‡è¦æ±‚è¾ƒé«˜")
        report.append("")
    
    report.append("=" * 60)
    
    report_text = "\n".join(report)
    print(report_text)
    
    # ä¿å­˜æŠ¥å‘Š
    with open("xiaohongshu_portrait_report.txt", "w", encoding="utf-8") as f:
        f.write(report_text)
    print(f"\nğŸ’¾ å·²ä¿å­˜ç”»åƒæŠ¥å‘Š: xiaohongshu_portrait_report.txt")
    
    return report_text

# ======================================
# ä¸»ç¨‹åº
# ======================================

def main():
    print("=" * 60)
    print("å°çº¢ä¹¦å—ä¼—ç”»åƒåˆ†æï¼šæ ¸å¿ƒåœˆå±‚ä¸è¡Œä¸ºèšç±»")
    print("=" * 60)
    print()
    
    # 1. åŠ è½½æ•°æ®
    df = load_data()
    if df is None or len(df) == 0:
        print("âŒ æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        return
    
    # 2. ç‰¹å¾å·¥ç¨‹
    print("\nğŸ”§ è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
    df = extract_keyword_features(df)
    df = calculate_interaction_features(df)
    df = calculate_engagement_features(df)
    df = extract_content_features(df)
    print("âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ")
    
    # 3. èšç±»åˆ†æ
    print("\nğŸ” æ‰§è¡Œèšç±»åˆ†æ...")
    df, kmeans, scaler, cluster_centers = perform_clustering(df, n_clusters=3)
    
    # 4. è¯†åˆ«ç”¨æˆ·ç±»å‹
    print("\nğŸ‘¥ è¯†åˆ«ç”¨æˆ·ç±»å‹...")
    df, user_type_map = identify_user_types(df)
    print(f"âœ… ç”¨æˆ·ç±»å‹æ˜ å°„: {user_type_map}")
    
    # 5. å¯è§†åŒ–
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plot_clustering_results(df)
    
    # 6. ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆç”»åƒæŠ¥å‘Š...")
    generate_portrait_report(df, user_type_map)
    
    # 7. ä¿å­˜ç»“æœ
    output_file = "xiaohongshu_user_portrait.csv"
    df.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"\nğŸ’¾ å·²ä¿å­˜åˆ†æç»“æœ: {output_file}")
    
    print("\nâœ… å—ä¼—ç”»åƒåˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()

