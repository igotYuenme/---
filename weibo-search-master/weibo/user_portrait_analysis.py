# ======================================
# å¾®åšå—ä¼—ç”»åƒåˆ†æï¼šæ ¸å¿ƒåœˆå±‚ä¸è¡Œä¸ºèšç±»
# ä»»åŠ¡äºŒï¼šå¯¹æœ‰æ•ˆç”¨æˆ·èšç±»åˆ†æï¼Œåˆ¤æ–­æ ¸å¿ƒåœˆå±‚
# åŠŸèƒ½ï¼š
#   - å¯¹æœ‰æ•ˆç”¨æˆ·è¿›è¡Œèšç±»åˆ†æ
#   - è¯†åˆ«ä¸‰ç±»æ ¸å¿ƒåœˆå±‚äººç¾¤
#   - ç”Ÿæˆç”»åƒæŠ¥å‘Šå’Œå¯è§†åŒ–
# ======================================

import json
import re
import jieba
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# ======================================
# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ======================================
def load_data(json_path="weibo_data_20251218_163102.json"):
    """åŠ è½½å¾®åšæ•°æ®"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        df = pd.DataFrame(data)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡å¾®åšæ•°æ®")
        return df
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ {json_path} ä¸å­˜åœ¨")
        return None
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return None

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if not isinstance(text, str):
        return ""
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'@.*?\s', '', text)
    text = re.sub(r'#.*?#', '', text)
    return text.strip()

def standardize_columns(df):
    """æ ‡å‡†åŒ–åˆ—å"""
    # ç¡®ä¿æ•°å€¼ç±»å‹æ­£ç¡®
    if 'reposts_count' not in df.columns:
        if 'reposts' in df.columns:
            df['reposts_count'] = pd.to_numeric(df['reposts'], errors='coerce').fillna(0)
        else:
            df['reposts_count'] = 0
    else:
        df['reposts_count'] = pd.to_numeric(df['reposts_count'], errors='coerce').fillna(0)
    
    if 'comments_count' not in df.columns:
        if 'comments' in df.columns:
            df['comments_count'] = pd.to_numeric(df['comments'], errors='coerce').fillna(0)
        else:
            df['comments_count'] = 0
    else:
        df['comments_count'] = pd.to_numeric(df['comments_count'], errors='coerce').fillna(0)
    
    if 'attitudes_count' not in df.columns:
        if 'likes' in df.columns:
            df['attitudes_count'] = pd.to_numeric(df['likes'], errors='coerce').fillna(0)
        else:
            df['attitudes_count'] = 0
    else:
        df['attitudes_count'] = pd.to_numeric(df['attitudes_count'], errors='coerce').fillna(0)
    
    return df

# ======================================
# 2. ç‰¹å¾å·¥ç¨‹
# ======================================

def extract_time_features(df):
    """æå–æ—¶é—´ç‰¹å¾"""
    def parse_time(created_at):
        """è§£ææ—¶é—´å­—ç¬¦ä¸²"""
        try:
            if isinstance(created_at, str):
                # æ ¼å¼1: "Sun Nov 16 21:03:35 +0800 2025"
                if ' +' in created_at:
                    time_str = created_at.split(' +')[0]
                    try:
                        # ä½¿ç”¨localeè®¾ç½®æ¥è§£æè‹±æ–‡æœˆä»½
                        import locale
                        # å°è¯•è®¾ç½®localeä¸ºè‹±æ–‡
                        try:
                            locale.setlocale(locale.LC_TIME, 'en_US.UTF-8')
                        except:
                            try:
                                locale.setlocale(locale.LC_TIME, 'English')
                            except:
                                pass
                        dt = datetime.strptime(time_str, "%a %b %d %H:%M:%S %Y")
                        return dt
                    except Exception as e:
                        # å¦‚æœlocaleæ–¹æ³•å¤±è´¥ï¼Œæ‰‹åŠ¨è§£æ
                        try:
                            # è§£æå®Œæ•´çš„æ—¶é—´å­—ç¬¦ä¸² "Mon Dec 08 21:18:03 +0800 2025"
                            parts = created_at.split()
                            if len(parts) >= 6:
                                # æœˆä»½æ˜ å°„
                                month_map = {
                                    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
                                    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12
                                }
                                # parts[0] = "Mon" (æ˜ŸæœŸï¼Œå¿½ç•¥)
                                month_str = parts[1]  # "Dec"
                                day = int(parts[2])   # "08"
                                time_part = parts[3]  # "21:18:03"
                                # parts[4] = "+0800" (æ—¶åŒºï¼Œå¿½ç•¥)
                                year = int(parts[5])  # "2025" - å¹´ä»½åœ¨æœ€åï¼
                                hour, minute, second = map(int, time_part.split(':'))
                                if month_str in month_map:
                                    dt = datetime(year, month_map[month_str], day, hour, minute, second)
                                    return dt
                        except (ValueError, IndexError, KeyError) as e:
                            pass
                
                # æ ¼å¼2: "2025-11-16 21:03:35"
                try:
                    dt = datetime.strptime(created_at, "%Y-%m-%d %H:%M:%S")
                    return dt
                except:
                    pass
                
                # æ ¼å¼3: "2025/11/16 21:03:35"
                try:
                    dt = datetime.strptime(created_at, "%Y/%m/%d %H:%M:%S")
                    return dt
                except:
                    pass
                
            return None
        except Exception as e:
            return None
    
    df['created_datetime'] = df['created_at'].apply(parse_time)
    
    # ç»Ÿè®¡æ—¶é—´è§£ææˆåŠŸç‡
    success_count = df['created_datetime'].notna().sum()
    success_rate = success_count / len(df) * 100
    print(f"  æ—¶é—´è§£ææˆåŠŸç‡: {success_count}/{len(df)} ({success_rate:.1f}%)")
    
    # æå–æ—¶é—´ç‰¹å¾
    df['hour'] = df['created_datetime'].apply(lambda x: x.hour if x else 0)
    df['month'] = df['created_datetime'].apply(lambda x: x.month if x else 0)
    df['day_of_week'] = df['created_datetime'].apply(lambda x: x.weekday() if x else 0)
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºè€ƒè¯•å‘¨ï¼ˆ6æœˆã€12æœˆã€1æœˆï¼‰
    df['is_exam_season'] = df['month'].apply(lambda x: 1 if x in [1, 6, 12] else 0)
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºæ‹›è˜å­£ï¼ˆ3-5æœˆï¼Œ9-11æœˆï¼‰
    df['is_recruitment_season'] = df['month'].apply(lambda x: 1 if x in [3, 4, 5, 9, 10, 11] else 0)
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºæ™šé—´æ—¶æ®µï¼ˆ18:00-23:59ï¼‰
    df['is_evening'] = df['hour'].apply(lambda x: 1 if 18 <= x <= 23 else 0)
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºä¼‘é—²æ—¶æ®µï¼ˆ19:00-22:00ï¼‰
    df['is_leisure_time'] = df['hour'].apply(lambda x: 1 if 19 <= x <= 22 else 0)
    
    return df

def extract_content_features(df):
    """æå–å†…å®¹ç‰¹å¾"""
    df['clean_text'] = df['text'].apply(clean_text)
    
    # å®šä¹‰å…³é”®è¯åˆ†ç±»
    academic_keywords = ['è€ƒè¯•', 'è€ƒç ”', 'æ¯•ä¸š', 'è®ºæ–‡', 'å¤ä¹ ', 'å››å…­çº§', 'æ•™èµ„', 'ä¸“å››', 'ä¸“å…«', 
                        'æœŸæœ«', 'æœŸä¸­', 'ä½œä¸š', 'å­¦ä¹ ', 'å¤‡è€ƒ', 'ä¸Šå²¸']
    career_keywords = ['å·¥ä½œ', 'é¢è¯•', 'æ±‚èŒ', 'offer', 'è·³æ§½', 'äº‹ä¸š', 'å²—ä½', 'æ‹›è˜', 'ç®€å†', 
                      'HR', 'è–ªèµ„', 'è½¬æ­£', 'å®ä¹ ']
    emotional_keywords = ['å¤åˆ', 'åˆ†æ‰‹', 'æ‹çˆ±', 'å–œæ¬¢', 'å‰ä»»', 'æš§æ˜§', 'æ¡ƒèŠ±', 'å©šå§»', 'æ„Ÿæƒ…', 
                         'æƒ…æ„Ÿ', 'çˆ±æƒ…', 'å¯¹è±¡']
    entertainment_keywords = ['è¿åŠ¿', 'æ°´é€†', 'MBTI', 'æ˜¾åŒ–', 'å¸å¼•åŠ›æ³•åˆ™', 'æ˜Ÿåº§', 'å¡”ç½—', 'å åœ']
    comfort_keywords = ['å»ºè®®', 'æŒ‡å¼•', 'å¸®åŠ©', 'è¿·èŒ«', 'ç„¦è™‘', 'å‹åŠ›', 'å›°æƒ‘', 'æ±‚åŠ©', 'æ€ä¹ˆåŠ', 
                       'å¦‚ä½•', 'æ±‚', 'å¸Œæœ›']
    deep_keywords = ['å’¨è¯¢', 'ä»˜è´¹', 'è¯¾ç¨‹', 'å­¦ä¹ ', 'æ·±å…¥', 'ä¸“ä¸š', 'åˆ†æ', 'è§£è¯»', 'è¯¦ç»†']
    
    def count_keywords(text, keyword_list):
        if not isinstance(text, str):
            return 0
        text_lower = text.lower()
        return sum(1 for kw in keyword_list if kw in text)
    
    # è®¡ç®—å„ç±»å…³é”®è¯å¾—åˆ†
    df['academic_score'] = df['clean_text'].apply(lambda x: count_keywords(x, academic_keywords))
    df['career_score'] = df['clean_text'].apply(lambda x: count_keywords(x, career_keywords))
    df['emotional_score'] = df['clean_text'].apply(lambda x: count_keywords(x, emotional_keywords))
    df['entertainment_score'] = df['clean_text'].apply(lambda x: count_keywords(x, entertainment_keywords))
    df['comfort_score'] = df['clean_text'].apply(lambda x: count_keywords(x, comfort_keywords))
    df['deep_score'] = df['clean_text'].apply(lambda x: count_keywords(x, deep_keywords))
    
    # å†…å®¹ç±»å‹åˆ†ç±»
    def classify_content_type(row):
        academic_career = row['academic_score'] + row['career_score']
        emotional = row['emotional_score']
        entertainment = row['entertainment_score']
        
        if academic_career > max(emotional, entertainment):
            return 'academic_career'
        elif emotional > entertainment:
            return 'emotional'
        elif entertainment > 0:
            return 'entertainment'
        else:
            return 'other'
    
    df['content_type'] = df.apply(classify_content_type, axis=1)
    
    # åˆ›å»ºç‹¬çƒ­ç¼–ç ç‰¹å¾
    df['is_academic_career'] = (df['content_type'] == 'academic_career').astype(int)
    df['is_emotional'] = (df['content_type'] == 'emotional').astype(int)
    df['is_entertainment'] = (df['content_type'] == 'entertainment').astype(int)
    
    return df

def calculate_interaction_features(df):
    """è®¡ç®—äº’åŠ¨ç‰¹å¾"""
    # ç¡®ä¿æ•°å€¼ç±»å‹
    for col in ['reposts_count', 'comments_count', 'attitudes_count']:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
    
    # è®¡ç®—äº’åŠ¨æ€»åˆ†
    df['interaction_score'] = (
        df['reposts_count'] * 0.3 +
        df['comments_count'] * 0.5 +
        df['attitudes_count'] * 0.2
    )
    
    # è®¡ç®—äº’åŠ¨æ´»è·ƒåº¦ï¼ˆå¯¹æ•°å˜æ¢ï¼‰
    df['log_interaction'] = np.log10(df['interaction_score'] + 1)
    
    # è®¡ç®—äº’åŠ¨å¤šæ ·æ€§
    df['has_reposts'] = (df['reposts_count'] > 0).astype(int)
    df['has_comments'] = (df['comments_count'] > 0).astype(int)
    df['has_likes'] = (df['attitudes_count'] > 0).astype(int)
    df['interaction_diversity'] = (
        df['has_reposts'] * 0.3 +
        df['has_comments'] * 0.4 +
        df['has_likes'] * 0.3
    )
    
    return df

def calculate_user_engagement_features(df):
    """è®¡ç®—ç”¨æˆ·å‚ä¸åº¦ç‰¹å¾"""
    if 'user' in df.columns:
        # åŸºäºç”¨æˆ·ç»´åº¦èšåˆ
        user_stats = df.groupby('user').agg({
            'interaction_score': ['sum', 'mean', 'count'],
            'reposts_count': 'sum',
            'comments_count': 'sum',
            'attitudes_count': 'sum',
            'id': 'count'
        }).reset_index()
        
        user_stats.columns = ['user', 'total_interaction', 'avg_interaction', 
                             'post_count', 'total_reposts', 'total_comments', 
                             'total_attitudes', 'weibo_count']
        
        # è®¡ç®—ç”¨æˆ·å‚ä¸åº¦æŒ‡æ ‡
        user_stats['engagement_level'] = (
            np.log10(user_stats['total_interaction'] + 1) * 0.4 +
            np.log10(user_stats['post_count'] + 1) * 0.3 +
            (user_stats['avg_interaction'] / (user_stats['avg_interaction'].max() + 1)) * 0.3
        )
        
        # è®¡ç®—ç”¨æˆ·æ´»è·ƒåº¦ï¼ˆå‘å¸–é¢‘ç‡ï¼‰
        user_stats['activity_level'] = np.log10(user_stats['weibo_count'] + 1)
        
        # åˆå¹¶å›åŸæ•°æ®
        df = df.merge(user_stats[['user', 'engagement_level', 'activity_level', 
                                  'post_count', 'weibo_count']], 
                     on='user', how='left')
        df['engagement_level'] = df['engagement_level'].fillna(0)
        df['activity_level'] = df['activity_level'].fillna(0)
        df['post_count'] = df['post_count'].fillna(1)
        df['weibo_count'] = df['weibo_count'].fillna(1)
    else:
        df['engagement_level'] = df['interaction_score'] / (df['interaction_score'].max() + 1)
        df['activity_level'] = 0
        df['post_count'] = 1
        df['weibo_count'] = 1
    
    return df

def extract_sentiment_features(df):
    """æå–æƒ…æ„Ÿç‰¹å¾"""
    positive_words = ['é¡ºåˆ©', 'å¼€å¿ƒ', 'å¸Œæœ›', 'æˆåŠŸ', 'ä¸Šå²¸', 'å¹¸è¿', 'æœŸå¾…', 'åŠ æ²¹', 'å¥½è¿']
    negative_words = ['ç„¦è™‘', 'éš¾å—', 'å´©æºƒ', 'å®³æ€•', 'è¿·èŒ«', 'å¤±è´¥', 'å‹åŠ›', 'emo', 'æ‹…å¿ƒ', 'ç´§å¼ ']
    
    def sentiment_score(text):
        if not isinstance(text, str):
            return 0
        pos = sum(1 for w in positive_words if w in text)
        neg = sum(1 for w in negative_words if w in text)
        return pos - neg
    
    df['sentiment_score'] = df['clean_text'].apply(sentiment_score)
    
    # å¿ƒç†æ…°è—‰éœ€æ±‚æŒ‡æ ‡ï¼ˆè´Ÿå‘æƒ…æ„Ÿ + å¯»æ±‚å¸®åŠ©ï¼‰
    df['comfort_need'] = (
        (df['sentiment_score'] < 0).astype(int) * 0.5 +
        (df['comfort_score'] > 0).astype(int) * 0.5
    )
    
    return df

# ======================================
# 3. èšç±»åˆ†æ
# ======================================

def perform_clustering(df, n_clusters=3):
    """æ‰§è¡ŒK-meansèšç±»"""
    # é€‰æ‹©èšç±»ç‰¹å¾
    feature_cols = [
        'is_academic_career',      # å­¦ä¸š/èŒä¸šå†…å®¹
        'is_emotional',            # æƒ…æ„Ÿå†…å®¹
        'is_entertainment',        # å¨±ä¹å†…å®¹
        'log_interaction',         # äº’åŠ¨å¼ºåº¦
        'interaction_diversity',   # äº’åŠ¨å¤šæ ·æ€§
        'engagement_level',        # å‚ä¸åº¦
        'activity_level',          # æ´»è·ƒåº¦
        'comfort_score',           # å¿ƒç†æ…°è—‰éœ€æ±‚
        'comfort_need',            # æ…°è—‰éœ€æ±‚æŒ‡æ ‡
        'deep_score',              # æ·±åº¦å‚ä¸æŒ‡æ ‡
        'is_exam_season',          # è€ƒè¯•å‘¨
        'is_recruitment_season',   # æ‹›è˜å­£
        'is_leisure_time',         # ä¼‘é—²æ—¶æ®µ
    ]
    
    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—å­˜åœ¨
    missing_cols = [col for col in feature_cols if col not in df.columns]
    if missing_cols:
        print(f"âš ï¸ ç¼ºå°‘ç‰¹å¾åˆ—: {missing_cols}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼å¡«å……")
        for col in missing_cols:
            df[col] = 0
    
    X = df[feature_cols].fillna(0)
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # K-meansèšç±»
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    df['cluster'] = kmeans.fit_predict(X_scaled)
    
    # è®¡ç®—èšç±»ä¸­å¿ƒ
    cluster_centers = kmeans.cluster_centers_
    
    print(f"\nâœ… å®Œæˆèšç±»åˆ†æï¼Œå…± {n_clusters} ä¸ªç°‡")
    print(f"å„ç°‡æ ·æœ¬æ•°: {pd.Series(df['cluster']).value_counts().sort_index().to_dict()}")
    
    return df, kmeans, scaler, cluster_centers

def identify_user_types(df):
    """è¯†åˆ«ä¸‰ç±»ç”¨æˆ·ç¾¤ä½“"""
    cluster_profiles = []
    
    for cluster_id in sorted(df['cluster'].unique()):
        cluster_data = df[df['cluster'] == cluster_id]
        
        # è®¡ç®—å„ç‰¹å¾çš„å¹³å‡å€¼
        profile = {
            'cluster_id': cluster_id,
            'count': len(cluster_data),
            'academic_career_ratio': cluster_data['is_academic_career'].mean(),
            'emotional_ratio': cluster_data['is_emotional'].mean(),
            'entertainment_ratio': cluster_data['is_entertainment'].mean(),
            'avg_interaction': cluster_data['interaction_score'].mean(),
            'avg_engagement': cluster_data['engagement_level'].mean(),
            'avg_activity': cluster_data['activity_level'].mean(),
            'avg_comfort_score': cluster_data['comfort_score'].mean(),
            'avg_comfort_need': cluster_data['comfort_need'].mean(),
            'avg_deep_score': cluster_data['deep_score'].mean(),
            'avg_academic_score': cluster_data['academic_score'].mean(),
            'avg_career_score': cluster_data['career_score'].mean(),
            'exam_season_ratio': cluster_data['is_exam_season'].mean(),
            'recruitment_season_ratio': cluster_data['is_recruitment_season'].mean(),
            'leisure_time_ratio': cluster_data['is_leisure_time'].mean(),
        }
        
        cluster_profiles.append(profile)
    
    # æ ¹æ®ç‰¹å¾è¯†åˆ«ç”¨æˆ·ç±»å‹ï¼ˆä¼˜åŒ–åçš„é€»è¾‘ï¼‰
    user_type_map = {}
    
    # å…ˆè®¡ç®—æ¯ä¸ªç°‡çš„ç»¼åˆå¾—åˆ†
    for profile in cluster_profiles:
        cluster_id = profile['cluster_id']
        
        # å½’ä¸€åŒ–å¾—åˆ†è®¡ç®—ï¼ˆä¼˜åŒ–æƒé‡ï¼Œç¡®ä¿ä¸‰ç±»ç”¨æˆ·éƒ½èƒ½è¢«è¯†åˆ«ï¼‰
        # å¿ƒç†æ…°è—‰å‹ï¼šå­¦ä¸š/èŒä¸šå†…å®¹ + æ…°è—‰éœ€æ±‚ï¼ˆä¼˜å…ˆè¯†åˆ«ï¼‰
        academic_career_content = profile['academic_career_ratio'] + \
                                  min((profile['avg_academic_score'] + profile['avg_career_score']) / 5, 0.5)
        comfort_score = (
            academic_career_content * 0.5 +
            profile['avg_comfort_need'] * 0.5  # æ…°è—‰éœ€æ±‚æ˜¯å…³é”®ç‰¹å¾
        )
        # å¦‚æœæ—¶é—´ç‰¹å¾æœ‰æ•ˆï¼Œé¢å¤–åŠ åˆ†
        if profile['exam_season_ratio'] > 0.1 or profile['recruitment_season_ratio'] > 0.1:
            comfort_score += 0.15
        
        # å¨±ä¹å‹ï¼šæƒ…æ„Ÿ/å¨±ä¹å†…å®¹ï¼ˆäº’åŠ¨ä¸­ç­‰ï¼Œå‚ä¸åº¦è¾ƒä½ï¼‰
        # æé«˜å¨±ä¹å‹å¾—åˆ†æƒé‡ï¼Œç¡®ä¿èƒ½è¢«è¯†åˆ«
        entertainment_score = (
            profile['entertainment_ratio'] * 0.6 +  # æé«˜æƒé‡
            profile['emotional_ratio'] * 0.4 +      # æé«˜æƒé‡
            min(profile['avg_interaction'] / 200, 0.1) -  # äº’åŠ¨ä¸­ç­‰ï¼ˆé™ä½æƒ©ç½šï¼‰
            min(profile['avg_engagement'], 0.5) * 0.1  # å‚ä¸åº¦è¾ƒä½ï¼ˆé™ä½æƒ©ç½šï¼‰
        )
        entertainment_score = max(entertainment_score, 0)  # ç¡®ä¿éè´Ÿ
        # å¦‚æœä¼‘é—²æ—¶æ®µç‰¹å¾æœ‰æ•ˆï¼Œé¢å¤–åŠ åˆ†
        if profile['leisure_time_ratio'] > 0.1:
            entertainment_score += 0.15
        # å¦‚æœå¨±ä¹å’Œæƒ…æ„Ÿå†…å®¹éƒ½è¾ƒé«˜ï¼Œé¢å¤–åŠ åˆ†
        if profile['entertainment_ratio'] > 0.15 and profile['emotional_ratio'] > 0.15:
            entertainment_score += 0.1
        
        # æ·±åº¦å‚ä¸å‹ï¼šé«˜å‚ä¸åº¦ + é«˜æ´»è·ƒåº¦ + é«˜äº’åŠ¨ï¼Œä½†å­¦ä¸š/èŒä¸šå’Œæ…°è—‰éœ€æ±‚è¾ƒä½
        # å¦‚æœå­¦ä¸š/èŒä¸šæˆ–æ…°è—‰éœ€æ±‚å¾ˆé«˜ï¼Œé™ä½æ·±åº¦å‚ä¸å‹å¾—åˆ†
        deep_penalty = 0
        if profile['academic_career_ratio'] > 0.3:
            deep_penalty += 0.2
        if profile['avg_comfort_need'] > 0.3:
            deep_penalty += 0.2
        
        deep_engagement_score = (
            min(profile['avg_engagement'], 1.0) * 0.5 +  # æé«˜å‚ä¸åº¦æƒé‡
            min(profile['avg_activity'], 1.0) * 0.3 +    # æé«˜æ´»è·ƒåº¦æƒé‡
            min(profile['avg_interaction'] / 200, 0.15) -  # é™ä½äº’åŠ¨æƒé‡
            deep_penalty  # æƒ©ç½šé¡¹
        )
        deep_engagement_score = max(deep_engagement_score, 0)  # ç¡®ä¿éè´Ÿ
        
        # å¦‚æœå¨±ä¹+æƒ…æ„Ÿç‰¹å¾éå¸¸æ˜æ˜¾ï¼ˆ>0.40ï¼‰ï¼Œæ‰é™ä½æ·±åº¦å‚ä¸å‹å¾—åˆ†ï¼ˆé¿å…è¿‡åº¦æƒ©ç½šï¼‰
        if (profile['entertainment_ratio'] + profile['emotional_ratio']) > 0.40:
            deep_engagement_score *= 0.85  # åªé™ä½15%
        
        # ä½¿ç”¨å¾—åˆ†è¿›è¡Œåˆ¤æ–­ï¼Œä½†è¦ç¡®ä¿ä¸‰ç±»ç”¨æˆ·éƒ½èƒ½è¢«è¯†åˆ«
        scores = {
            'å¿ƒç†æ…°è—‰å‹': comfort_score,
            'å¨±ä¹å‹': entertainment_score,
            'æ·±åº¦å‚ä¸å‹': deep_engagement_score
        }
        
        # è®¡ç®—å¾—åˆ†æ’åº
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        top_score_type = sorted_scores[0][0]
        top_score = sorted_scores[0][1]
        second_score = sorted_scores[1][1] if len(sorted_scores) > 1 else 0
        
        # ä¼˜å…ˆåˆ¤æ–­ï¼šå¦‚æœå­¦ä¸š/èŒä¸šæ¯”ä¾‹é«˜ä¸”æ…°è—‰éœ€æ±‚é«˜ï¼Œä¼˜å…ˆè¯†åˆ«ä¸ºå¿ƒç†æ…°è—‰å‹
        if (profile['academic_career_ratio'] > 0.35 and profile['avg_comfort_need'] > 0.4):
            user_type = 'å¿ƒç†æ…°è—‰å‹'
        # ä¼˜å…ˆåˆ¤æ–­å¨±ä¹å‹ï¼šå¦‚æœå¨±ä¹+æƒ…æ„Ÿç‰¹å¾æ˜æ˜¾ï¼ˆ>0.30ï¼‰ï¼Œä¼˜å…ˆè€ƒè™‘å¨±ä¹å‹
        elif ((profile['entertainment_ratio'] + profile['emotional_ratio']) > 0.30 and
              entertainment_score > 0.30):
            # å¦‚æœå¨±ä¹å‹å¾—åˆ†æœ€é«˜æˆ–ä¸æœ€é«˜åˆ†å·®è·ä¸å¤§ï¼ˆ<0.2ï¼‰ï¼Œè¯†åˆ«ä¸ºå¨±ä¹å‹
            if (entertainment_score == top_score or 
                (top_score - entertainment_score) < 0.2):
                user_type = 'å¨±ä¹å‹'
            # å¦‚æœå¨±ä¹å‹å¾—åˆ†ç¬¬äºŒé«˜ï¼Œä¸”å·®è·ä¸å¤§ï¼Œä¹Ÿè¯†åˆ«ä¸ºå¨±ä¹å‹
            elif (sorted_scores[1][0] == 'å¨±ä¹å‹' and 
                  (top_score - entertainment_score) < 0.25):
                user_type = 'å¨±ä¹å‹'
            else:
                user_type = top_score_type
        # å¦‚æœå¨±ä¹å‹å¾—åˆ†è¾ƒé«˜ä¸”ç‰¹å¾æ˜æ˜¾ï¼Œä½†å‚ä¸åº¦ä¸é«˜ï¼Œè¯†åˆ«ä¸ºå¨±ä¹å‹
        elif (entertainment_score > 0.35 and 
              profile['avg_engagement'] < 0.75):
            user_type = 'å¨±ä¹å‹'
        # å¦åˆ™æ ¹æ®å¾—åˆ†åˆ¤æ–­
        else:
            user_type = top_score_type
        
        user_type_map[cluster_id] = user_type
        
        print(f"\nç°‡ {cluster_id} å¾—åˆ†åˆ†æ (æ ·æœ¬æ•°: {profile['count']}):")
        print(f"  å¿ƒç†æ…°è—‰å‹å¾—åˆ†: {comfort_score:.3f} (å­¦ä¸š/èŒä¸š: {academic_career_content:.3f}, æ…°è—‰éœ€æ±‚: {profile['avg_comfort_need']:.3f})")
        print(f"  å¨±ä¹å‹å¾—åˆ†: {entertainment_score:.3f} (å¨±ä¹: {profile['entertainment_ratio']:.3f}, æƒ…æ„Ÿ: {profile['emotional_ratio']:.3f})")
        print(f"  æ·±åº¦å‚ä¸å‹å¾—åˆ†: {deep_engagement_score:.3f} (å‚ä¸åº¦: {profile['avg_engagement']:.3f}, æ´»è·ƒåº¦: {profile['avg_activity']:.3f})")
        print(f"  â†’ è¯†åˆ«ä¸º: {user_type}")
    
    # æ˜ å°„åˆ°æ•°æ®æ¡†
    df['user_type'] = df['cluster'].map(user_type_map)
    
    # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿ä¸‰ç±»ç”¨æˆ·éƒ½æœ‰è¢«è¯†åˆ«
    identified_types = set(user_type_map.values())
    required_types = {'å¿ƒç†æ…°è—‰å‹', 'å¨±ä¹å‹', 'æ·±åº¦å‚ä¸å‹'}
    missing_types = required_types - identified_types
    
    if missing_types:
        print(f"\nâš ï¸ è­¦å‘Š: ç¼ºå°‘ä»¥ä¸‹ç”¨æˆ·ç±»å‹: {missing_types}")
        print("   å°è¯•è°ƒæ•´è¯†åˆ«é€»è¾‘...")
        
        # å¦‚æœç¼ºå°‘å¨±ä¹å‹ï¼Œå¼ºåˆ¶åˆ†é…ä¸€ä¸ª
        if 'å¨±ä¹å‹' in missing_types:
            max_entertainment_cluster = None
            max_entertainment_score = -1
            max_entertainment_profile = None
            
            for profile in cluster_profiles:
                cluster_id = profile['cluster_id']
                if user_type_map[cluster_id] != 'å¿ƒç†æ…°è—‰å‹':
                    ent_ratio = profile['entertainment_ratio']
                    emo_ratio = profile['emotional_ratio']
                    entertainment_score_simple = ent_ratio * 0.6 + emo_ratio * 0.4
                    if (ent_ratio + emo_ratio) > 0.20:
                        entertainment_score_simple += 0.1
                    
                    if entertainment_score_simple > max_entertainment_score:
                        max_entertainment_score = entertainment_score_simple
                        max_entertainment_cluster = cluster_id
                        max_entertainment_profile = profile
            
            if max_entertainment_cluster is not None:
                old_type = user_type_map[max_entertainment_cluster]
                user_type_map[max_entertainment_cluster] = 'å¨±ä¹å‹'
                df['user_type'] = df['cluster'].map(user_type_map)
                print(f"  âœ… å°†ç°‡ {max_entertainment_cluster} ä» '{old_type}' è°ƒæ•´ä¸º 'å¨±ä¹å‹'")
                print(f"     å¨±ä¹æ¯”ä¾‹: {max_entertainment_profile['entertainment_ratio']:.3f}, "
                      f"æƒ…æ„Ÿæ¯”ä¾‹: {max_entertainment_profile['emotional_ratio']:.3f}")
        
        # å¦‚æœç¼ºå°‘æ·±åº¦å‚ä¸å‹ï¼Œå¼ºåˆ¶åˆ†é…ä¸€ä¸ª
        if 'æ·±åº¦å‚ä¸å‹' in missing_types:
            max_engagement_cluster = None
            max_engagement_score = -1
            max_engagement_profile = None
            
            for profile in cluster_profiles:
                cluster_id = profile['cluster_id']
                # é€‰æ‹©å‚ä¸åº¦æœ€é«˜çš„ç°‡ï¼ˆä½†ä¸èƒ½æ˜¯å¿ƒç†æ…°è—‰å‹ï¼‰
                if user_type_map[cluster_id] != 'å¿ƒç†æ…°è—‰å‹':
                    engagement_score = profile['avg_engagement'] * 0.5 + profile['avg_activity'] * 0.3
                    # å¦‚æœå‚ä¸åº¦å¾ˆé«˜ï¼Œä¼˜å…ˆé€‰æ‹©
                    if engagement_score > max_engagement_score:
                        max_engagement_score = engagement_score
                        max_engagement_cluster = cluster_id
                        max_engagement_profile = profile
            
            if max_engagement_cluster is not None:
                old_type = user_type_map[max_engagement_cluster]
                user_type_map[max_engagement_cluster] = 'æ·±åº¦å‚ä¸å‹'
                df['user_type'] = df['cluster'].map(user_type_map)
                print(f"  âœ… å°†ç°‡ {max_engagement_cluster} ä» '{old_type}' è°ƒæ•´ä¸º 'æ·±åº¦å‚ä¸å‹'")
                print(f"     å‚ä¸åº¦: {max_engagement_profile['avg_engagement']:.3f}, "
                      f"æ´»è·ƒåº¦: {max_engagement_profile['avg_activity']:.3f}")
    
    return df, user_type_map

# ======================================
# 4. å¯è§†åŒ–
# ======================================

def plot_clustering_results(df, save_path="weibo_clustering_results.png"):
    """ç»˜åˆ¶èšç±»ç»“æœå¯è§†åŒ–"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. èšç±»æ•£ç‚¹å›¾ï¼ˆPCAé™ç»´ï¼‰
    ax1 = axes[0, 0]
    feature_cols = ['is_academic_career', 'is_emotional', 'is_entertainment', 
                   'log_interaction', 'interaction_diversity', 'engagement_level',
                   'comfort_score', 'deep_score', 'is_exam_season', 'is_leisure_time']
    X = df[feature_cols].fillna(0)
    
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(StandardScaler().fit_transform(X))
    
    # æŒ‰ç”¨æˆ·ç±»å‹ç€è‰²
    type_colors = {'å¿ƒç†æ…°è—‰å‹': '#FF6B6B', 'å¨±ä¹å‹': '#4ECDC4', 'æ·±åº¦å‚ä¸å‹': '#45B7D1'}
    colors = df['user_type'].map(type_colors).fillna('#999999')
    
    scatter = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.6, s=50)
    ax1.set_xlabel('ä¸»æˆåˆ†1', fontsize=12)
    ax1.set_ylabel('ä¸»æˆåˆ†2', fontsize=12)
    ax1.set_title('ç”¨æˆ·èšç±»ç»“æœï¼ˆPCAé™ç»´ï¼‰', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ å›¾ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor=color, label=label) 
                      for label, color in type_colors.items() 
                      if label in df['user_type'].values]
    ax1.legend(handles=legend_elements, loc='best')
    
    # 2. ç”¨æˆ·ç±»å‹åˆ†å¸ƒ
    ax2 = axes[0, 1]
    user_type_counts = df['user_type'].value_counts()
    colors_list = [type_colors.get(ut, '#999999') for ut in user_type_counts.index]
    ax2.bar(user_type_counts.index, user_type_counts.values, color=colors_list)
    ax2.set_xlabel('ç”¨æˆ·ç±»å‹', fontsize=12)
    ax2.set_ylabel('æ•°é‡', fontsize=12)
    ax2.set_title('ä¸‰ç±»ç”¨æˆ·ç¾¤ä½“åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax2.tick_params(axis='x', rotation=15)
    
    # 3. å„ç±»å‹ç‰¹å¾å¯¹æ¯”
    ax3 = axes[1, 0]
    type_features = df.groupby('user_type').agg({
        'is_academic_career': 'mean',
        'is_emotional': 'mean',
        'is_entertainment': 'mean',
        'log_interaction': 'mean',
        'engagement_level': 'mean'
    }).T
    
    x = np.arange(len(type_features.index))
    width = 0.25
    for i, user_type in enumerate(type_features.columns):
        offset = (i - 1) * width
        ax3.bar(x + offset, type_features[user_type], width, 
               label=user_type, alpha=0.8, color=type_colors.get(user_type, '#999999'))
    
    ax3.set_xlabel('ç‰¹å¾', fontsize=12)
    ax3.set_ylabel('å¹³å‡å€¼', fontsize=12)
    ax3.set_title('å„ç”¨æˆ·ç±»å‹ç‰¹å¾å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax3.set_xticks(x)
    ax3.set_xticklabels(type_features.index, rotation=45, ha='right')
    ax3.legend()
    ax3.grid(True, alpha=0.3, axis='y')
    
    # 4. æ—¶é—´ç‰¹å¾åˆ†æ
    ax4 = axes[1, 1]
    time_features = df.groupby('user_type').agg({
        'is_exam_season': 'mean',
        'is_recruitment_season': 'mean',
        'is_leisure_time': 'mean'
    }).T
    
    x = np.arange(len(time_features.index))
    width = 0.25
    for i, user_type in enumerate(time_features.columns):
        offset = (i - 1) * width
        ax4.bar(x + offset, time_features[user_type], width, 
               label=user_type, alpha=0.8, color=type_colors.get(user_type, '#999999'))
    
    ax4.set_xlabel('æ—¶é—´ç‰¹å¾', fontsize=12)
    ax4.set_ylabel('æ¯”ä¾‹', fontsize=12)
    ax4.set_title('å„ç±»å‹ç”¨æˆ·æ—¶é—´è¡Œä¸ºç‰¹å¾', fontsize=14, fontweight='bold')
    ax4.set_xticks(x)
    ax4.set_xticklabels(time_features.index, rotation=45, ha='right')
    ax4.legend()
    ax4.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å·²ä¿å­˜å¯è§†åŒ–ç»“æœ: {save_path}")
    plt.show()

def create_additional_visualizations(df, user_type_map):
    """åˆ›å»ºé¢å¤–çš„ä¸“ä¸šå¯è§†åŒ–å›¾è¡¨"""
    type_colors = {'å¿ƒç†æ…°è—‰å‹': '#FF6B6B', 'å¨±ä¹å‹': '#4ECDC4', 'æ·±åº¦å‚ä¸å‹': '#45B7D1'}
    
    # 1. é›·è¾¾å›¾ - ä¸‰ç±»ç”¨æˆ·ç‰¹å¾å¯¹æ¯”
    fig1, ax1 = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
    
    # é€‰æ‹©å…³é”®ç‰¹å¾
    features = ['å­¦ä¸š/èŒä¸š', 'æƒ…æ„Ÿ', 'å¨±ä¹', 'å‚ä¸åº¦', 'æ´»è·ƒåº¦', 'äº’åŠ¨å¼ºåº¦']
    feature_keys = ['is_academic_career', 'is_emotional', 'is_entertainment', 
                    'engagement_level', 'activity_level', 'log_interaction']
    
    # è®¡ç®—æ¯ä¸ªç±»å‹çš„å¹³å‡å€¼
    angles = np.linspace(0, 2 * np.pi, len(features), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ
    
    for user_type in df['user_type'].unique():
        type_data = df[df['user_type'] == user_type]
        values = []
        for key in feature_keys:
            if key == 'log_interaction':
                max_val = df[key].max()
                val = type_data[key].mean() / max_val if max_val > 0 else 0
            else:
                val = type_data[key].mean()
            values.append(val)
        values += values[:1]  # é—­åˆ
        
        ax1.plot(angles, values, 'o-', linewidth=2, label=user_type, 
                color=type_colors.get(user_type, '#999999'))
        ax1.fill(angles, values, alpha=0.15, color=type_colors.get(user_type, '#999999'))
    
    ax1.set_xticks(angles[:-1])
    ax1.set_xticklabels(features, fontsize=11)
    ax1.set_ylim(0, 1)
    ax1.set_title('ä¸‰ç±»ç”¨æˆ·ç‰¹å¾é›·è¾¾å›¾å¯¹æ¯”', fontsize=14, fontweight='bold', pad=20)
    ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)
    ax1.grid(True, linestyle='--', alpha=0.5)
    
    plt.tight_layout()
    plt.savefig('user_portrait_radar.png', dpi=300, bbox_inches='tight')
    print("  ğŸ’¾ å·²ä¿å­˜: user_portrait_radar.png")
    plt.close()
    
    # 2. çƒ­åŠ›å›¾ - ç‰¹å¾å¯¹æ¯”
    fig2, ax2 = plt.subplots(figsize=(12, 8))
    
    # é€‰æ‹©æ•°å€¼ç‰¹å¾
    numeric_features = ['is_academic_career', 'is_emotional', 'is_entertainment',
                       'engagement_level', 'activity_level', 'log_interaction',
                       'comfort_score', 'deep_score', 'interaction_score']
    feature_names = ['å­¦ä¸š/èŒä¸š', 'æƒ…æ„Ÿ', 'å¨±ä¹', 'å‚ä¸åº¦', 'æ´»è·ƒåº¦', 'äº’åŠ¨å¼ºåº¦',
                    'æ…°è—‰éœ€æ±‚', 'æ·±åº¦å¾—åˆ†', 'äº’åŠ¨åˆ†æ•°']
    
    corr_data = df.groupby('user_type')[numeric_features].mean().T
    corr_data.index = feature_names
    
    # ç”±äº"äº’åŠ¨åˆ†æ•°"çš„å€¼åŸŸï¼ˆ120-140ï¼‰è¿œå¤§äºå…¶ä»–ç‰¹å¾ï¼ˆ0-2ï¼‰ï¼Œå¯¼è‡´é¢œè‰²å¯¹æ¯”ä¸æ˜æ˜¾
    # æ–¹æ¡ˆ1ï¼šåˆ†ç¦»å¤„ç† - å°†"äº’åŠ¨åˆ†æ•°"å•ç‹¬å¤„ç†ï¼Œå…¶ä»–ç‰¹å¾ä½¿ç”¨åŸå§‹å€¼
    # æ–¹æ¡ˆ2ï¼šå¯¹æ¯ä¸ªç‰¹å¾è¡Œå•ç‹¬å½’ä¸€åŒ–ï¼Œä½¿æ¯è¡Œå†…éƒ¨éƒ½èƒ½çœ‹åˆ°é¢œè‰²å·®å¼‚
    corr_data_normalized = corr_data.copy()
    
    # å¯¹æ¯ä¸ªç‰¹å¾è¡Œè¿›è¡Œå½’ä¸€åŒ–ï¼ˆ0-1èŒƒå›´ï¼‰ï¼Œæ¯è¡Œç‹¬ç«‹å½’ä¸€åŒ–
    for idx in corr_data.index:
        row = corr_data.loc[idx]
        row_min, row_max = row.min(), row.max()
        if row_max > row_min:  # é¿å…é™¤é›¶
            # è¡Œå†…å½’ä¸€åŒ–åˆ°0-1
            corr_data_normalized.loc[idx] = (row - row_min) / (row_max - row_min)
        else:
            # å¦‚æœè¡Œå†…å€¼éƒ½ç›¸åŒï¼Œè®¾ä¸º0.5ï¼ˆä¸­ç­‰é¢œè‰²ï¼‰
            corr_data_normalized.loc[idx] = 0.5
    
    # åˆ›å»ºçƒ­åŠ›å›¾ï¼šä½¿ç”¨å½’ä¸€åŒ–æ•°æ®æ˜ å°„é¢œè‰²ï¼Œæ ‡æ³¨æ˜¾ç¤ºåŸå§‹æ•°å€¼
    # ä½¿ç”¨æ›´å¼ºçš„é¢œè‰²æ˜ å°„æ–¹æ¡ˆï¼Œç¡®ä¿é¢œè‰²å¯¹æ¯”æ˜æ˜¾
    im = sns.heatmap(corr_data_normalized, annot=corr_data, fmt='.2f', 
                     cmap='RdYlGn_r', vmin=0, vmax=1,  # ä½¿ç”¨åè½¬çš„çº¢-é»„-ç»¿è‰²è°±ï¼Œé¢œè‰²å¯¹æ¯”æ›´å¼º
                     cbar=True, cbar_kws={'label': 'å½’ä¸€åŒ–å€¼ (è¡Œå†…)', 'shrink': 0.8}, 
                     ax=ax2, linewidths=0.5, linecolor='gray', linewidth=1)
    ax2.set_title('ä¸‰ç±»ç”¨æˆ·ç‰¹å¾çƒ­åŠ›å›¾ï¼ˆå•å…ƒæ ¼æ˜¾ç¤ºåŸå§‹å€¼ï¼Œé¢œè‰²è¡¨ç¤ºè¡Œå†…ç›¸å¯¹å¤§å°ï¼‰', 
                  fontsize=13, fontweight='bold', pad=15)
    ax2.set_xlabel('ç”¨æˆ·ç±»å‹', fontsize=12)
    ax2.set_ylabel('ç‰¹å¾ç»´åº¦', fontsize=12)
    plt.xticks(rotation=0)
    plt.yticks(rotation=0)
    
    # è®¾ç½®colorbaræ ‡ç­¾å’Œåˆ»åº¦å­—ä½“ï¼ˆé€šè¿‡figureçš„axesè®¿é—®colorbarï¼‰
    # seaborn heatmapä¼šåœ¨figureä¸­æ·»åŠ colorbar axes
    fig = ax2.figure
    # colorbaré€šå¸¸æ˜¯figureä¸­çš„æœ€åä¸€ä¸ªaxes
    if len(fig.axes) > 1:
        # colorbaré€šå¸¸æ˜¯æœ€åä¸€ä¸ªaxes
        cbar_ax = fig.axes[-1]
        if cbar_ax != ax2:  # ç¡®ä¿ä¸æ˜¯ä¸»axes
            cbar_ax.set_ylabel('å½’ä¸€åŒ–å€¼ï¼ˆè¡Œå†…0-1ï¼‰', fontsize=10, rotation=270, labelpad=20)
            cbar_ax.tick_params(labelsize=9)
    
    plt.tight_layout()
    plt.savefig('user_portrait_heatmap.png', dpi=300, bbox_inches='tight')
    print("  ğŸ’¾ å·²ä¿å­˜: user_portrait_heatmap.png")
    plt.close()
    
    # 3. ç®±çº¿å›¾ - äº’åŠ¨è¡Œä¸ºåˆ†å¸ƒ
    fig3, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    interaction_metrics = {
        'reposts_count': 'è½¬å‘æ•°',
        'comments_count': 'è¯„è®ºæ•°',
        'attitudes_count': 'ç‚¹èµæ•°',
        'interaction_score': 'äº’åŠ¨æ€»åˆ†'
    }
    
    for idx, (metric, label) in enumerate(interaction_metrics.items()):
        ax = axes[idx // 2, idx % 2]
        
        data_to_plot = [df[df['user_type'] == ut][metric].values 
                        for ut in df['user_type'].unique()]
        labels = list(df['user_type'].unique())
        colors = [type_colors.get(ut, '#999999') for ut in labels]
        
        bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True, 
                       showmeans=True, meanline=True)
        
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        
        ax.set_ylabel(label, fontsize=11)
        ax.set_title(f'{label}åˆ†å¸ƒå¯¹æ¯”', fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=15)
    
    plt.suptitle('ä¸‰ç±»ç”¨æˆ·äº’åŠ¨è¡Œä¸ºåˆ†å¸ƒå¯¹æ¯”', fontsize=14, fontweight='bold', y=0.995)
    plt.tight_layout()
    plt.savefig('user_portrait_boxplot.png', dpi=300, bbox_inches='tight')
    print("  ğŸ’¾ å·²ä¿å­˜: user_portrait_boxplot.png")
    plt.close()
    
    # 4. å †å æŸ±çŠ¶å›¾ - å†…å®¹ç±»å‹å æ¯”
    fig4, ax4 = plt.subplots(figsize=(10, 6))
    
    content_data = df.groupby('user_type').agg({
        'is_academic_career': 'mean',
        'is_emotional': 'mean',
        'is_entertainment': 'mean'
    })
    
    x = np.arange(len(content_data.index))
    width = 0.6
    
    bottom = np.zeros(len(content_data.index))
    colors_content = ['#FF9999', '#66B2FF', '#99FF99']
    labels_content = ['å­¦ä¸š/èŒä¸š', 'æƒ…æ„Ÿ', 'å¨±ä¹']
    
    for i, (col, label) in enumerate(zip(['is_academic_career', 'is_emotional', 'is_entertainment'], 
                                          labels_content)):
        values = content_data[col].values * 100
        ax4.bar(x, values, width, label=label, bottom=bottom, 
               color=colors_content[i], alpha=0.8, edgecolor='black', linewidth=0.5)
        bottom += values
    
    ax4.set_xlabel('ç”¨æˆ·ç±»å‹', fontsize=12)
    ax4.set_ylabel('å†…å®¹å æ¯” (%)', fontsize=12)
    ax4.set_title('ä¸‰ç±»ç”¨æˆ·å†…å®¹ç±»å‹å æ¯”', fontsize=14, fontweight='bold')
    ax4.set_xticks(x)
    ax4.set_xticklabels(content_data.index, rotation=0)
    ax4.legend(loc='upper right', fontsize=10)
    ax4.set_ylim(0, 100)
    ax4.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('user_portrait_stacked_bar.png', dpi=300, bbox_inches='tight')
    print("  ğŸ’¾ å·²ä¿å­˜: user_portrait_stacked_bar.png")
    plt.close()
    
    # 5. æ—¶é—´ç‰¹å¾å¯¹æ¯”
    fig5, ax5 = plt.subplots(figsize=(10, 6))
    
    time_data = df.groupby('user_type').agg({
        'is_exam_season': 'mean',
        'is_recruitment_season': 'mean',
        'is_leisure_time': 'mean'
    }) * 100
    
    x = np.arange(len(time_data.index))
    width = 0.25
    
    time_features = ['is_exam_season', 'is_recruitment_season', 'is_leisure_time']
    time_labels = ['è€ƒè¯•å‘¨', 'æ‹›è˜å­£', 'ä¼‘é—²æ—¶æ®µ']
    time_colors = ['#FF6B6B', '#4ECDC4', '#FFD93D']
    
    for i, (feat, label, color) in enumerate(zip(time_features, time_labels, time_colors)):
        offset = (i - 1) * width
        values = time_data[feat].values
        bars = ax5.bar(x + offset, values, width, label=label, 
                      color=color, alpha=0.8, edgecolor='black', linewidth=0.5)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, val in zip(bars, values):
            if val > 1:
                ax5.text(bar.get_x() + bar.get_width()/2., val,
                        f'{val:.1f}%', ha='center', va='bottom', fontsize=9)
    
    ax5.set_xlabel('ç”¨æˆ·ç±»å‹', fontsize=12)
    ax5.set_ylabel('å‘å¸–æ¯”ä¾‹ (%)', fontsize=12)
    ax5.set_title('ä¸‰ç±»ç”¨æˆ·æ—¶é—´è¡Œä¸ºç‰¹å¾å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax5.set_xticks(x)
    ax5.set_xticklabels(time_data.index, rotation=0)
    ax5.legend(loc='upper left', fontsize=10)
    ax5.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('user_portrait_time_features.png', dpi=300, bbox_inches='tight')
    print("  ğŸ’¾ å·²ä¿å­˜: user_portrait_time_features.png")
    plt.close()
    
    print("âœ… æ‰€æœ‰ä¸“ä¸šå¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆå®Œæˆï¼")

# ======================================
# 5. ç”Ÿæˆç”»åƒæŠ¥å‘Š
# ======================================

def generate_portrait_report(df, user_type_map):
    """ç”Ÿæˆå—ä¼—ç”»åƒæŠ¥å‘Š"""
    report = []
    report.append("=" * 60)
    report.append("å¾®åšå—ä¼—ç”»åƒåˆ†ææŠ¥å‘Šï¼šæ ¸å¿ƒåœˆå±‚ä¸è¡Œä¸ºèšç±»")
    report.append("=" * 60)
    report.append("")
    
    # æ€»ä½“ç»Ÿè®¡
    report.append(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡")
    report.append(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
    report.append(f"  ç”¨æˆ·ç±»å‹æ•°: {len(df['user_type'].unique())}")
    if 'user' in df.columns:
        unique_users = df['user'].nunique()
        report.append(f"  å”¯ä¸€ç”¨æˆ·æ•°: {unique_users}")
    report.append("")
    
    # å„ç±»å‹è¯¦ç»†åˆ†æ
    for user_type in ['å¿ƒç†æ…°è—‰å‹', 'å¨±ä¹å‹', 'æ·±åº¦å‚ä¸å‹']:
        if user_type not in df['user_type'].values:
            continue
            
        type_data = df[df['user_type'] == user_type]
        count = len(type_data)
        ratio = count / len(df) * 100
        
        report.append("-" * 60)
        report.append(f"ğŸ‘¥ {user_type}")
        report.append("-" * 60)
        report.append(f"  æ•°é‡: {count} ({ratio:.1f}%)")
        if 'user' in df.columns:
            unique_users_type = type_data['user'].nunique()
            report.append(f"  å”¯ä¸€ç”¨æˆ·æ•°: {unique_users_type}")
        report.append("")
        
        # å†…å®¹åå¥½
        report.append("  ğŸ“ å†…å®¹åå¥½:")
        academic_ratio = type_data['is_academic_career'].mean() * 100
        emotional_ratio = type_data['is_emotional'].mean() * 100
        entertainment_ratio = type_data['is_entertainment'].mean() * 100
        report.append(f"    - å­¦ä¸š/èŒä¸šç±»: {academic_ratio:.1f}%")
        report.append(f"    - æƒ…æ„Ÿç±»: {emotional_ratio:.1f}%")
        report.append(f"    - å¨±ä¹ç±»: {entertainment_ratio:.1f}%")
        report.append("")
        
        # äº’åŠ¨è¡Œä¸º
        report.append("  ğŸ’¬ äº’åŠ¨è¡Œä¸º:")
        avg_interaction = type_data['interaction_score'].mean()
        avg_reposts = type_data['reposts_count'].mean()
        avg_comments = type_data['comments_count'].mean()
        avg_likes = type_data['attitudes_count'].mean()
        report.append(f"    - å¹³å‡äº’åŠ¨åˆ†æ•°: {avg_interaction:.2f}")
        report.append(f"    - å¹³å‡è½¬å‘æ•°: {avg_reposts:.1f}")
        report.append(f"    - å¹³å‡è¯„è®ºæ•°: {avg_comments:.1f}")
        report.append(f"    - å¹³å‡ç‚¹èµæ•°: {avg_likes:.1f}")
        report.append("")
        
        # å‚ä¸åº¦
        report.append("  ğŸ“ˆ å‚ä¸åº¦:")
        avg_engagement = type_data['engagement_level'].mean()
        avg_activity = type_data['activity_level'].mean()
        report.append(f"    - å¹³å‡å‚ä¸åº¦: {avg_engagement:.3f}")
        report.append(f"    - å¹³å‡æ´»è·ƒåº¦: {avg_activity:.3f}")
        report.append("")
        
        # æ—¶é—´ç‰¹å¾
        report.append("  â° æ—¶é—´ç‰¹å¾:")
        exam_ratio = type_data['is_exam_season'].mean() * 100
        recruit_ratio = type_data['is_recruitment_season'].mean() * 100
        leisure_ratio = type_data['is_leisure_time'].mean() * 100
        report.append(f"    - è€ƒè¯•å‘¨å‘å¸–æ¯”ä¾‹: {exam_ratio:.1f}%")
        report.append(f"    - æ‹›è˜å­£å‘å¸–æ¯”ä¾‹: {recruit_ratio:.1f}%")
        report.append(f"    - ä¼‘é—²æ—¶æ®µå‘å¸–æ¯”ä¾‹: {leisure_ratio:.1f}%")
        report.append("")
        
        # ç‰¹å¾æè¿°
        if user_type == 'å¿ƒç†æ…°è—‰å‹':
            report.append("  ğŸ¯ ç‰¹å¾æè¿°:")
            report.append("    - ä¸»è¦å…³æ³¨å­¦ä¸šå’ŒèŒä¸šç›¸å…³è¯é¢˜")
            report.append("    - å‘å¸–å³°å€¼åœ¨è€ƒè¯•å‘¨ï¼ˆ1æœˆã€6æœˆã€12æœˆï¼‰ä¸æ‹›è˜å­£ï¼ˆ3-5æœˆã€9-11æœˆï¼‰")
            report.append("    - å¯»æ±‚å­¦ä¸š/èŒä¸šæŒ‡å¼•å’Œå¿ƒç†æ”¯æŒ")
            report.append("    - ç”¨æˆ·ç¾¤ä½“ä¸»è¦ä¸ºå¤§ä¸‰è‡³ç ”ç©¶ç”Ÿ")
            report.append("    - å¿ƒç†æ…°è—‰éœ€æ±‚è¾ƒé«˜")
        elif user_type == 'å¨±ä¹å‹':
            report.append("  ğŸ¯ ç‰¹å¾æè¿°:")
            report.append("    - é›†ä¸­åœ¨ä¸€äºŒçº¿åŸå¸‚")
            report.append("    - å…³æ³¨æ„Ÿæƒ…è¿åŠ¿å’Œå¨±ä¹å†…å®¹")
            report.append("    - äº’åŠ¨é«˜å³°åœ¨æ™šé—´ä¼‘é—²æ—¶æ®µï¼ˆ19:00-22:00ï¼‰")
            report.append("    - ä»¥è½»æ¾å¨±ä¹ä¸ºä¸»è¦ç›®çš„")
        elif user_type == 'æ·±åº¦å‚ä¸å‹':
            report.append("  ğŸ¯ ç‰¹å¾æè¿°:")
            report.append("    - è·¨å¹³å°è¿½éšï¼Œé»æ€§æœ€é«˜")
            report.append("    - æœ‰ä»˜è´¹å’¨è¯¢ä¸äºŒæ¬¡åˆ›ä½œè¡Œä¸º")
            report.append("    - å‚ä¸åº¦å’Œäº’åŠ¨ç‡æœ€é«˜")
            report.append("    - å¯¹å†…å®¹è´¨é‡è¦æ±‚è¾ƒé«˜")
            report.append("    - æ´»è·ƒåº¦å’Œå‘å¸–é¢‘ç‡é«˜")
        report.append("")
    
    report.append("=" * 60)
    
    report_text = "\n".join(report)
    print(report_text)
    
    # ä¿å­˜æŠ¥å‘Š
    with open("weibo_portrait_report.txt", "w", encoding="utf-8") as f:
        f.write(report_text)
    print(f"\nğŸ’¾ å·²ä¿å­˜ç”»åƒæŠ¥å‘Š: weibo_portrait_report.txt")
    
    return report_text

# ======================================
# ä¸»ç¨‹åº
# ======================================

def main():
    print("=" * 60)
    print("å¾®åšå—ä¼—ç”»åƒåˆ†æï¼šæ ¸å¿ƒåœˆå±‚ä¸è¡Œä¸ºèšç±»")
    print("=" * 60)
    print()
    
    # 1. åŠ è½½æ•°æ®
    # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®æ–‡ä»¶
    import glob
    import os
    data_files = glob.glob("weibo_data_*.json")
    if data_files:
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
        latest_file = max(data_files, key=os.path.getmtime)
        print(f"ğŸ“ æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {latest_file}")
        df = load_data(latest_file)
    else:
        # ä½¿ç”¨é»˜è®¤æ–‡ä»¶å
        df = load_data()
    
    if df is None or len(df) == 0:
        print("âŒ æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        return
    
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"  å”¯ä¸€ç”¨æˆ·æ•°: {df['user'].nunique() if 'user' in df.columns else 'N/A'}")
    print(f"  å…³é”®è¯æ•°: {df['keyword'].nunique() if 'keyword' in df.columns else 'N/A'}")
    
    # 2. æ•°æ®é¢„å¤„ç†
    print("\nğŸ”§ è¿›è¡Œæ•°æ®é¢„å¤„ç†...")
    print("  æ­¥éª¤1/6: æ ‡å‡†åŒ–åˆ—å...")
    df = standardize_columns(df)
    print("  æ­¥éª¤2/6: æå–æ—¶é—´ç‰¹å¾...")
    df = extract_time_features(df)
    print("  æ­¥éª¤3/6: æå–å†…å®¹ç‰¹å¾...")
    df = extract_content_features(df)
    print("  æ­¥éª¤4/6: è®¡ç®—äº’åŠ¨ç‰¹å¾...")
    df = calculate_interaction_features(df)
    print("  æ­¥éª¤5/6: è®¡ç®—ç”¨æˆ·å‚ä¸åº¦ç‰¹å¾...")
    df = calculate_user_engagement_features(df)
    print("  æ­¥éª¤6/6: æå–æƒ…æ„Ÿç‰¹å¾...")
    df = extract_sentiment_features(df)
    print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
    
    # 3. èšç±»åˆ†æ
    print("\nğŸ” æ‰§è¡Œèšç±»åˆ†æ...")
    df, kmeans, scaler, cluster_centers = perform_clustering(df, n_clusters=3)
    
    # 4. è¯†åˆ«ç”¨æˆ·ç±»å‹
    print("\nğŸ‘¥ è¯†åˆ«ç”¨æˆ·ç±»å‹...")
    df, user_type_map = identify_user_types(df)
    print(f"âœ… ç”¨æˆ·ç±»å‹æ˜ å°„: {user_type_map}")
    
    # 5. å¯è§†åŒ–
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plot_clustering_results(df)
    
    # 5.1 ç”Ÿæˆé¢å¤–ä¸“ä¸šå¯è§†åŒ–å›¾è¡¨
    print("\nğŸ“Š ç”Ÿæˆé¢å¤–ä¸“ä¸šå¯è§†åŒ–å›¾è¡¨...")
    create_additional_visualizations(df, user_type_map)
    
    # 6. ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆç”»åƒæŠ¥å‘Š...")
    generate_portrait_report(df, user_type_map)
    
    # 7. ä¿å­˜ç»“æœ
    output_file = "weibo_user_portrait.csv"
    df.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"\nğŸ’¾ å·²ä¿å­˜åˆ†æç»“æœ: {output_file}")
    
    print("\nâœ… å—ä¼—ç”»åƒåˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()

