"""æ£€æŸ¥æ•°æ®è´¨é‡å¹¶ç»™å‡ºå»ºè®®"""
import json
import pandas as pd

# åŠ è½½æ•°æ®
with open('weibo_data_20251218_012526.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

df = pd.DataFrame(data)

print("=" * 70)
print("æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š")
print("=" * 70)
print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡:")
print(f"   æ€»è®°å½•æ•°: {len(df)} æ¡")
print(f"   ç”¨æˆ·æ•°: {df['user'].nunique() if 'user' in df.columns else 0}")
print(f"   å…³é”®è¯æ•°: {df['keyword'].nunique() if 'keyword' in df.columns else 0}")

# å…³é”®è¯åˆ†å¸ƒ
if 'keyword' in df.columns:
    keyword_counts = df['keyword'].value_counts()
    print(f"\nğŸ“Œ å…³é”®è¯åˆ†å¸ƒï¼ˆå‰10ï¼‰:")
    for kw, count in keyword_counts.head(10).items():
        print(f"   {kw}: {count}æ¡ ({count/len(df)*100:.1f}%)")

# ç”¨æˆ·åˆ†å¸ƒ
if 'user' in df.columns:
    user_counts = df['user'].value_counts()
    print(f"\nğŸ‘¥ ç”¨æˆ·åˆ†å¸ƒ:")
    print(f"   å¹³å‡æ¯ä¸ªç”¨æˆ·å‘å¸–æ•°: {len(df) / len(user_counts):.1f}")
    print(f"   å‘å¸–æ•°â‰¥3çš„ç”¨æˆ·: {(user_counts >= 3).sum()}ä¸ª")
    print(f"   åªå‘1æ¡çš„ç”¨æˆ·: {(user_counts == 1).sum()}ä¸ª")

# äº’åŠ¨æ•°æ®ç»Ÿè®¡
if 'reposts' in df.columns and 'comments' in df.columns and 'likes' in df.columns:
    df['reposts'] = pd.to_numeric(df['reposts'], errors='coerce').fillna(0)
    df['comments'] = pd.to_numeric(df['comments'], errors='coerce').fillna(0)
    df['likes'] = pd.to_numeric(df['likes'], errors='coerce').fillna(0)
    
    print(f"\nğŸ’¬ äº’åŠ¨æ•°æ®:")
    print(f"   å¹³å‡è½¬å‘: {df['reposts'].mean():.1f}")
    print(f"   å¹³å‡è¯„è®º: {df['comments'].mean():.1f}")
    print(f"   å¹³å‡ç‚¹èµ: {df['likes'].mean():.1f}")
    print(f"   æœ‰äº’åŠ¨çš„å¾®åš: {(df['reposts'] + df['comments'] + df['likes'] > 0).sum()}æ¡ ({(df['reposts'] + df['comments'] + df['likes'] > 0).sum()/len(df)*100:.1f}%)")

# æ•°æ®è´¨é‡è¯„ä¼°
print(f"\nğŸ“ˆ æ•°æ®è´¨é‡è¯„ä¼°:")
quality_issues = []

if len(df) < 500:
    quality_issues.append(f"âš ï¸ æ•°æ®é‡è¾ƒå°‘ï¼ˆ{len(df)}æ¡ï¼‰ï¼Œå»ºè®®è‡³å°‘500æ¡ä»¥ä¸Š")
    
if df['user'].nunique() < 100:
    quality_issues.append(f"âš ï¸ ç”¨æˆ·æ•°è¾ƒå°‘ï¼ˆ{df['user'].nunique()}ä¸ªï¼‰ï¼Œå»ºè®®è‡³å°‘100ä¸ªä»¥ä¸Š")
    
# æ£€æŸ¥å…³é”®è¯åˆ†å¸ƒ
if 'keyword' in df.columns:
    keyword_counts = df['keyword'].value_counts()
    top_5_ratio = keyword_counts.head(5).sum() / len(df)
    if top_5_ratio > 0.7:
        quality_issues.append(f"âš ï¸ å…³é”®è¯åˆ†å¸ƒä¸å‡ï¼ˆå‰5ä¸ªå…³é”®è¯å {top_5_ratio*100:.1f}%ï¼‰")

if quality_issues:
    print("   å‘ç°ä»¥ä¸‹é—®é¢˜:")
    for issue in quality_issues:
        print(f"   {issue}")
else:
    print("   âœ… æ•°æ®è´¨é‡è‰¯å¥½")

# æ”¹è¿›å»ºè®®
print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
print(f"   1. æ‰©å¤§æ•°æ®æ”¶é›†:")
print(f"      â€¢ å½“å‰{len(df)}æ¡ï¼Œå»ºè®®è‡³å°‘1000-2000æ¡")
print(f"      â€¢ å¯ä»¥é€šè¿‡å¢åŠ å…³é”®è¯ã€å¢åŠ ç¿»é¡µæ•°æ¥è·å–æ›´å¤šæ•°æ®")
print(f"   2. æ‰©å¤§å…³é”®è¯èŒƒå›´:")
print(f"      â€¢ å½“å‰{df['keyword'].nunique() if 'keyword' in df.columns else 0}ä¸ªå…³é”®è¯")
print(f"      â€¢ å»ºè®®æ·»åŠ æ›´å¤šç›¸å…³å…³é”®è¯ï¼ˆå¦‚ï¼šæƒ…æ„Ÿå’¨è¯¢ã€å¿ƒç†åˆ†æã€MBTIç­‰ï¼‰")
print(f"   3. ä¼˜åŒ–åˆ†æç­–ç•¥:")
if len(df) < 300:
    print(f"      â€¢ å½“å‰æ•°æ®é‡è¾ƒå°‘ï¼Œåˆ†æç»“æœä»…ä¾›å‚è€ƒ")
    print(f"      â€¢ å¯ä»¥æ”¾å®½ç­›é€‰æ¡ä»¶ï¼Œä½¿ç”¨æ‰€æœ‰{len(df)}æ¡æ•°æ®è¿›è¡Œåˆæ­¥åˆ†æ")
else:
    print(f"      â€¢ å½“å‰æ•°æ®é‡å¯ä»¥è¿›è¡Œåˆ†æï¼Œä½†æ ·æœ¬é‡è¶Šå¤§ç»“æœè¶Šå¯é ")

print(f"\n" + "=" * 70)

