import json
import re
import jieba
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# ===== matplotlib è‹±æ–‡å­—ä½“è®¾ç½® =====
plt.rcParams["font.sans-serif"] = ["Arial"]
plt.rcParams["axes.unicode_minus"] = False

# =====================================================
# 1. Load data
# =====================================================
DATA_FILE = 'weibo_data_20251218_163102.json'

print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {DATA_FILE}")
with open(DATA_FILE, 'r', encoding='utf-8') as f:
    data = json.load(f)

df = pd.DataFrame(data)
print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡å¾®åšæ•°æ®")

# ç¡®ä¿åˆ—åä¸€è‡´
if 'reposts_count' not in df.columns:
    if 'reposts' in df.columns:
        df['reposts_count'] = df['reposts']
    else:
        df['reposts_count'] = 0
    
if 'comments_count' not in df.columns:
    if 'comments' in df.columns:
        df['comments_count'] = df['comments']
    else:
        df['comments_count'] = 0
    
if 'attitudes_count' not in df.columns:
    if 'likes' in df.columns:
        df['attitudes_count'] = df['likes']
    else:
        df['attitudes_count'] = 0

print(f"ğŸ“Š æ•°æ®åˆ—: {df.columns.tolist()}")
print(f"ğŸ“Š æ•°æ®é¢„è§ˆ:\n{df.head()}")

# =====================================================
# 2. Text cleaning
# =====================================================
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'@.*?\s', '', text)
    text = re.sub(r'#.*?#', '', text)
    return text.strip()

df['clean_text'] = df['text'].apply(clean_text)

# =====================================================
# 2.1 Decision scene tagging (ä¼˜åŒ–ç‰ˆ - æ‰©å±•å…³é”®è¯ä»¥é™ä½Otheræ¯”ä¾‹)
# =====================================================
decision_scenes = {
    "Emotion": [
        "å¤åˆ", "åˆ†æ‰‹", "æ‹çˆ±", "å–œæ¬¢", "å‰ä»»", "æš§æ˜§", "æ¡ƒèŠ±", "å©šå§»", 
        "æ„Ÿæƒ…", "æƒ…æ„Ÿ", "çˆ±æƒ…", "å¯¹è±¡", "æ‹äºº", "æƒ…ä¾£", "ç›¸äº²", "è„±å•",
        "è¿½æ±‚", "è¡¨ç™½", "æš—æ‹", "å•æ‹", "å¤±æ‹", "æŒ½å›", "å’Œå¥½", "å†·æˆ˜",
        "åµæ¶", "çŸ›ç›¾", "å¼‚åœ°", "å¼‚åœ°æ‹", "ç»“å©š", "ç¦»å©š", "å•èº«"
    ],
    "Study": [
        "è€ƒè¯•", "è€ƒç ”", "æ¯•ä¸š", "è®ºæ–‡", "å¤ä¹ ", "å››å…­çº§", "æ•™èµ„", "ä¸“å››",
        "å­¦ä¹ ", "å¤‡è€ƒ", "ä¸Šå²¸", "å½•å–", "æˆç»©", "æŒ‚ç§‘", "è¡¥è€ƒ", "é‡ä¿®",
        "æœŸæœ«", "æœŸä¸­", "ä½œä¸š", "è¯¾ç¨‹", "ä¸“ä¸š", "é€‰è¯¾", "é€‰ä¸“ä¸š", "è½¬ä¸“ä¸š",
        "ä¿ç ”", "å‡ºå›½", "ç•™å­¦", "è¯­è¨€", "è‹±è¯­", "æ‰˜ç¦", "é›…æ€", "GRE",
        "å­¦å†", "å­¦ä½", "è¯ä¹¦", "èµ„æ ¼è¯", "æ•™å¸ˆ", "å…¬åŠ¡å‘˜", "äº‹ä¸šå•ä½"
    ],
    "Career": [
        "å·¥ä½œ", "é¢è¯•", "æ±‚èŒ", "offer", "è·³æ§½", "äº‹ä¸š", "å²—ä½",
        "èŒä¸š", "å°±ä¸š", "åº”è˜", "ç®€å†", "HR", "è–ªèµ„", "å·¥èµ„", "è–ªæ°´",
        "è½¬æ­£", "å®ä¹ ", "è¯•ç”¨æœŸ", "ç¦»èŒ", "è¾èŒ", "è¢«è¾", "è£å‘˜",
        "å‡èŒ", "åŠ è–ª", "åŒäº‹", "é¢†å¯¼", "è€æ¿", "å›¢é˜Ÿ", "é¡¹ç›®",
        "åˆ›ä¸š", "å…¬å¸", "ä¼ä¸š", "è¡Œä¸š", "èŒä½", "æ‹›è˜", "æŠ•é€’"
    ],
    "Daily": [
        "æ°´é€†", "è¿åŠ¿", "é€‰æ‹©", "å¥åº·", "å‡ºè¡Œ", "ä»Šå¤©", "æœ¬å‘¨",
        "æ˜Ÿåº§", "å åœ", "å¡”ç½—", "æ˜Ÿç›˜", "å æ˜Ÿ", "å‘½ç†", "ç„å­¦",
        "é¢„æµ‹", "åˆ†æ", "å»ºè®®", "æŒ‡å¯¼", "å’¨è¯¢", "ç­”ç–‘", "è§£æƒ‘",
        "è¿·èŒ«", "å›°æƒ‘", "ç„¦è™‘", "å‹åŠ›", "çƒ¦æ¼", "çº ç»“", "çŠ¹è±«",
        "å†³å®š", "å†³ç­–", "é€‰æ‹©å›°éš¾", "äººç”Ÿ", "æœªæ¥", "è§„åˆ’", "ç›®æ ‡",
        "ç”Ÿæ´»", "æ—¥å¸¸", "ä¹ æƒ¯", "æ”¹å˜", "æ”¹å–„", "æå‡", "æˆé•¿"
    ]
}

def tag_scene(text):
    """åœºæ™¯æ ‡è®°å‡½æ•° - æ”¹è¿›ç‰ˆï¼šå…è®¸å¤šæ ‡ç­¾ï¼Œé™ä½Otheræ¯”ä¾‹"""
    if not isinstance(text, str) or len(text.strip()) == 0:
        return 'Other'
    
    hits = []
    # ç»Ÿè®¡æ¯ä¸ªåœºæ™¯çš„åŒ¹é…æ•°é‡
    scene_scores = {}
    
    for scene, words in decision_scenes.items():
        count = sum(1 for w in words if w in text)
        if count > 0:
            scene_scores[scene] = count
            hits.append(scene)
    
    # å¦‚æœæœ‰åŒ¹é…ï¼Œè¿”å›åŒ¹é…çš„åœºæ™¯ï¼ˆæŒ‰åŒ¹é…åº¦æ’åºï¼‰
    if hits:
        # æŒ‰åŒ¹é…æ•°é‡æ’åºï¼Œè¿”å›å‰2ä¸ªæœ€é‡è¦çš„åœºæ™¯
        sorted_scenes = sorted(scene_scores.items(), key=lambda x: x[1], reverse=True)
        return ','.join([s[0] for s in sorted_scenes[:2]])
    
    # å¦‚æœæ²¡æœ‰ä»»ä½•åŒ¹é…ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…ï¼ˆæ£€æŸ¥å…³é”®è¯çš„å­ä¸²ï¼‰
    # è¿™æ˜¯æœ€åçš„æ‰‹æ®µï¼Œå°½é‡é™ä½Otherçš„æ¯”ä¾‹
    loose_keywords = {
        "Emotion": ["çˆ±", "æƒ…", "æ‹", "å©š"],
        "Study": ["å­¦", "è€ƒ", "è¯•", "ä¹¦"],
        "Career": ["å·¥", "èŒ", "ä¸š", "ä½œ"],
        "Daily": ["è¿", "åŠ¿", "æ˜Ÿ", "å ", "é—®", "é¢˜", "æƒ³", "è¦"]
    }
    
    for scene, loose_words in loose_keywords.items():
        if any(w in text for w in loose_words):
            return scene  # ä½¿ç”¨å®½æ¾åŒ¹é…ï¼Œè‡³å°‘ç»™ä¸€ä¸ªåˆ†ç±»
    
    return 'Other'

df['scene_tag'] = df['clean_text'].apply(tag_scene)

# ç»Ÿè®¡åœºæ™¯åˆ†å¸ƒï¼ˆå¤„ç†å¤šæ ‡ç­¾æƒ…å†µï¼‰
scene_dist = df['scene_tag'].str.split(',').explode().value_counts()

# æ‰“å°ç»Ÿè®¡ä¿¡æ¯
print(f"\nğŸ“Š åœºæ™¯åˆ†å¸ƒç»Ÿè®¡:")
print(f"   æ€»æ ·æœ¬æ•°: {len(df)}")
print(f"   åœºæ™¯æ ‡ç­¾åˆ†å¸ƒ:")
print("   " + "="*50)
scene_dist_dict = {}
for scene, count in scene_dist.items():
    ratio = count / len(df) * 100
    scene_dist_dict[scene] = {'count': count, 'ratio': ratio}
    print(f"     {scene:15s}: {count:4d} æ¡ ({ratio:5.2f}%)")
print("   " + "="*50)

# è®¡ç®—Otherçš„æ¯”ä¾‹
other_ratio = scene_dist.get('Other', 0) / len(df) * 100
if other_ratio > 20:
    print(f"\nâš ï¸ æ³¨æ„: Otheræ¯”ä¾‹è¾ƒé«˜ ({other_ratio:.1f}%)")
    print(f"   å»ºè®®: å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•å…³é”®è¯åˆ—è¡¨æˆ–è°ƒæ•´åŒ¹é…é€»è¾‘")
else:
    print(f"\nâœ… Otheræ¯”ä¾‹: {other_ratio:.1f}% (å·²ä¼˜åŒ–)")

# ---- Pie chart ----
plt.figure(figsize=(10, 10))
scene_dist.plot.pie(
    autopct='%1.1f%%',
    startangle=90,
    textprops={'fontsize': 10}
)
plt.title("Distribution of Decision Scenarios in Mystic-related Posts", fontsize=14, fontweight='bold')
plt.ylabel("")
plt.tight_layout()
plt.savefig('weibo_scenario_distribution.png', dpi=300, bbox_inches='tight')
print("\nğŸ’¾ å·²ä¿å­˜å†³ç­–åœºæ™¯åˆ†å¸ƒå›¾: weibo_scenario_distribution.png")
print("   ğŸ“Š å›¾ç‰‡ä¸­çš„å…·ä½“æ•°å€¼:")
for scene in scene_dist.index:
    count = scene_dist[scene]
    ratio = count / len(df) * 100
    print(f"      {scene}: {count}æ¡ ({ratio:.1f}%)")
plt.show()

# =====================================================
# 2.3 Topic Modeling (LDA)
# =====================================================
def cut_words(text):
    return ' '.join(jieba.cut(text))

df['cut_text'] = df['clean_text'].apply(cut_words)

vectorizer = CountVectorizer(min_df=5, max_df=0.8)
dtm = vectorizer.fit_transform(df['cut_text'])

lda = LatentDirichletAllocation(
    n_components=4,
    random_state=42
)
lda.fit(dtm)

print("\nğŸ“Š LDAä¸»é¢˜å»ºæ¨¡ç»“æœ:")
print("   " + "="*60)
for idx, topic in enumerate(lda.components_):
    words = [vectorizer.get_feature_names_out()[i]
             for i in topic.argsort()[:-11:-1]]
    print(f"   Topic {idx}: {' '.join(words)}")
print("   " + "="*60)

# =====================================================
# 2.4 Sentiment analysis
# =====================================================
positive_words = ['é¡ºåˆ©', 'å¼€å¿ƒ', 'å¸Œæœ›', 'æˆåŠŸ', 'ä¸Šå²¸', 'å¹¸è¿', 'æœŸå¾…']
negative_words = ['ç„¦è™‘', 'éš¾å—', 'å´©æºƒ', 'å®³æ€•', 'è¿·èŒ«', 'å¤±è´¥', 'å‹åŠ›', 'emo']

def sentiment_score(text):
    pos = sum(1 for w in positive_words if w in text)
    neg = sum(1 for w in negative_words if w in text)
    return pos - neg

df['sentiment_score'] = df['clean_text'].apply(sentiment_score)

scene_sentiment = df.groupby('scene_tag')['sentiment_score'].mean()

# æ‰“å°æƒ…æ„Ÿå¾—åˆ†ç»Ÿè®¡
print(f"\nğŸ“Š å„åœºæ™¯æƒ…æ„Ÿå¾—åˆ†ç»Ÿè®¡:")
print("   " + "="*60)
print(f"   {'åœºæ™¯':15s} {'å¹³å‡æƒ…æ„Ÿå¾—åˆ†':12s} {'è§£é‡Š':30s}")
print("   " + "-"*60)
scene_sentiment_sorted = scene_sentiment.sort_values()
for scene, score in scene_sentiment_sorted.items():
    interpretation = "æ­£é¢æƒ…ç»ª" if score > 0 else "è´Ÿé¢æƒ…ç»ªï¼ˆç„¦è™‘ï¼‰" if score < 0 else "ä¸­æ€§"
    print(f"   {scene:15s} {score:12.3f} {interpretation:30s}")
print("   " + "="*60)
print("   è¯´æ˜: æ­£å€¼=æ­£é¢æƒ…ç»ªï¼Œè´Ÿå€¼=è´Ÿé¢æƒ…ç»ª/ç„¦è™‘ï¼Œ0=ä¸­æ€§")

# ---- Bar chart (sentiment) ----
plt.figure(figsize=(8, 5))
scene_sentiment.sort_values().plot(kind='barh')
plt.axvline(0, color='gray', linestyle='--')
plt.title("Average Sentiment Score by Decision Scenario")
plt.xlabel("Sentiment Score (Negative = Anxiety)")
plt.ylabel("Decision Scenario")
plt.tight_layout()
plt.savefig('weibo_scenario_sentiment.png', dpi=300, bbox_inches='tight')
print("\nğŸ’¾ å·²ä¿å­˜å„åœºæ™¯æƒ…æ„Ÿåˆ†æå›¾: weibo_scenario_sentiment.png")
print("   ğŸ“Š å›¾ç‰‡ä¸­çš„å…·ä½“æ•°å€¼:")
for scene, score in scene_sentiment_sorted.items():
    print(f"      {scene}: {score:.3f}")
plt.show()

# =====================================================
# 2.5 Mystic Dependence Index
# =====================================================
mystic_words = ['æ˜Ÿåº§', 'å¡”ç½—', 'å åœ', 'æ˜¾åŒ–', 'è¿åŠ¿', 'å®‡å®™', 'æ°´é€†', 'ç„å­¦']

def mystic_density(text):
    return sum(1 for w in mystic_words if w in text)

df['mystic_density'] = df['clean_text'].apply(mystic_density)

df['interaction_score'] = (
    df['reposts_count'] +
    df['comments_count'] * 2 +
    df['attitudes_count'] * 0.5
)

depend_features = df[['mystic_density', 'interaction_score', 'sentiment_score']].fillna(0)
depend_scaled = StandardScaler().fit_transform(depend_features)

df['depend_index'] = (
    depend_scaled[:, 0] * 0.4 +
    depend_scaled[:, 1] * 0.4 +
    (-depend_scaled[:, 2]) * 0.2
)

scene_depend = df.groupby('scene_tag')['depend_index'].mean()

# æ‰“å°ä¾èµ–æŒ‡æ•°ç»Ÿè®¡ï¼ˆæŒ‰é™åºæ’åˆ—ï¼‰
scene_depend_sorted = scene_depend.sort_values(ascending=False)
print(f"\nğŸ“Š å„åœºæ™¯ç¥ç§˜ä¾èµ–æŒ‡æ•°ç»Ÿè®¡ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰:")
print("   " + "="*70)
print(f"   {'åœºæ™¯':15s} {'ä¾èµ–æŒ‡æ•°':12s} {'æ’å':6s} {'è§£é‡Š':30s}")
print("   " + "-"*70)
max_depend = scene_depend_sorted.max()
min_depend = scene_depend_sorted.min()
for rank, (scene, depend_idx) in enumerate(scene_depend_sorted.items(), 1):
    if depend_idx == max_depend:
        interpretation = "ä¾èµ–ç¨‹åº¦æœ€é«˜ â­"
    elif depend_idx == min_depend:
        interpretation = "ä¾èµ–ç¨‹åº¦æœ€ä½"
    elif depend_idx > 0:
        interpretation = "ä¾èµ–ç¨‹åº¦è¾ƒé«˜"
    else:
        interpretation = "ä¾èµ–ç¨‹åº¦è¾ƒä½"
    print(f"   {scene:15s} {depend_idx:12.4f} #{rank:<5d} {interpretation:30s}")
print("   " + "="*70)
print(f"   ä¾èµ–æŒ‡æ•°èŒƒå›´: {min_depend:.4f} ~ {max_depend:.4f}")
print(f"   æŒ‡æ•°æœ€é«˜åœºæ™¯: {scene_depend_sorted.index[0]} ({max_depend:.4f})")
print("   è¯´æ˜: ä¾èµ–æŒ‡æ•°è¶Šé«˜ï¼Œè¯´æ˜è¯¥åœºæ™¯ä¸‹ç”¨æˆ·å¯¹ç„å­¦å†…å®¹çš„éœ€æ±‚è¶Šå¼ºçƒˆ")
print("         æŒ‡æ•°è®¡ç®—åŸºäº: ç¥ç§˜è¯æ±‡å¯†åº¦(40%) + äº’åŠ¨å¼ºåº¦(40%) + è´Ÿé¢æƒ…ç»ª(20%)")

# ---- Bar chart (dependence) ----
plt.figure(figsize=(8, 5))
scene_depend.sort_values(ascending=False).plot(kind='bar')
plt.title("Mystic Dependence Index by Decision Scenario")
plt.ylabel("Dependence Index")
plt.xlabel("Decision Scenario")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('weibo_scenario_dependence.png', dpi=300, bbox_inches='tight')
print("\nğŸ’¾ å·²ä¿å­˜å„åœºæ™¯ç¥ç§˜ä¾èµ–æŒ‡æ•°å›¾: weibo_scenario_dependence.png")
print("   ğŸ“Š å›¾ç‰‡ä¸­çš„å…·ä½“æ•°å€¼ï¼ˆæŒ‰ä¾èµ–æŒ‡æ•°é™åºï¼‰:")
for scene, depend_idx in scene_depend_sorted.items():
    print(f"      {scene}: {depend_idx:.4f}")
plt.show()

# =====================================================
# 3. User clustering
# =====================================================
df['log_interaction'] = np.log10(df['interaction_score'] + 1)

X = df[['log_interaction', 'depend_index']].fillna(0)
X_scaled = StandardScaler().fit_transform(X)

kmeans = KMeans(n_clusters=3, random_state=42)
df['cluster'] = kmeans.fit_predict(X_scaled)

# æ‰“å°èšç±»ç»“æœç»Ÿè®¡
print(f"\nğŸ“Š ç”¨æˆ·èšç±»ç»“æœç»Ÿè®¡:")
print("   " + "="*70)
cluster_stats = df.groupby('cluster').agg({
    'log_interaction': ['count', 'mean', 'std'],
    'depend_index': ['mean', 'std'],
    'interaction_score': 'mean',
    'mystic_density': 'mean',
    'sentiment_score': 'mean'
}).round(4)

print(f"   {'èšç±»ID':8s} {'æ ·æœ¬æ•°':8s} {'å¯¹æ•°äº’åŠ¨(å‡å€¼)':15s} {'ä¾èµ–æŒ‡æ•°(å‡å€¼)':15s} {'äº’åŠ¨åˆ†æ•°(å‡å€¼)':15s}")
print("   " + "-"*70)
for cluster_id in sorted(df['cluster'].unique()):
    cluster_data = df[df['cluster'] == cluster_id]
    count = len(cluster_data)
    log_int_mean = cluster_data['log_interaction'].mean()
    depend_mean = cluster_data['depend_index'].mean()
    int_score_mean = cluster_data['interaction_score'].mean()
    mystic_mean = cluster_data['mystic_density'].mean()
    sent_mean = cluster_data['sentiment_score'].mean()
    ratio = count / len(df) * 100
    
    print(f"   Cluster {cluster_id:<4d} {count:8d} {log_int_mean:15.4f} {depend_mean:15.4f} {int_score_mean:15.2f}")
    print(f"             ({ratio:5.1f}%)  ç¥ç§˜è¯æ±‡å¯†åº¦: {mystic_mean:.3f}, æƒ…æ„Ÿå¾—åˆ†: {sent_mean:.3f}")

print("   " + "="*70)
print(f"   æ€»æ ·æœ¬æ•°: {len(df)}")
print(f"   èšç±»ç‰¹å¾: å¯¹æ•°äº’åŠ¨å¼ºåº¦ vs ç¥ç§˜ä¾èµ–æŒ‡æ•°")

# ---- Scatter plot ----
plt.figure(figsize=(9, 6))
scatter = plt.scatter(
    df['log_interaction'],
    df['depend_index'],
    c=df['cluster'],
    cmap='viridis',
    alpha=0.6
)
plt.xlabel("Log Interaction Intensity")
plt.ylabel("Mystic Dependence Index")
plt.title("User Clustering Based on Mystic Engagement")
plt.colorbar(scatter, label="Cluster ID")
plt.tight_layout()
plt.savefig('weibo_user_clustering.png', dpi=300, bbox_inches='tight')
print("\nğŸ’¾ å·²ä¿å­˜ç”¨æˆ·èšç±»æ•£ç‚¹å›¾: weibo_user_clustering.png")
print("   ğŸ“Š å›¾ç‰‡ä¸­çš„èšç±»ç»“æœ:")
for cluster_id in sorted(df['cluster'].unique()):
    cluster_data = df[df['cluster'] == cluster_id]
    count = len(cluster_data)
    ratio = count / len(df) * 100
    print(f"      Cluster {cluster_id}: {count}ä¸ªç”¨æˆ· ({ratio:.1f}%)")
    print(f"         - å¯¹æ•°äº’åŠ¨å¼ºåº¦èŒƒå›´: {cluster_data['log_interaction'].min():.3f} ~ {cluster_data['log_interaction'].max():.3f} (å‡å€¼: {cluster_data['log_interaction'].mean():.3f})")
    print(f"         - ä¾èµ–æŒ‡æ•°èŒƒå›´: {cluster_data['depend_index'].min():.4f} ~ {cluster_data['depend_index'].max():.4f} (å‡å€¼: {cluster_data['depend_index'].mean():.4f})")
plt.show()
