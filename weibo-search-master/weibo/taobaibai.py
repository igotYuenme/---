# ======================================
# åšä¸»ä¸‰ç»´è¯„ä¼°ï¼šæ„å»º"å†…å®¹â€”ä¼ æ’­â€”å¿ƒç†"ä¸‰ç»´è¯„ä¼°æ¡†æ¶ - ä¿®æ­£ç‰ˆ
# ======================================

import json
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# ======================================
# è¾…åŠ©å‡½æ•°
# ======================================

def calculate_gini(x):
    """è®¡ç®—åŸºå°¼ç³»æ•°"""
    x = np.sort(x)
    n = len(x)
    if n == 0:
        return 0
    cumx = np.cumsum(x, dtype=float)
    return (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n if cumx[-1] > 0 else 0

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if not isinstance(text, str):
        return ""
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'@.*?\s', '', text)
    text = re.sub(r'#.*?#', '', text)
    return text.strip()

def parse_time(time_str):
    """è§£ææ—¶é—´å­—ç¬¦ä¸²"""
    try:
        if pd.isna(time_str):
            return None
            
        if isinstance(time_str, str):
            # æ ¼å¼1: "Sun Nov 16 21:03:35 +0800 2025"
            if ' +' in time_str:
                time_str = time_str.split(' +')[0]
                try:
                    # å°è¯•æ ‡å‡†æ ¼å¼
                    dt = datetime.strptime(time_str, "%a %b %d %H:%M:%S %Y")
                    return dt
                except:
                    try:
                        # å°è¯•æ‰‹åŠ¨è§£æ
                        months = {
                            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4,
                            'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8,
                            'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12
                        }
                        
                        parts = time_str.split()
                        if len(parts) >= 5:
                            month_str = parts[1]
                            day = int(parts[2])
                            time_part = parts[3]
                            year = int(parts[4])
                            
                            if month_str in months:
                                hour, minute, second = map(int, time_part.split(':'))
                                return datetime(year, months[month_str], day, hour, minute, second)
                    except:
                        pass
            
            # æ ¼å¼2: "2025-11-16 21:03:35"
            try:
                dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
                return dt
            except:
                pass
            
            # æ ¼å¼3: "2025/11/16 21:03:35"
            try:
                dt = datetime.strptime(time_str, "%Y/%m/%d %H:%M:%S")
                return dt
            except:
                pass
        
        return None
    except Exception as e:
        return None

# ======================================
# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ======================================
def load_blogger_data(json_path="weibo_data_20251218_012526.json", blogger_name="é™¶ç™½ç™½"):
    """åŠ è½½åšä¸»ç›¸å…³æ•°æ®"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        df = pd.DataFrame(data)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åšä¸»ä¸“é—¨æ–‡ä»¶ï¼ˆé€šè¿‡æ–‡ä»¶åæˆ–keywordå­—æ®µåˆ¤æ–­ï¼‰
        import os
        import glob
        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«åšä¸»åç§°æˆ–ç›¸å…³å…³é”®è¯
        is_blogger_specific_file = (
            blogger_name in json_path or
            'é™¶' in json_path and 'ç™½' in json_path or  # åŒ…å«"é™¶"å’Œ"ç™½"çš„æ–‡ä»¶
            ('keyword' in df.columns and df['keyword'].str.contains(f'åšä¸»:', na=False).any())  # keywordå­—æ®µåŒ…å«"åšä¸»:"
        )
        
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼Œæ€»æ ·æœ¬æ•°: {len(df)}")
        print(f"  åˆ—å: {df.columns.tolist()}")
        if is_blogger_specific_file:
            print(f"  ğŸ“Œ è¯†åˆ«ä¸ºåšä¸»ä¸“é—¨æ–‡ä»¶ï¼Œæ•°æ®ä¸»è¦ä¸ºåšä¸» '{blogger_name}' çš„å¾®åš")
        
        # æ ‡å‡†åŒ–åˆ—å
        if 'reposts' in df.columns:
            df['reposts_count'] = pd.to_numeric(df['reposts'], errors='coerce').fillna(0)
        if 'comments' in df.columns:
            df['comments_count'] = pd.to_numeric(df['comments'], errors='coerce').fillna(0)
        if 'likes' in df.columns:
            df['attitudes_count'] = pd.to_numeric(df['likes'], errors='coerce').fillna(0)
        
        # æ£€æŸ¥äº’åŠ¨æ•°æ®
        total_reposts = df['reposts_count'].sum() if 'reposts_count' in df.columns else 0
        total_comments = df['comments_count'].sum() if 'comments_count' in df.columns else 0
        total_likes = df['attitudes_count'].sum() if 'attitudes_count' in df.columns else 0
        
        print(f"  æ€»äº’åŠ¨æ•°æ®: è½¬å‘{total_reposts:.0f}, "
              f"è¯„è®º{total_comments:.0f}, ç‚¹èµ{total_likes:.0f}")
        
        # æ£€æŸ¥ç”¨æˆ·åˆ†å¸ƒ
        if 'user' in df.columns:
            user_counts = df['user'].value_counts()
            print(f"  ç”¨æˆ·æ•°: {len(user_counts)}")
            print(f"  æ´»è·ƒç”¨æˆ·å‰5: {dict(user_counts.head(5))}")
            
            # è°ƒè¯•ï¼šæŸ¥æ‰¾å¯èƒ½çš„åšä¸»åç§°å˜ä½“
            if not is_blogger_specific_file:
                print(f"\n  ğŸ” è°ƒè¯•ï¼šæŸ¥æ‰¾å¯èƒ½çš„åšä¸»åç§°åŒ¹é…...")
                # æ£€æŸ¥æ˜¯å¦æœ‰åŒ…å«"é™¶"æˆ–"ç™½"çš„ç”¨æˆ·å
                possible_users = [u for u in user_counts.index if isinstance(u, str) and ('é™¶' in u or 'ç™½' in u or 'tao' in u.lower() or 'bai' in u.lower())]
                if possible_users:
                    print(f"  æ‰¾åˆ°å¯èƒ½ç›¸å…³çš„ç”¨æˆ·: {possible_users[:10]}")
                else:
                    print(f"  âš ï¸ æœªæ‰¾åˆ°åŒ…å«'é™¶'ã€'ç™½'ç­‰å­—ç¬¦çš„ç”¨æˆ·å")
        
        # 1. æœç´¢åšä¸»æœ¬äººå‘å¸ƒçš„å¾®åš
        if is_blogger_specific_file:
            # å¦‚æœæ˜¯åšä¸»ä¸“é—¨æ–‡ä»¶ï¼Œæ‰€æœ‰æ•°æ®éƒ½è§†ä¸ºåšä¸»å¾®åš
            blogger_posts = df.copy()
            print(f"\nğŸ“Š åšä¸» '{blogger_name}' ç›¸å…³å¾®åš:")
            print(f"  åšä¸»æœ¬äººå‘å¸ƒå¾®åšæ•°: {len(blogger_posts)} (æ¥è‡ªä¸“é—¨æ–‡ä»¶)")
        elif 'user' in df.columns:
            # ä»é€šç”¨æ–‡ä»¶ä¸­ç­›é€‰åšä¸»å¾®åš
            # ç²¾ç¡®åŒ¹é…åšä¸»åç§°
            blogger_posts_exact = df[df['user'] == blogger_name].copy()
            
            # æ‰©å±•åŒ¹é…æ¨¡å¼ï¼ˆè€ƒè™‘å¯èƒ½çš„å˜ä½“ï¼‰
            blogger_variants = [
                blogger_name,  # ç²¾ç¡®åŒ¹é…
                'é™¶ç™½ç™½',  # æ˜ç¡®æŒ‡å®š
                'Taobai',  # æ‹¼éŸ³
                'taobai',  # å°å†™æ‹¼éŸ³
                'TAOBAI',  # å¤§å†™æ‹¼éŸ³
            ]
            
            # æ¨¡ç³ŠåŒ¹é…ï¼ˆåŒ…å«å…³é”®è¯ï¼‰
            blogger_patterns = [
                re.compile(rf'{re.escape(blogger_name)}', re.IGNORECASE),
                re.compile(r'é™¶.*ç™½|ç™½.*é™¶', re.IGNORECASE),
                re.compile(r'tao.*bai|bai.*tao', re.IGNORECASE),
            ]
            
            blogger_posts_fuzzy = pd.DataFrame()
            for pattern in blogger_patterns:
                matched = df[df['user'].apply(lambda x: bool(pattern.search(str(x))) if pd.notna(x) else False)]
                blogger_posts_fuzzy = pd.concat([blogger_posts_fuzzy, matched])
            
            blogger_posts_fuzzy = blogger_posts_fuzzy.drop_duplicates(subset=['id'] if 'id' in df.columns else None)
            
            # åˆå¹¶ç²¾ç¡®åŒ¹é…å’Œæ¨¡ç³ŠåŒ¹é…çš„ç»“æœ
            blogger_posts = pd.concat([blogger_posts_exact, blogger_posts_fuzzy]).drop_duplicates(subset=['id'] if 'id' in df.columns else None)
            
            print(f"\nğŸ“Š åšä¸» '{blogger_name}' ç›¸å…³å¾®åš:")
            print(f"  åšä¸»æœ¬äººå‘å¸ƒå¾®åšæ•°: {len(blogger_posts)} (ç²¾ç¡®åŒ¹é…:{len(blogger_posts_exact)}, æ¨¡ç³ŠåŒ¹é…:{len(blogger_posts_fuzzy)})")
            
            if len(blogger_posts) == 0:
                print(f"  âš ï¸ æœªåœ¨æ•°æ®ä¸­æ‰¾åˆ°åšä¸»æœ¬äººçš„å¾®åš")
                print(f"  ğŸ’¡ é‡è¦æç¤ºï¼š")
                print(f"     å½“å‰ä½¿ç”¨çš„æ˜¯é€šç”¨å…³é”®è¯æœç´¢æ•°æ®ï¼Œä¸åŒ…å«åšä¸»æœ¬äººçš„å¾®åš")
                print(f"     è¦åˆ†æåšä¸»æœ¬äººçš„å†…å®¹ï¼Œè¯·å…ˆè¿è¡Œæ”¶é›†è„šæœ¬ï¼š")
                print(f"     python collect_taobaibai_weibo.py")
                print(f"     è¿™å°†ä¸“é—¨æ”¶é›†åšä¸» '{blogger_name}' çš„å¾®åšå¹¶ç”Ÿæˆä¸“é—¨çš„æ•°æ®æ–‡ä»¶")
            
            # å¦‚æœåšä¸»å¾®åšæ•°æ®å……è¶³ï¼Œä¼˜å…ˆä½¿ç”¨åšä¸»æœ¬äººçš„å¾®åšè¿›è¡Œåˆ†æ
            if len(blogger_posts) >= 20:
                print(f"  âœ… åšä¸»æœ¬äººå¾®åšæ•°æ®å……è¶³ï¼ˆ{len(blogger_posts)}æ¡ï¼‰ï¼Œå°†ä¼˜å…ˆåˆ†æåšä¸»å†…å®¹")
            elif len(blogger_posts) > 0:
                print(f"  âš ï¸ åšä¸»æœ¬äººå¾®åšè¾ƒå°‘ï¼ˆ{len(blogger_posts)}æ¡ï¼‰ï¼Œå°†åˆå¹¶å…¶ä»–ç›¸å…³å¾®åšè¿›è¡Œåˆ†æ")
        else:
            blogger_posts = pd.DataFrame()
            print(f"\nğŸ“Š åšä¸» '{blogger_name}' ç›¸å…³å¾®åš:")
            print(f"  åšä¸»æœ¬äººå‘å¸ƒå¾®åšæ•°: {len(blogger_posts)} (æ— æ³•è¯†åˆ«ç”¨æˆ·å­—æ®µ)")
        
        # 2. æœç´¢æåŠåšä¸»çš„å¾®åš
        mention_patterns = [
            r'é™¶ç™½ç™½', r'#é™¶ç™½ç™½#', r'@é™¶ç™½ç™½', r'é™¶ç™½ç™½è€å¸ˆ', 
            r'é™¶ç™½ç™½è¯´', r'é™¶ç™½ç™½æ˜Ÿåº§', r'taobaibai'
        ]
        mention_posts = pd.DataFrame()
        if 'text' in df.columns:
            for pattern in mention_patterns:
                matched = df[df['text'].str.contains(pattern, na=False, regex=True)]
                mention_posts = pd.concat([mention_posts, matched])
            mention_posts = mention_posts.drop_duplicates()
            print(f"  æ˜ç¡®æåŠåšä¸»å¾®åšæ•°: {len(mention_posts)}")
        
        # 3. åšä¸»ç›¸å…³è¯é¢˜çš„å¾®åšï¼ˆæ‰©å±•å…³é”®è¯èŒƒå›´ä»¥æé«˜æ•°æ®è¦†ç›–ç‡ï¼‰
        blogger_keywords = ['æ˜Ÿåº§è¿åŠ¿', 'æ˜Ÿåº§', 'è¿åŠ¿', 'æ°´é€†', 'MBTI', 'å¡”ç½—', 'å åœ', 
                           'å¤åˆ', 'åˆ†æ‰‹', 'æ‹çˆ±', 'æƒ…æ„Ÿ', 'æƒ…æ„Ÿå’¨è¯¢', 'æƒ…æ„Ÿåˆ†æ',
                           'å¿ƒç†', 'æ€§æ ¼', 'äººæ ¼', 'æµ‹è¯•', 'åˆ†æ', 'é¢„æµ‹', 'å»ºè®®',
                           'å’¨è¯¢', 'æŒ‡å¯¼', 'å¸®åŠ©', 'è§£æƒ‘', 'ç­”ç–‘', 'è§£ç­”']
        keyword_posts = pd.DataFrame()
        if 'text' in df.columns:
            for keyword in blogger_keywords:
                matched = df[df['text'].str.contains(keyword, na=False)]
                keyword_posts = pd.concat([keyword_posts, matched])
            keyword_posts = keyword_posts.drop_duplicates()
            print(f"  ç›¸å…³è¯é¢˜å¾®åšæ•°: {len(keyword_posts)}")
        
        # 4. æ˜Ÿåº§ç›¸å…³å¾®åš
        zodiac_keywords = [
            'ç™½ç¾Šåº§', 'é‡‘ç‰›åº§', 'åŒå­åº§', 'å·¨èŸ¹åº§', 'ç‹®å­åº§', 'å¤„å¥³åº§',
            'å¤©ç§¤åº§', 'å¤©èåº§', 'å°„æ‰‹åº§', 'æ‘©ç¾¯åº§', 'æ°´ç“¶åº§', 'åŒé±¼åº§',
            'ç™½ç¾Š', 'é‡‘ç‰›', 'åŒå­', 'å·¨èŸ¹', 'ç‹®å­', 'å¤„å¥³',
            'å¤©ç§¤', 'å¤©è', 'å°„æ‰‹', 'æ‘©ç¾¯', 'æ°´ç“¶', 'åŒé±¼'
        ]
        zodiac_posts = pd.DataFrame()
        if 'text' in df.columns:
            for keyword in zodiac_keywords:
                matched = df[df['text'].str.contains(keyword, na=False)]
                zodiac_posts = pd.concat([zodiac_posts, matched])
            zodiac_posts = zodiac_posts.drop_duplicates()
            print(f"  æ˜Ÿåº§ç›¸å…³å¾®åšæ•°: {len(zodiac_posts)}")
        
        # 5. åˆå¹¶åˆ†ææ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨åšä¸»æœ¬äººå¾®åšï¼‰
        if is_blogger_specific_file:
            # åšä¸»ä¸“é—¨æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æ•°æ®
            all_related_posts = blogger_posts.copy()
            print(f"  ğŸ’¡ ä½¿ç”¨ç­–ç•¥ï¼šä½¿ç”¨åšä¸»ä¸“é—¨æ–‡ä»¶ä¸­çš„æ‰€æœ‰å¾®åšï¼ˆ{len(all_related_posts)}æ¡ï¼‰")
        elif len(blogger_posts) >= 30:
            # åšä¸»å¾®åšå……è¶³ï¼Œä¸»è¦ä½¿ç”¨åšä¸»å¾®åšï¼Œè¡¥å……ä¸€äº›ç›¸å…³å¾®åš
            print(f"  ğŸ’¡ ä½¿ç”¨ç­–ç•¥ï¼šä»¥åšä¸»æœ¬äººå¾®åšä¸ºä¸»ï¼ˆ{len(blogger_posts)}æ¡ï¼‰ï¼Œè¡¥å……ç›¸å…³å¾®åš")
            # åˆå¹¶æ—¶ï¼Œåšä¸»å¾®åšä¼˜å…ˆ
            all_related_posts = pd.concat([
                blogger_posts, 
                mention_posts, 
                keyword_posts.head(100) if len(keyword_posts) > 100 else keyword_posts,  # é™åˆ¶å…¶ä»–å¾®åšæ•°é‡
            ]).drop_duplicates(subset=['id'] if 'id' in df.columns else None)
        else:
            # åšä¸»å¾®åšä¸è¶³ï¼Œåˆå¹¶æ‰€æœ‰ç›¸å…³å¾®åš
            print(f"  ğŸ’¡ ä½¿ç”¨ç­–ç•¥ï¼šåˆå¹¶æ‰€æœ‰ç›¸å…³å¾®åšï¼ˆåšä¸»{len(blogger_posts)}æ¡ + ç›¸å…³å¾®åšï¼‰")
            all_related_posts = pd.concat([
                blogger_posts, 
                mention_posts, 
                keyword_posts, 
                zodiac_posts
            ]).drop_duplicates(subset=['id'] if 'id' in df.columns else None)
        
        print(f"\nğŸ“Š ç»¼åˆåˆ†ææ•°æ®ç»Ÿè®¡:")
        print(f"  åˆå¹¶å»é‡ååˆ†ææ•°æ®: {len(all_related_posts)}æ¡")
        print(f"  æ•°æ®è¦†ç›–ç‡: {len(all_related_posts)/len(df)*100:.1f}%")
        
        # æ•°æ®é‡è¯„ä¼°
        if len(all_related_posts) < 200:
            print(f"  âš ï¸ æ•°æ®é‡è¾ƒå°‘ï¼ˆ{len(all_related_posts)}æ¡ï¼‰ï¼Œå»ºè®®è‡³å°‘200æ¡ä»¥ä¸Šè·å¾—æ›´å¯é çš„åˆ†æç»“æœ")
        elif len(all_related_posts) < 500:
            print(f"  âš ï¸ æ•°æ®é‡ä¸­ç­‰ï¼ˆ{len(all_related_posts)}æ¡ï¼‰ï¼Œå»ºè®®æ”¶é›†æ›´å¤šæ•°æ®ä»¥æé«˜åˆ†æå‡†ç¡®æ€§")
        else:
            print(f"  âœ… æ•°æ®é‡å……è¶³ï¼ˆ{len(all_related_posts)}æ¡ï¼‰ï¼Œå¯ä»¥è¿›è¡Œåˆ†æ")
        
        # å¦‚æœç›¸å…³æ•°æ®å¤ªå°‘ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œåˆ†æï¼ˆæ·»åŠ æ ‡è®°ï¼‰
        use_all_data = False
        if len(all_related_posts) < 50:
            print(f"  ğŸ’¡ ç›¸å…³æ•°æ®è¿‡å°‘ï¼Œå°†ä½¿ç”¨å…¨éƒ¨{len(df)}æ¡æ•°æ®è¿›è¡Œåˆæ­¥åˆ†æ")
            all_related_posts = df.copy()
            use_all_data = True
        
        # æ£€æŸ¥å…³é”®è¯è¦†ç›–ç‡
        if len(all_related_posts) > 0 and 'text' in all_related_posts.columns:
            text_sample = all_related_posts['text'].str.cat(sep=' ')
            keyword_coverage = {}
            for keyword in blogger_keywords[:10]:  # æ£€æŸ¥å‰10ä¸ªå…³é”®è¯
                count = text_sample.count(keyword)
                if count > 0:
                    keyword_coverage[keyword] = count
            print(f"  é«˜é¢‘å…³é”®è¯: {dict(Counter(keyword_coverage).most_common(5))}")
        
        # æ£€æŸ¥äº’åŠ¨æ•°æ®å¯ç”¨æ€§
        interaction_available = False
        if 'reposts_count' in all_related_posts.columns:
            total_interaction = all_related_posts['reposts_count'].sum() + \
                              all_related_posts['comments_count'].sum() + \
                              all_related_posts['attitudes_count'].sum()
            interaction_available = total_interaction > 0
        
        return {
            'blogger_posts': blogger_posts,
            'mention_posts': mention_posts,
            'keyword_posts': keyword_posts,
            'zodiac_posts': zodiac_posts,
            'analysis_posts': all_related_posts,
            'all_data': df,
            'data_summary': {
                'total_posts': len(df),
                'analysis_posts': len(all_related_posts),
                'interaction_data_available': interaction_available
            }
        }
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

# ======================================
# 2. ä¸‰ç»´è¯„ä¼°æ¡†æ¶
# ======================================

def enhanced_content_analysis(analysis_data, blogger_name="é™¶ç™½ç™½"):
    """å¢å¼ºçš„å†…å®¹ç»´åº¦åˆ†æ"""
    if len(analysis_data) == 0:
        print("âš ï¸ æ²¡æœ‰åˆ†ææ•°æ®")
        return None
    
    print(f"ğŸ” æ‰§è¡Œå¢å¼ºå†…å®¹åˆ†æï¼Œæ ·æœ¬æ•°: {len(analysis_data)}")
    
    # æ¸…ç†æ–‡æœ¬
    analysis_data['clean_text'] = analysis_data['text'].apply(clean_text)
    
    content_metrics = {}
    
    # 1. å†…å®¹å½¢å¼åˆ†æ
    text_lengths = analysis_data['clean_text'].apply(lambda x: len(x))
    content_metrics['text_length'] = {
        'mean': text_lengths.mean(),
        'median': text_lengths.median(),
        'std': text_lengths.std(),
        'min': text_lengths.min(),
        'max': text_lengths.max()
    }
    
    # å¾®åšé•¿åº¦åˆ†å¸ƒ
    length_bins = [0, 50, 100, 140, 200, 500, float('inf')]
    length_labels = ['è¶…çŸ­(<50)', 'çŸ­(50-100)', 'ä¸­ç­‰(100-140)', 'é•¿(140-200)', 'è¾ƒé•¿(200-500)', 'è¶…é•¿(>500)']
    length_dist = pd.cut(text_lengths, bins=length_bins, labels=length_labels).value_counts()
    content_metrics['length_distribution'] = (length_dist / len(analysis_data)).to_dict()
    
    # 2. å†…å®¹ä¸»é¢˜æ·±åº¦åˆ†æ
    # é™¶ç™½ç™½çš„æ ¸å¿ƒä¸»é¢˜
    themes = {
        'æ˜Ÿåº§è¿åŠ¿': ['æ˜Ÿåº§', 'è¿åŠ¿', 'æ°´é€†', 'æ˜Ÿåº§è¿åŠ¿', 'æœ¬å‘¨è¿åŠ¿', 'ä¸‹å‘¨è¿åŠ¿', 'æœ¬æœˆè¿åŠ¿', 'å¹´è¿'],
        'æƒ…æ„Ÿå’¨è¯¢': ['å¤åˆ', 'åˆ†æ‰‹', 'æ‹çˆ±', 'å–œæ¬¢', 'å‰ä»»', 'æš§æ˜§', 'æ¡ƒèŠ±', 'å©šå§»', 'æ„Ÿæƒ…', 'æƒ…æ„Ÿ'],
        'èŒä¸šå‘å±•': ['offer', 'é¢è¯•', 'æ±‚èŒ', 'å·¥ä½œ', 'äº‹ä¸š', 'å²—ä½', 'æ‹›è˜', 'ç®€å†', 'HR'],
        'å­¦ä¸šæŒ‡å¯¼': ['è€ƒè¯•', 'è€ƒç ”', 'æ¯•ä¸š', 'è®ºæ–‡', 'å¤ä¹ ', 'å››å…­çº§', 'æ•™èµ„', 'å­¦ä¹ ', 'å¤‡è€ƒ', 'ä¸Šå²¸'],
        'å¿ƒç†åˆ†æ': ['MBTI', 'æ˜¾åŒ–', 'å¸å¼•åŠ›æ³•åˆ™', 'å¡”ç½—', 'å åœ', 'å¿ƒç†', 'æ€§æ ¼', 'äººæ ¼'],
        'è¡ŒåŠ¨æŒ‡å¯¼': ['å»ºè®®', 'åº”è¯¥', 'éœ€è¦', 'å¯ä»¥', 'æ–¹æ³•', 'æ­¥éª¤', 'æ¸…å•', 'æŒ‡å—', 'å¦‚ä½•']
    }
    
    theme_analysis = {}
    for theme, keywords in themes.items():
        # è®¡ç®—ä¸»é¢˜å‡ºç°é¢‘ç‡
        theme_posts = analysis_data['clean_text'].apply(
            lambda x: any(keyword in x for keyword in keywords)
        ).sum()
        
        # è®¡ç®—ä¸»é¢˜å…³é”®è¯å¯†åº¦
        keyword_counts = analysis_data['clean_text'].apply(
            lambda x: sum(x.count(keyword) for keyword in keywords)
        ).sum()
        
        theme_analysis[theme] = {
            'post_count': theme_posts,
            'post_ratio': theme_posts / len(analysis_data),
            'keyword_density': keyword_counts / text_lengths.sum() * 1000 if text_lengths.sum() > 0 else 0
        }
    
    content_metrics['themes'] = theme_analysis
    
    # 3. å†…å®¹ç‰¹å¾åˆ†æ
    # ç†æ€§é¢„æµ‹ç‰¹å¾
    rational_patterns = [
        'é¢„æµ‹', 'åˆ†æ', 'è§£è¯»', 'åŸå› ', 'ç»“æœ', 'å› ä¸º', 'æ‰€ä»¥', 
        'é€»è¾‘', 'ç†æ€§', 'å®¢è§‚', 'æ•°æ®', 'æ¨æµ‹', 'åˆ¤æ–­', 'è¯„ä¼°'
    ]
    
    # è¡ŒåŠ¨æ¸…å•ç‰¹å¾
    action_patterns = [
        'å»ºè®®', 'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦', 'æ–¹æ³•', 'æ­¥éª¤', 'æ¸…å•', 
        'åˆ—è¡¨', 'ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰', 'å¦‚ä½•åš', 'æ€ä¹ˆåš', 'è¡ŒåŠ¨'
    ]
    
    # å¿ƒç†æ…°è—‰ç‰¹å¾
    comfort_patterns = [
        'å®‰æ…°', 'é¼“åŠ±', 'æ”¯æŒ', 'ç†è§£', 'é™ªä¼´', 'å…±é¸£', 
        'æ²»æ„ˆ', 'æ¸©æš–', 'å¸Œæœ›', 'åŠ æ²¹', 'ç¥ç¦'
    ]
    
    def count_patterns(text, patterns):
        return sum(1 for pattern in patterns if pattern in text)
    
    analysis_data['rational_score'] = analysis_data['clean_text'].apply(
        lambda x: count_patterns(x, rational_patterns)
    )
    analysis_data['action_score'] = analysis_data['clean_text'].apply(
        lambda x: count_patterns(x, action_patterns)
    )
    analysis_data['comfort_score'] = analysis_data['clean_text'].apply(
        lambda x: count_patterns(x, comfort_patterns)
    )
    
    content_metrics['content_features'] = {
        'rational_mean': analysis_data['rational_score'].mean(),
        'action_mean': analysis_data['action_score'].mean(),
        'comfort_mean': analysis_data['comfort_score'].mean(),
        'has_rational': (analysis_data['rational_score'] > 0).mean(),
        'has_action': (analysis_data['action_score'] > 0).mean(),
        'has_comfort': (analysis_data['comfort_score'] > 0).mean()
    }
    
    # 4. å†…å®¹è´¨é‡è¯„ä¼°
    # è®¡ç®—å†…å®¹å¤šæ ·æ€§ï¼ˆä¸åŒä¸»é¢˜çš„è¦†ç›–ï¼‰
    theme_coverage = len([t for t in theme_analysis.values() if t['post_ratio'] > 0.1])
    content_metrics['quality'] = {
        'theme_diversity': theme_coverage / len(themes),
        'avg_length_score': min(text_lengths.mean() / 140, 1.0),  # å¾®åš140å­—ä¸Šé™
        'structure_score': (analysis_data['action_score'] > 0).mean(),
        'rationality_score': (analysis_data['rational_score'] > 0).mean()
    }
    
    # 5. é™¶ç™½ç™½ç‰¹è‰²åˆ†æ
    taobaibai_signatures = [
        'æ˜Ÿåº§è¿åŠ¿åˆ†æ', 'ç†æ€§é¢„æµ‹', 'è¡ŒåŠ¨æ¸…å•', 'æƒ…æ„ŸæŒ‡å¯¼', 
        'å¿ƒç†åˆ†æ', 'MBTIæ€§æ ¼', 'å¤åˆå»ºè®®', 'æ°´é€†æŒ‡å—'
    ]
    
    signature_counts = {}
    for signature in taobaibai_signatures:
        count = analysis_data['clean_text'].apply(
            lambda x: signature in x
        ).sum()
        signature_counts[signature] = count / len(analysis_data)
    
    content_metrics['signatures'] = signature_counts
    content_metrics['signature_match'] = sum(1 for v in signature_counts.values() if v > 0.05) / len(signature_counts)
    
    print(f"âœ… å¢å¼ºå†…å®¹åˆ†æå®Œæˆ")
    print(f"   å¹³å‡æ–‡æœ¬é•¿åº¦: {content_metrics['text_length']['mean']:.1f}å­—ç¬¦")
    print(f"   ä¸»é¢˜å¤šæ ·æ€§: {content_metrics['quality']['theme_diversity']:.1%}")
    print(f"   ç†æ€§å†…å®¹æ¯”ä¾‹: {content_metrics['content_features']['has_rational']:.1%}")
    print(f"   è¡ŒåŠ¨æŒ‡å—æ¯”ä¾‹: {content_metrics['content_features']['has_action']:.1%}")
    print(f"   é™¶ç™½ç™½ç‰¹å¾åŒ¹é…åº¦: {content_metrics['signature_match']:.1%}")
    
    return content_metrics

def enhanced_communication_analysis(data_dict, blogger_name="é™¶ç™½ç™½"):
    """å¢å¼ºçš„ä¼ æ’­ç»´åº¦åˆ†æ - é’ˆå¯¹ç¼ºå°‘äº’åŠ¨æ•°æ®"""
    print(f"\nğŸ“¢ æ‰§è¡Œå¢å¼ºä¼ æ’­åˆ†æ")
    
    comm_metrics = {}
    
    # ä½¿ç”¨åˆå¹¶çš„åˆ†ææ•°æ®
    analysis_data = data_dict.get('analysis_posts', pd.DataFrame())
    all_data = data_dict.get('all_data', pd.DataFrame())
    
    if len(analysis_data) == 0:
        print("âš ï¸ æ²¡æœ‰åˆ†ææ•°æ®")
        return comm_metrics
    
    # 1. ä¼ æ’­å¹¿åº¦æŒ‡æ ‡
    total_posts = len(all_data)
    related_posts = len(analysis_data)
    
    comm_metrics['reach'] = {
        'total_posts': total_posts,
        'related_posts': related_posts,
        'coverage_ratio': related_posts / total_posts if total_posts > 0 else 0,
        'user_count': analysis_data['user'].nunique() if 'user' in analysis_data.columns else 0
    }
    
    # 2. ä¼ æ’­æ·±åº¦æŒ‡æ ‡ï¼ˆåŸºäºå†…å®¹åˆ†æï¼‰
    # å¦‚æœç¼ºå°‘äº’åŠ¨æ•°æ®ï¼Œä½¿ç”¨å†…å®¹ç‰¹å¾ä½œä¸ºä»£ç†æŒ‡æ ‡
    if 'clean_text' in analysis_data.columns:
        # è®¡ç®—ä¼ æ’­æ½œåŠ›ï¼ˆå†…å®¹è´¨é‡æŒ‡æ ‡ï¼‰
        text_lengths = analysis_data['clean_text'].apply(lambda x: len(str(x)))
        avg_length = text_lengths.mean()
        
        # é«˜è´¨é‡å†…å®¹ç‰¹å¾
        quality_features = {
            'has_questions': analysis_data['clean_text'].str.contains('[?ï¼Ÿ]').mean(),
            'has_exclamations': analysis_data['clean_text'].str.contains('[!ï¼]').mean(),
            'has_hashtags': analysis_data['clean_text'].str.contains('#').mean(),
            'has_mentions': analysis_data['clean_text'].str.contains('@').mean(),
            'avg_sentence_length': text_lengths.mean() / (analysis_data['clean_text'].str.count('[ã€‚ï¼ï¼Ÿ.!?]') + 1).mean()
        }
        
        comm_metrics['content_potential'] = quality_features
        
        # ä¼ æ’­æ½œåŠ›ç»¼åˆè¯„åˆ†
        engagement_potential = (
            quality_features['has_questions'] * 0.3 +
            quality_features['has_exclamations'] * 0.2 +
            quality_features['has_hashtags'] * 0.3 +
            quality_features['has_mentions'] * 0.2
        )
        comm_metrics['engagement_potential'] = engagement_potential
    else:
        comm_metrics['engagement_potential'] = 0
    
    # 3. è¯é¢˜æ‰©æ•£åˆ†æ
    if 'text' in analysis_data.columns:
        # æå–è¯é¢˜æ ‡ç­¾
        hashtags = []
        for text in analysis_data['text'].dropna():
            matches = re.findall(r'#([^#]+)#', str(text))
            hashtags.extend(matches)
        
        if hashtags:
            hashtag_counts = Counter(hashtags)
            top_hashtags = dict(hashtag_counts.most_common(10))
            comm_metrics['hashtags'] = {
                'total_unique': len(hashtag_counts),
                'top_hashtags': top_hashtags,
                'avg_per_post': len(hashtags) / len(analysis_data)
            }
    
    # 4. æ—¶é—´åˆ†å¸ƒåˆ†æ
    if 'created_at' in analysis_data.columns:
        try:
            # è§£ææ—¶é—´
            analysis_data['parsed_time'] = analysis_data['created_at'].apply(parse_time)
            time_data = analysis_data.dropna(subset=['parsed_time'])
            
            if len(time_data) > 0:
                # æŒ‰å°æ—¶åˆ†å¸ƒ
                time_data['hour'] = time_data['parsed_time'].apply(lambda x: x.hour)
                hourly_dist = time_data['hour'].value_counts().sort_index()
                
                # æ´»è·ƒæ—¶æ®µåˆ†æ
                peak_hours = hourly_dist[hourly_dist > hourly_dist.quantile(0.75)].index.tolist()
                
                comm_metrics['time_distribution'] = {
                    'total_with_time': len(time_data),
                    'hourly_distribution': hourly_dist.to_dict(),
                    'peak_hours': peak_hours,
                    'temporal_consistency': len(peak_hours) / 24 if len(peak_hours) > 0 else 0
                }
        except Exception as e:
            print(f"  æ—¶é—´åˆ†æå‡ºé”™: {e}")
    
    # 5. ç”¨æˆ·å‚ä¸åº¦ï¼ˆåŸºäºç”¨æˆ·è¡Œä¸ºï¼‰
    if 'user' in analysis_data.columns:
        user_stats = analysis_data['user'].value_counts()
        comm_metrics['user_engagement'] = {
            'total_users': len(user_stats),
            'active_users': (user_stats >= 2).sum(),  # å‘å¸–2æ¡ä»¥ä¸Šä¸ºæ´»è·ƒç”¨æˆ·
            'top_users': dict(user_stats.head(5)),
            'gini_coefficient': calculate_gini(user_stats.values) if len(user_stats) > 1 else 0
        }
    
    print(f"âœ… å¢å¼ºä¼ æ’­åˆ†æå®Œæˆ")
    print(f"   è¯é¢˜è¦†ç›–ç‡: {comm_metrics['reach']['coverage_ratio']:.1%}")
    print(f"   å‚ä¸ç”¨æˆ·æ•°: {comm_metrics['reach']['user_count']}")
    print(f"   ä¼ æ’­æ½œåŠ›: {comm_metrics.get('engagement_potential', 0):.3f}")
    
    return comm_metrics

def enhanced_psychological_analysis(data_dict, blogger_name="é™¶ç™½ç™½"):
    """å¢å¼ºçš„å¿ƒç†ç»´åº¦åˆ†æ"""
    print(f"\nğŸ§  æ‰§è¡Œå¢å¼ºå¿ƒç†åˆ†æ")
    
    analysis_data = data_dict.get('analysis_posts', pd.DataFrame())
    
    if len(analysis_data) == 0:
        print("âš ï¸ æ²¡æœ‰åˆ†ææ•°æ®")
        return None
    
    # æ¸…ç†æ–‡æœ¬
    analysis_data['clean_text'] = analysis_data['text'].apply(clean_text)
    
    psych_metrics = {}
    
    # 1. æƒ…æ„Ÿåˆ†æ
    emotion_words = {
        'positive': ['å¼€å¿ƒ', 'é«˜å…´', 'å¿«ä¹', 'å¹¸ç¦', 'å¹¸è¿', 'é¡ºåˆ©', 'æˆåŠŸ', 'å¸Œæœ›', 'æœŸå¾…', 'åŠ æ²¹',
                    'ç¥ç¦', 'æ­å–œ', 'æ„Ÿè°¢', 'æ„ŸåŠ¨', 'æ¸©æš–', 'ç”œèœœ', 'ç¾å¥½', 'æ»¡æ„', 'ä¼˜ç§€', 'æ£’'],
        'negative': ['ç„¦è™‘', 'å‹åŠ›', 'ç´§å¼ ', 'æ‹…å¿ƒ', 'å®³æ€•', 'ç—›è‹¦', 'éš¾è¿‡', 'ä¼¤å¿ƒ', 'å¤±æœ›', 'ç»æœ›',
                    'ç”Ÿæ°”', 'æ„¤æ€’', 'çƒ¦æ¼', 'çº ç»“', 'è¿·èŒ«', 'å›°æƒ‘', 'å­¤ç‹¬', 'å¯‚å¯', 'ç–²æƒ«', 'ç´¯'],
        'neutral': ['åˆ†æ', 'é¢„æµ‹', 'å»ºè®®', 'æ–¹æ³•', 'æ­¥éª¤', 'å¯ä»¥', 'å¯èƒ½', 'ä¹Ÿè®¸', 'æˆ–è€…', 'ç†æ€§',
                   'å®¢è§‚', 'æ•°æ®', 'äº‹å®', 'ç»“æœ', 'åŸå› ', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'é‚£ä¹ˆ', 'å› æ­¤']
    }
    
    emotion_counts = {cat: [] for cat in emotion_words}
    for category, words in emotion_words.items():
        counts = analysis_data['clean_text'].apply(
            lambda x: sum(1 for word in words if word in x)
        )
        emotion_counts[category] = {
            'total': counts.sum(),
            'mean': counts.mean(),
            'posts_with': (counts > 0).sum(),
            'ratio': (counts > 0).sum() / len(analysis_data)
        }
    
    psych_metrics['emotion_analysis'] = emotion_counts
    
    # æƒ…æ„Ÿå¹³è¡¡åº¦
    positive_ratio = emotion_counts['positive']['ratio']
    negative_ratio = emotion_counts['negative']['ratio']
    emotion_balance = 1 - abs(positive_ratio - negative_ratio) / (positive_ratio + negative_ratio + 0.001)
    
    psych_metrics['emotion_balance'] = {
        'positive_ratio': positive_ratio,
        'negative_ratio': negative_ratio,
        'balance_score': emotion_balance,
        'dominant_emotion': 'positive' if positive_ratio > negative_ratio else 'negative' if negative_ratio > positive_ratio else 'balanced'
    }
    
    # 2. å¿ƒç†éœ€æ±‚åˆ†æ
    psychological_needs = {
        'æƒ…æ„Ÿéœ€æ±‚': ['çˆ±', 'å–œæ¬¢', 'æ„Ÿæƒ…', 'æƒ…æ„Ÿ', 'æ‹çˆ±', 'åˆ†æ‰‹', 'å¤åˆ', 'å©šå§»', 'å®¶åº­', 'äº²å¯†'],
        'è®¤çŸ¥éœ€æ±‚': ['çŸ¥é“', 'äº†è§£', 'æ˜ç™½', 'ç†è§£', 'å­¦ä¹ ', 'è®¤çŸ¥', 'çŸ¥è¯†', 'ä¿¡æ¯', 'æ€è€ƒ', 'åˆ†æ'],
        'å®‰å…¨éœ€æ±‚': ['å®‰å…¨', 'ç¨³å®š', 'ä¿éšœ', 'ä¿æŠ¤', 'å±é™©', 'é£é™©', 'å®³æ€•', 'æ‹…å¿ƒ', 'ç„¦è™‘', 'å‹åŠ›'],
        'å½’å±éœ€æ±‚': ['æœ‹å‹', 'ç¤¾äº¤', 'ç¾¤ä½“', 'ç¤¾åŒº', 'å½’å±', 'è®¤åŒ', 'æ¥å—', 'æ‹’ç»', 'å­¤ç‹¬', 'å¯‚å¯'],
        'æˆé•¿éœ€æ±‚': ['æˆé•¿', 'è¿›æ­¥', 'å‘å±•', 'æå‡', 'æ”¹å˜', 'æ”¹å–„', 'ä¼˜åŒ–', 'ç›®æ ‡', 'æ¢¦æƒ³', 'ç†æƒ³'],
        'å°Šé‡éœ€æ±‚': ['å°Šé‡', 'å°Šä¸¥', 'é¢å­', 'åèª‰', 'å£°èª‰', 'è¯„ä»·', 'æ‰¹è¯„', 'è¡¨æ‰¬', 'è®¤å¯', 'å¦å®š']
    }
    
    need_analysis = {}
    for need, keywords in psychological_needs.items():
        posts_with_need = analysis_data['clean_text'].apply(
            lambda x: any(keyword in x for keyword in keywords)
        ).sum()
        
        need_analysis[need] = {
            'posts': posts_with_need,
            'ratio': posts_with_need / len(analysis_data),
            'intensity': analysis_data['clean_text'].apply(
                lambda x: sum(x.count(keyword) for keyword in keywords)
            ).sum() / len(analysis_data)
        }
    
    psych_metrics['psychological_needs'] = need_analysis
    
    # ä¸»è¦å¿ƒç†éœ€æ±‚
    need_ratios = {need: data['ratio'] for need, data in need_analysis.items()}
    primary_needs = sorted(need_ratios.items(), key=lambda x: x[1], reverse=True)[:3]
    psych_metrics['primary_needs'] = dict(primary_needs)
    
    # 3. å¿ƒç†æ”¯æŒæ•ˆæœè¯„ä¼°
    support_indicators = {
        'advice_given': ['å»ºè®®', 'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦', 'æ–¹æ³•', 'æ­¥éª¤', 'å¦‚ä½•', 'æ€æ ·'],
        'comfort_provided': ['å®‰æ…°', 'é¼“åŠ±', 'æ”¯æŒ', 'ç†è§£', 'é™ªä¼´', 'å…±é¸£', 'æ¸©æš–', 'å…³å¿ƒ'],
        'solution_offered': ['è§£å†³', 'å¤„ç†', 'åº”å¯¹', 'é¢å¯¹', 'å…‹æœ', 'æ”¹å–„', 'è°ƒæ•´', 'æ”¹å˜'],
        'hope_inspired': ['å¸Œæœ›', 'æœªæ¥', 'æ˜å¤©', 'åŠ æ²¹', 'åšæŒ', 'åŠªåŠ›', 'æˆåŠŸ', 'ç¾å¥½']
    }
    
    support_analysis = {}
    for indicator, keywords in support_indicators.items():
        posts_with_support = analysis_data['clean_text'].apply(
            lambda x: any(keyword in x for keyword in keywords)
        ).sum()
        
        support_analysis[indicator] = {
            'posts': posts_with_support,
            'ratio': posts_with_support / len(analysis_data),
            'effectiveness': posts_with_support / max(1, emotion_counts['negative']['posts_with'])
        }
    
    psych_metrics['support_analysis'] = support_analysis
    
    # ç»¼åˆå¿ƒç†æ”¯æŒæŒ‡æ•°
    support_scores = [data['ratio'] for data in support_analysis.values()]
    psych_metrics['support_index'] = np.mean(support_scores) if support_scores else 0
    
    # 4. è¡Œä¸ºæ¿€å‘åˆ†æ
    behavior_indicators = {
        'action_intent': ['è¦', 'æƒ³', 'æ‰“ç®—', 'è®¡åˆ’', 'å‡†å¤‡', 'å†³å®š', 'å°è¯•', 'å¼€å§‹'],
        'goal_setting': ['ç›®æ ‡', 'è®¡åˆ’', 'flag', 'æ‰“å¡', 'è®°å½•', 'åšæŒ', 'åŠªåŠ›', 'å¥‹æ–—'],
        'progress_sharing': ['åˆ†äº«', 'å‘Šè¯‰', 'æ±‡æŠ¥', 'æ›´æ–°', 'è¿›æ­¥', 'æˆæœ', 'æˆç»©', 'æ”¶è·'],
        'help_seeking': ['æ±‚åŠ©', 'å¸®å¿™', 'å¸®åŠ©', 'è¯·é—®', 'æ±‚é—®', 'å’¨è¯¢', 'è¯¢é—®', 'è¯·æ•™']
    }
    
    behavior_analysis = {}
    for behavior, keywords in behavior_indicators.items():
        posts_with_behavior = analysis_data['clean_text'].apply(
            lambda x: any(keyword in x for keyword in keywords)
        ).sum()
        
        behavior_analysis[behavior] = {
            'posts': posts_with_behavior,
            'ratio': posts_with_behavior / len(analysis_data),
            'engagement': posts_with_behavior / len(analysis_data) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        }
    
    psych_metrics['behavior_analysis'] = behavior_analysis
    
    # è¡Œä¸ºæ¿€å‘æŒ‡æ•°
    behavior_ratios = [data['ratio'] for data in behavior_analysis.values()]
    psych_metrics['behavior_index'] = np.mean(behavior_ratios) if behavior_ratios else 0
    
    # 5. ç„¦è™‘ç®¡ç†åˆ†æ
    anxiety_terms = ['ç„¦è™‘', 'å‹åŠ›', 'ç´§å¼ ', 'æ‹…å¿ƒ', 'å®³æ€•', 'ææ…Œ', 'ä¸å®‰', 'å¿§è™‘']
    solution_terms = ['æ–¹æ³•', 'è§£å†³', 'ç¼“è§£', 'å‡å°‘', 'åº”å¯¹', 'å¤„ç†', 'è°ƒæ•´', 'æ”¹å–„']
    
    anxiety_posts = analysis_data['clean_text'].apply(
        lambda x: any(term in x for term in anxiety_terms)
    ).sum()
    
    solution_posts = analysis_data['clean_text'].apply(
        lambda x: any(term in x for term in solution_terms)
    ).sum()
    
    anxiety_solution_posts = analysis_data['clean_text'].apply(
        lambda x: any(at in x for at in anxiety_terms) and any(st in x for st in solution_terms)
    ).sum()
    
    psych_metrics['anxiety_management'] = {
        'anxiety_mentioned': anxiety_posts / len(analysis_data),
        'solutions_provided': solution_posts / len(analysis_data),
        'targeted_solutions': anxiety_solution_posts / max(1, anxiety_posts),
        'anxiety_coverage': anxiety_solution_posts / len(analysis_data)
    }
    
    print(f"âœ… å¢å¼ºå¿ƒç†åˆ†æå®Œæˆ")
    print(f"   æƒ…æ„Ÿå¹³è¡¡åº¦: {psych_metrics['emotion_balance']['balance_score']:.3f}")
    print(f"   ä¸»è¦å¿ƒç†éœ€æ±‚: {list(psych_metrics['primary_needs'].keys())[:2]}")
    print(f"   å¿ƒç†æ”¯æŒæŒ‡æ•°: {psych_metrics['support_index']:.3f}")
    print(f"   è¡Œä¸ºæ¿€å‘æŒ‡æ•°: {psych_metrics['behavior_index']:.3f}")
    
    return psych_metrics

def calculate_enhanced_scores(content_metrics, comm_metrics, psych_metrics):
    """è®¡ç®—å¢å¼ºç‰ˆä¸‰ç»´è¯„åˆ†"""
    print(f"\nğŸ“Š è®¡ç®—å¢å¼ºç‰ˆä¸‰ç»´è¯„åˆ†...")
    
    scores = {}
    
    # 1. å†…å®¹ç»´åº¦è¯„åˆ† (0-100åˆ†)
    if content_metrics:
        content_score = 0
        
        # å†…å®¹è´¨é‡ (40åˆ†)
        quality_indicators = content_metrics.get('quality', {})
        quality_score = (
            quality_indicators.get('theme_diversity', 0) * 0.4 +
            quality_indicators.get('avg_length_score', 0) * 0.3 +
            quality_indicators.get('structure_score', 0) * 0.2 +
            quality_indicators.get('rationality_score', 0) * 0.1
        ) * 40
        
        # ä¸»é¢˜èšç„¦ (30åˆ†)
        theme_analysis = content_metrics.get('themes', {})
        # é™¶ç™½ç™½æ ¸å¿ƒä¸»é¢˜ï¼šæ˜Ÿåº§è¿åŠ¿ã€æƒ…æ„Ÿå’¨è¯¢ã€è¡ŒåŠ¨æŒ‡å¯¼
        core_themes = ['æ˜Ÿåº§è¿åŠ¿', 'æƒ…æ„Ÿå’¨è¯¢', 'è¡ŒåŠ¨æŒ‡å¯¼']
        core_theme_score = sum(
            theme_analysis.get(theme, {}).get('post_ratio', 0) 
            for theme in core_themes
        ) / len(core_themes) * 30
        
        # ç‰¹å¾åŒ¹é… (30åˆ†)
        signature_score = (
            content_metrics.get('signature_match', 0) * 0.5 +
            content_metrics.get('content_features', {}).get('has_rational', 0) * 0.3 +
            content_metrics.get('content_features', {}).get('has_action', 0) * 0.2
        ) * 30
        
        content_score = quality_score + core_theme_score + signature_score
        scores['å†…å®¹ç»´åº¦'] = min(max(content_score, 0), 100)
    else:
        scores['å†…å®¹ç»´åº¦'] = 0
    
    # 2. ä¼ æ’­ç»´åº¦è¯„åˆ† (0-100åˆ†)
    if comm_metrics:
        comm_score = 0
        
        # ä¼ æ’­å¹¿åº¦ (40åˆ†)
        reach = comm_metrics.get('reach', {})
        coverage_score = reach.get('coverage_ratio', 0) * 40
        
        # ç”¨æˆ·å‚ä¸ (30åˆ†)
        user_engagement = comm_metrics.get('user_engagement', {})
        user_score = (
            min(user_engagement.get('active_users', 0) / max(1, user_engagement.get('total_users', 1)), 1) * 0.6 +
            (1 - min(user_engagement.get('gini_coefficient', 0), 0.8)) * 0.4  # åŸºå°¼ç³»æ•°è¶Šä½ï¼Œåˆ†å¸ƒè¶Šå‡åŒ€
        ) * 30
        
        # ä¼ æ’­æ½œåŠ› (30åˆ†)
        potential_score = (
            comm_metrics.get('engagement_potential', 0) * 0.5 +
            comm_metrics.get('content_potential', {}).get('has_hashtags', 0) * 0.3 +
            comm_metrics.get('content_potential', {}).get('has_mentions', 0) * 0.2
        ) * 30
        
        comm_score = coverage_score + user_score + potential_score
        scores['ä¼ æ’­ç»´åº¦'] = min(max(comm_score, 0), 100)
    else:
        scores['ä¼ æ’­ç»´åº¦'] = 0
    
    # 3. å¿ƒç†ç»´åº¦è¯„åˆ† (0-100åˆ†)
    if psych_metrics:
        psych_score = 0
        
        # æƒ…æ„Ÿæ”¯æŒ (35åˆ†)
        emotion = psych_metrics.get('emotion_balance', {})
        emotion_score = (
            emotion.get('balance_score', 0) * 0.6 +
            max(emotion.get('positive_ratio', 0) - emotion.get('negative_ratio', 0), 0) * 0.4
        ) * 35
        
        # å¿ƒç†éœ€æ±‚æ»¡è¶³ (35åˆ†)
        primary_needs = psych_metrics.get('primary_needs', {})
        need_score = sum(primary_needs.values()) / len(primary_needs) if primary_needs else 0
        need_score *= 35
        
        # æ”¯æŒæ•ˆæœ (30åˆ†)
        support_score = (
            psych_metrics.get('support_index', 0) * 0.4 +
            psych_metrics.get('behavior_index', 0) * 0.3 +
            psych_metrics.get('anxiety_management', {}).get('targeted_solutions', 0) * 0.3
        ) * 30
        
        psych_score = emotion_score + need_score + support_score
        scores['å¿ƒç†ç»´åº¦'] = min(max(psych_score, 0), 100)
    else:
        scores['å¿ƒç†ç»´åº¦'] = 0
    
    # 4. ç»¼åˆè¯„åˆ†
    if scores:
        weight_content = 0.35
        weight_comm = 0.30  # é™ä½ä¼ æ’­æƒé‡ï¼Œå› ä¸ºç¼ºå°‘äº’åŠ¨æ•°æ®
        weight_psych = 0.35  # æé«˜å¿ƒç†æƒé‡
        
        total_score = (
            scores.get('å†…å®¹ç»´åº¦', 0) * weight_content +
            scores.get('ä¼ æ’­ç»´åº¦', 0) * weight_comm +
            scores.get('å¿ƒç†ç»´åº¦', 0) * weight_psych
        )
        scores['ç»¼åˆè¯„åˆ†'] = min(max(total_score, 0), 100)
        
        # è¯„ä¼°ç­‰çº§
        if total_score >= 85:
            scores['è¯„ä¼°ç­‰çº§'] = 'ä¼˜ç§€'
            scores['æ²»ç†å»ºè®®'] = 'ä¸‰ç»´è¡¨ç°å‡è¡¡ä¼˜ç§€ï¼Œå¯ç»§ç»­æ·±åŒ–ä¸“ä¸šå½±å“åŠ›'
        elif total_score >= 70:
            scores['è¯„ä¼°ç­‰çº§'] = 'è‰¯å¥½'
            scores['æ²»ç†å»ºè®®'] = 'æ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œå¯åœ¨è–„å¼±ç¯èŠ‚è¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–'
        elif total_score >= 60:
            scores['è¯„ä¼°ç­‰çº§'] = 'åˆæ ¼'
            scores['æ²»ç†å»ºè®®'] = 'åŸºæœ¬æ»¡è¶³éœ€æ±‚ï¼Œéœ€ç³»ç»Ÿæ€§æå‡å†…å®¹è´¨é‡å’Œä¼ æ’­æ•ˆæœ'
        elif total_score >= 40:
            scores['è¯„ä¼°ç­‰çº§'] = 'å¾…æ”¹è¿›'
            scores['æ²»ç†å»ºè®®'] = 'éœ€è¦é‡ç‚¹æ”¹è¿›å†…å®¹è´¨é‡å’Œç”¨æˆ·å‚ä¸åº¦'
        else:
            scores['è¯„ä¼°ç­‰çº§'] = 'ä¸è¶³'
            scores['æ²»ç†å»ºè®®'] = 'éœ€è¦å…¨é¢ä¼˜åŒ–ï¼Œé‡æ–°è¯„ä¼°å†…å®¹ç­–ç•¥å’Œç”¨æˆ·å®šä½'
    
    print(f"âœ… å¢å¼ºè¯„åˆ†è®¡ç®—å®Œæˆ")
    print(f"   å†…å®¹ç»´åº¦: {scores.get('å†…å®¹ç»´åº¦', 0):.1f}åˆ†")
    print(f"   ä¼ æ’­ç»´åº¦: {scores.get('ä¼ æ’­ç»´åº¦', 0):.1f}åˆ†")
    print(f"   å¿ƒç†ç»´åº¦: {scores.get('å¿ƒç†ç»´åº¦', 0):.1f}åˆ†")
    print(f"   ç»¼åˆè¯„åˆ†: {scores.get('ç»¼åˆè¯„åˆ†', 0):.1f}åˆ† ({scores.get('è¯„ä¼°ç­‰çº§', 'æœªçŸ¥')})")
    
    return scores

# ======================================
# 3. å¯è§†åŒ–ä¸æŠ¥å‘Š
# ======================================

def generate_enhanced_report(content_metrics, comm_metrics, psych_metrics, scores, data_summary):
    """ç”Ÿæˆå¢å¼ºç‰ˆè¯„ä¼°æŠ¥å‘Š"""
    report = []
    report.append("=" * 70)
    report.append("åšä¸»ä¸‰ç»´è¯„ä¼°æŠ¥å‘Šï¼ˆå¢å¼ºç‰ˆï¼‰")
    report.append("=" * 70)
    report.append("")
    
    # æ•°æ®æ¦‚å†µ
    report.append("ğŸ“Š æ•°æ®æ¦‚å†µ")
    report.append(f"   æ€»æ•°æ®é‡: {data_summary.get('total_posts', 0)}æ¡å¾®åš")
    report.append(f"   åˆ†ææ•°æ®: {data_summary.get('analysis_posts', 0)}æ¡ç›¸å…³å¾®åš")
    report.append(f"   æ•°æ®è¦†ç›–ç‡: {data_summary.get('analysis_posts', 0)/max(1, data_summary.get('total_posts', 1))*100:.1f}%")
    report.append(f"   äº’åŠ¨æ•°æ®å¯ç”¨æ€§: {'æ˜¯' if data_summary.get('interaction_data_available') else 'å¦ï¼ˆä½¿ç”¨å¢å¼ºåˆ†æï¼‰'}")
    report.append("")
    
    # è¯„ä¼°ç»“æœæ‘˜è¦
    report.append("ğŸ“ˆ è¯„ä¼°ç»“æœæ‘˜è¦")
    report.append(f"   ç»¼åˆè¯„åˆ†: {scores.get('ç»¼åˆè¯„åˆ†', 0):.1f}åˆ† ({scores.get('è¯„ä¼°ç­‰çº§', 'æœªçŸ¥')})")
    report.append(f"   å†…å®¹ç»´åº¦: {scores.get('å†…å®¹ç»´åº¦', 0):.1f}åˆ†")
    report.append(f"   ä¼ æ’­ç»´åº¦: {scores.get('ä¼ æ’­ç»´åº¦', 0):.1f}åˆ†")
    report.append(f"   å¿ƒç†ç»´åº¦: {scores.get('å¿ƒç†ç»´åº¦', 0):.1f}åˆ†")
    report.append("")
    
    # è¯¦ç»†åˆ†æ
    if content_metrics:
        report.append("ğŸ“ å†…å®¹ç»´åº¦è¯¦ç»†åˆ†æ")
        report.append("-" * 40)
        
        # å†…å®¹å½¢å¼
        text_len = content_metrics.get('text_length', {})
        report.append(f"   1. å†…å®¹å½¢å¼:")
        report.append(f"      â€¢ å¹³å‡é•¿åº¦: {text_len.get('mean', 0):.1f}å­—ç¬¦")
        report.append(f"      â€¢ ä¸­ä½æ•°: {text_len.get('median', 0):.1f}å­—ç¬¦")
        
        # é•¿åº¦åˆ†å¸ƒ
        length_dist = content_metrics.get('length_distribution', {})
        for length_type, ratio in sorted(length_dist.items()):
            if ratio > 0.05:
                report.append(f"      â€¢ {length_type}: {ratio:.1%}")
        
        # ä¸»é¢˜åˆ†æ
        themes = content_metrics.get('themes', {})
        report.append(f"   2. æ ¸å¿ƒä¸»é¢˜:")
        for theme, data in sorted(themes.items(), key=lambda x: x[1]['post_ratio'], reverse=True):
            if data['post_ratio'] > 0.1:
                report.append(f"      â€¢ {theme}: {data['post_ratio']:.1%} (å¯†åº¦: {data['keyword_density']:.2f})")
        
        # å†…å®¹ç‰¹å¾
        features = content_metrics.get('content_features', {})
        report.append(f"   3. å†…å®¹ç‰¹å¾:")
        report.append(f"      â€¢ ç†æ€§åˆ†æ: {features.get('has_rational', 0):.1%}")
        report.append(f"      â€¢ è¡ŒåŠ¨æŒ‡å—: {features.get('has_action', 0):.1%}")
        report.append(f"      â€¢ å¿ƒç†æ…°è—‰: {features.get('has_comfort', 0):.1%}")
        
        report.append("")
    
    if comm_metrics:
        report.append("ğŸ“¢ ä¼ æ’­ç»´åº¦è¯¦ç»†åˆ†æ")
        report.append("-" * 40)
        
        # ä¼ æ’­å¹¿åº¦
        reach = comm_metrics.get('reach', {})
        report.append(f"   1. ä¼ æ’­å¹¿åº¦:")
        report.append(f"      â€¢ è¯é¢˜è¦†ç›–ç‡: {reach.get('coverage_ratio', 0):.1%}")
        report.append(f"      â€¢ å‚ä¸ç”¨æˆ·æ•°: {reach.get('user_count', 0)}äºº")
        
        # ç”¨æˆ·å‚ä¸
        user_eng = comm_metrics.get('user_engagement', {})
        if user_eng:
            report.append(f"   2. ç”¨æˆ·å‚ä¸:")
            report.append(f"      â€¢ æ´»è·ƒç”¨æˆ·: {user_eng.get('active_users', 0)}äºº")
            report.append(f"      â€¢ ç”¨æˆ·é›†ä¸­åº¦: {user_eng.get('gini_coefficient', 0):.3f}")
        
        # ä¼ æ’­æ½œåŠ›
        report.append(f"   3. ä¼ æ’­æ½œåŠ›:")
        report.append(f"      â€¢ ç»¼åˆæ½œåŠ›: {comm_metrics.get('engagement_potential', 0):.3f}")
        
        # æ—¶é—´åˆ†å¸ƒ
        time_dist = comm_metrics.get('time_distribution', {})
        if time_dist:
            report.append(f"   4. æ—¶é—´åˆ†å¸ƒ:")
            report.append(f"      â€¢ æ´»è·ƒæ—¶æ®µ: {', '.join(map(str, time_dist.get('peak_hours', [])))}ç‚¹")
        
        report.append("")
    
    if psych_metrics:
        report.append("ğŸ§  å¿ƒç†ç»´åº¦è¯¦ç»†åˆ†æ")
        report.append("-" * 40)
        
        # æƒ…æ„Ÿåˆ†æ
        emotion = psych_metrics.get('emotion_balance', {})
        report.append(f"   1. æƒ…æ„Ÿåˆ†æ:")
        report.append(f"      â€¢ ç§¯ææƒ…ç»ª: {emotion.get('positive_ratio', 0):.1%}")
        report.append(f"      â€¢ æ¶ˆææƒ…ç»ª: {emotion.get('negative_ratio', 0):.1%}")
        report.append(f"      â€¢ æƒ…æ„Ÿå¹³è¡¡åº¦: {emotion.get('balance_score', 0):.3f}")
        
        # å¿ƒç†éœ€æ±‚
        primary_needs = psych_metrics.get('primary_needs', {})
        report.append(f"   2. ä¸»è¦å¿ƒç†éœ€æ±‚:")
        for need, ratio in sorted(primary_needs.items(), key=lambda x: x[1], reverse=True)[:3]:
            report.append(f"      â€¢ {need}: {ratio:.1%}")
        
        # å¿ƒç†æ”¯æŒ
        report.append(f"   3. å¿ƒç†æ”¯æŒæ•ˆæœ:")
        report.append(f"      â€¢ æ”¯æŒæŒ‡æ•°: {psych_metrics.get('support_index', 0):.3f}")
        report.append(f"      â€¢ è¡Œä¸ºæ¿€å‘: {psych_metrics.get('behavior_index', 0):.3f}")
        
        # ç„¦è™‘ç®¡ç†
        anxiety = psych_metrics.get('anxiety_management', {})
        report.append(f"   4. ç„¦è™‘ç®¡ç†:")
        report.append(f"      â€¢ é’ˆå¯¹æ€§è§£å†³: {anxiety.get('targeted_solutions', 0):.1%}")
        
        report.append("")
    
    # æ²»ç†å»ºè®®
    report.append("ğŸ’¡ æ²»ç†å»ºè®®ä¸ä¼˜åŒ–ç­–ç•¥")
    report.append("-" * 40)
    report.append(f"   {scores.get('æ²»ç†å»ºè®®', '')}")
    report.append("")
    
    # å…·ä½“å»ºè®®
    content_score = scores.get('å†…å®¹ç»´åº¦', 0)
    comm_score = scores.get('ä¼ æ’­ç»´åº¦', 0)
    psych_score = scores.get('å¿ƒç†ç»´åº¦', 0)
    
    if content_score < 70:
        report.append("   1. å†…å®¹ä¼˜åŒ–å»ºè®®:")
        report.append("     â€¢ å¢åŠ æ·±åº¦åˆ†æå†…å®¹ï¼Œæå‡ä¸“ä¸šæ€§")
        report.append("     â€¢ åŠ å¼ºç»“æ„åŒ–è¡¨è¾¾ï¼Œæä¾›æ¸…æ™°è¡ŒåŠ¨æŒ‡å—")
        report.append("     â€¢ ä¸°å¯Œä¸»é¢˜å†…å®¹ï¼Œè¦†ç›–æ›´å¤šç”¨æˆ·éœ€æ±‚")
    
    if comm_score < 70:
        report.append("   2. ä¼ æ’­ä¼˜åŒ–å»ºè®®:")
        report.append("     â€¢ è®¾è®¡äº’åŠ¨è¯é¢˜ï¼Œé¼“åŠ±ç”¨æˆ·å‚ä¸")
        report.append("     â€¢ ä¼˜åŒ–å‘å¸ƒæ—¶é—´ï¼Œæé«˜å†…å®¹æ›å…‰")
        report.append("     â€¢ å»ºç«‹ç”¨æˆ·ç¤¾ç¾¤ï¼Œå¢å¼ºç”¨æˆ·é»æ€§")
    
    if psych_score < 70:
        report.append("   3. å¿ƒç†ä¼˜åŒ–å»ºè®®:")
        report.append("     â€¢ å¢å¼ºæƒ…æ„Ÿæ”¯æŒå†…å®¹ï¼Œæä¾›å¿ƒç†æ…°è—‰")
        report.append("     â€¢ æä¾›å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©ç”¨æˆ·åº”å¯¹é—®é¢˜")
        report.append("     â€¢ å»ºç«‹ä¿¡ä»»å…³ç³»ï¼Œæå‡ç”¨æˆ·å¿ƒç†å®‰å…¨æ„Ÿ")
    
    if all(score >= 75 for score in [content_score, comm_score, psych_score]):
        report.append("   1. æ•´ä½“è¡¨ç°ä¼˜ç§€ï¼Œå»ºè®®:")
        report.append("     â€¢ ç»§ç»­ä¿æŒé«˜è´¨é‡å†…å®¹è¾“å‡º")
        report.append("     â€¢ æ¢ç´¢æ–°çš„å†…å®¹å½¢å¼å’Œä¼ æ’­æ¸ é“")
        report.append("     â€¢ å»ºç«‹å“ç‰Œä½“ç³»ï¼Œæå‡é•¿æœŸå½±å“åŠ›")
    
    report.append("")
    report.append("ğŸ“‹ è¯„ä¼°è¯´æ˜")
    report.append("-" * 40)
    report.append("   â€¢ æœ¬è¯„ä¼°åŸºäºç›¸å…³è¯é¢˜å¾®åšçš„å†…å®¹åˆ†æ")
    report.append("   â€¢ åœ¨ç¼ºå°‘äº’åŠ¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¢å¼ºåˆ†ææ–¹æ³•")
    report.append("   â€¢ è¯„ä¼°ç»“æœå¯ç”¨äºå†…å®¹ç­–ç•¥ä¼˜åŒ–å’Œè¯é¢˜æ²»ç†")
    report.append("   â€¢ å»ºè®®è¡¥å……å®Œæ•´æ•°æ®ä»¥è·å¾—æ›´å‡†ç¡®çš„è¯„ä¼°")
    report.append("")
    report.append("=" * 70)
    
    report_text = "\n".join(report)
    print(report_text)
    
    # ä¿å­˜æŠ¥å‘Š
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"blogger_enhanced_assessment_{timestamp}.txt"
    
    try:
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report_text)
        print(f"\nğŸ’¾ å·²ä¿å­˜å¢å¼ºç‰ˆè¯„ä¼°æŠ¥å‘Š: {report_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    return report_text

def create_content_theme_chart(content_metrics, save_path="content_theme_distribution.png"):
    """åˆ›å»ºå†…å®¹ä¸»é¢˜å æ¯”å›¾è¡¨"""
    if not content_metrics or 'themes' not in content_metrics:
        print("âš ï¸ ç¼ºå°‘å†…å®¹ä¸»é¢˜æ•°æ®")
        return
    
    themes = content_metrics['themes']
    theme_names = list(themes.keys())
    theme_ratios = [themes[theme]['post_ratio'] * 100 for theme in theme_names]
    
    # åˆ›å»ºå›¾è¡¨
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # 1. é¥¼å›¾
    colors = plt.cm.Set3(np.linspace(0, 1, len(theme_names)))
    wedges, texts, autotexts = ax1.pie(theme_ratios, labels=theme_names, autopct='%1.1f%%',
                                       colors=colors, startangle=90)
    ax1.set_title('å†…å®¹ä¸»é¢˜å æ¯”åˆ†å¸ƒï¼ˆé¥¼å›¾ï¼‰', fontsize=14, fontweight='bold', pad=20)
    
    # è°ƒæ•´æ ‡ç­¾å­—ä½“
    for autotext in autotexts:
        autotext.set_color('black')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(9)
    
    # 2. æŸ±çŠ¶å›¾ï¼ˆæŒ‰å æ¯”æ’åºï¼‰
    sorted_indices = sorted(range(len(theme_ratios)), key=lambda i: theme_ratios[i], reverse=True)
    sorted_themes = [theme_names[i] for i in sorted_indices]
    sorted_ratios = [theme_ratios[i] for i in sorted_indices]
    
    bars = ax2.barh(sorted_themes, sorted_ratios, color=colors[sorted_indices], alpha=0.8)
    ax2.set_xlabel('å æ¯” (%)', fontsize=12)
    ax2.set_title('å†…å®¹ä¸»é¢˜å æ¯”åˆ†å¸ƒï¼ˆæŸ±çŠ¶å›¾ï¼‰', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='x')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, ratio) in enumerate(zip(bars, sorted_ratios)):
        if ratio > 0:
            ax2.text(ratio + 0.5, i, f'{ratio:.1f}%', 
                    va='center', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å·²ä¿å­˜å†…å®¹ä¸»é¢˜å æ¯”å›¾è¡¨: {save_path}")
    plt.show()

def create_communication_network(data_dict, save_path="communication_network.png"):
    """åˆ›å»ºä¼ æ’­ç½‘ç»œå›¾"""
    try:
        import networkx as nx
    except ImportError:
        print("âš ï¸ éœ€è¦å®‰è£…networkxåº“: pip install networkx")
        return
    
    analysis_data = data_dict.get('analysis_posts', pd.DataFrame())
    if len(analysis_data) == 0:
        print("âš ï¸ æ²¡æœ‰æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆä¼ æ’­ç½‘ç»œ")
        return
    
    # åˆ›å»ºç½‘ç»œå›¾
    G = nx.Graph()
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦æ·»åŠ çš„èŠ‚ç‚¹
    user_nodes_dict = {}
    keyword_nodes_dict = {}
    edges_list = []
    
    # æ–¹æ³•1: åŸºäºç”¨æˆ·çš„ä¼ æ’­ç½‘ç»œï¼ˆå¦‚æœç”¨æˆ·æ•°æ®å¯ç”¨ï¼‰
    if 'user' in analysis_data.columns:
        # ç»Ÿè®¡ç”¨æˆ·å‘å¸–æ•°
        user_counts = analysis_data['user'].value_counts()
        
        # å¦‚æœæ•°æ®é‡å¤ªå¤§ï¼Œåªé€‰æ‹©æ´»è·ƒç”¨æˆ·
        if len(user_counts) > 100:
            top_users = user_counts.head(50).index.tolist()
            selected_users = [str(u) for u in top_users if pd.notna(u) and str(u).strip()]
        else:
            selected_users = [str(u) for u in user_counts.index if pd.notna(u) and str(u).strip()]
        
        # æ”¶é›†ç”¨æˆ·èŠ‚ç‚¹
        for user in selected_users:
            if user in user_counts.index:
                user_nodes_dict[user] = {'weight': int(user_counts[user]), 'node_type': 'user'}
    
    # æ–¹æ³•2: åŸºäºå…³é”®è¯/ä¸»é¢˜çš„å…±ç°ç½‘ç»œ
    if 'text' in analysis_data.columns and 'keyword' in analysis_data.columns:
        # æå–å…³é”®è¯
        keywords = analysis_data['keyword'].dropna().unique().tolist()
        
        # æ”¶é›†å…³é”®è¯èŠ‚ç‚¹ï¼ˆåªé€‰æ‹©å‰20ä¸ªçƒ­é—¨å…³é”®è¯ï¼‰
        keyword_counts = analysis_data['keyword'].value_counts()
        top_keywords = keyword_counts.head(20).index.tolist()
        
        for keyword in top_keywords:
            if pd.notna(keyword) and str(keyword).strip():
                keyword_node = f"å…³é”®è¯:{keyword}"
                keyword_nodes_dict[keyword_node] = {
                    'weight': int(keyword_counts[keyword]),
                    'node_type': 'keyword'
                }
        
        # å¦‚æœæœ‰ç”¨æˆ·æ•°æ®ï¼Œè¿æ¥ç”¨æˆ·å’Œå…³é”®è¯
        if 'user' in analysis_data.columns and user_nodes_dict:
            for idx, row in analysis_data.iterrows():
                user = str(row.get('user', ''))
                keyword = str(row.get('keyword', ''))
                if (pd.notna(row.get('user')) and pd.notna(row.get('keyword')) and
                    user in user_nodes_dict and f"å…³é”®è¯:{keyword}" in keyword_nodes_dict):
                    edges_list.append((user, f"å…³é”®è¯:{keyword}", {'weight': 1}))
    
    # ä¸€æ¬¡æ€§æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
    G.add_nodes_from([(node, attrs) for node, attrs in user_nodes_dict.items()])
    G.add_nodes_from([(node, attrs) for node, attrs in keyword_nodes_dict.items()])
    
    # æ·»åŠ è¾¹
    if edges_list:
        G.add_edges_from(edges_list)
    
    if len(G.nodes()) == 0:
        print("âš ï¸ æ— æ³•æ„å»ºç½‘ç»œå›¾ï¼šæ•°æ®ä¸è¶³")
        return
    
    # ç»˜åˆ¶ç½‘ç»œå›¾
    plt.figure(figsize=(14, 10))
    
    # ä½¿ç”¨springå¸ƒå±€
    pos = nx.spring_layout(G, k=1, iterations=50)
    
    # åŒºåˆ†èŠ‚ç‚¹ç±»å‹
    user_nodes = [n for n in G.nodes() if G.nodes[n].get('node_type') != 'keyword' and 'å…³é”®è¯:' not in n]
    keyword_nodes = [n for n in G.nodes() if G.nodes[n].get('node_type') == 'keyword' or 'å…³é”®è¯:' in n]
    
    # ç»˜åˆ¶è¾¹
    nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5, edge_color='gray')
    
    # ç»˜åˆ¶ç”¨æˆ·èŠ‚ç‚¹
    if user_nodes:
        node_sizes = [G.nodes[n].get('weight', 1) * 100 for n in user_nodes]
        nx.draw_networkx_nodes(G, pos, nodelist=user_nodes, node_color='#FF6B6B',
                              node_size=node_sizes, alpha=0.7, label='ç”¨æˆ·')
    
    # ç»˜åˆ¶å…³é”®è¯èŠ‚ç‚¹
    if keyword_nodes:
        keyword_sizes = [G.nodes[n].get('weight', 1) * 200 for n in keyword_nodes]
        nx.draw_networkx_nodes(G, pos, nodelist=keyword_nodes, node_color='#4ECDC4',
                              node_size=keyword_sizes, alpha=0.7, label='å…³é”®è¯')
    
    # åªæ ‡æ³¨é‡è¦èŠ‚ç‚¹ï¼ˆé¿å…è¿‡äºæ‹¥æŒ¤ï¼‰
    important_nodes = []
    if user_nodes:
        user_weights = [(n, G.nodes[n].get('weight', 0)) for n in user_nodes]
        important_nodes.extend([n for n, w in sorted(user_weights, key=lambda x: x[1], reverse=True)[:10]])
    if keyword_nodes:
        important_nodes.extend(keyword_nodes[:10])
    
    labels = {n: n.replace('å…³é”®è¯:', '') if 'å…³é”®è¯:' in n else n[:10] 
             for n in important_nodes}
    nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold')
    
    plt.title('ä¼ æ’­ç½‘ç»œå›¾\nï¼ˆèŠ‚ç‚¹å¤§å°è¡¨ç¤ºå‚ä¸åº¦ï¼Œè¿çº¿è¡¨ç¤ºå…³è”ï¼‰', 
             fontsize=14, fontweight='bold', pad=20)
    plt.axis('off')
    plt.legend(loc='upper right')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å·²ä¿å­˜ä¼ æ’­ç½‘ç»œå›¾: {save_path}")
    plt.show()

def create_emotion_radar(psych_metrics, save_path="emotion_radar.png"):
    """åˆ›å»ºç²‰ä¸æƒ…ç»ªé›·è¾¾å›¾"""
    if not psych_metrics:
        print("âš ï¸ ç¼ºå°‘å¿ƒç†åˆ†ææ•°æ®")
        return
    
    # æå–æƒ…ç»ªæ•°æ®
    emotion_analysis = psych_metrics.get('emotion_analysis', {})
    
    # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
    categories = ['ç§¯ææƒ…ç»ª', 'æ¶ˆææƒ…ç»ª', 'ä¸­æ€§æƒ…ç»ª']
    values = [
        emotion_analysis.get('positive', {}).get('ratio', 0) * 100,
        emotion_analysis.get('negative', {}).get('ratio', 0) * 100,
        emotion_analysis.get('neutral', {}).get('ratio', 0) * 100
    ]
    
    # å¦‚æœæœ‰å¿ƒç†éœ€æ±‚æ•°æ®ï¼Œä¹Ÿå¯ä»¥åŠ å…¥
    psychological_needs = psych_metrics.get('psychological_needs', {})
    if psychological_needs:
        # é€‰æ‹©å‰3ä¸ªä¸»è¦éœ€æ±‚
        need_ratios = {need: data.get('ratio', 0) * 100 
                      for need, data in psychological_needs.items()}
        top_needs = sorted(need_ratios.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # æ‰©å±•é›·è¾¾å›¾ç»´åº¦
        categories.extend([need for need, _ in top_needs])
        values.extend([ratio for _, ratio in top_needs])
    
    # åˆ›å»ºé›·è¾¾å›¾
    fig = plt.figure(figsize=(10, 10))
    ax = plt.subplot(111, projection='polar')
    
    # è®¡ç®—è§’åº¦
    N = len(categories)
    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
    values_plot = values + values[:1]  # é—­åˆ
    angles_plot = angles + angles[:1]
    
    # ç»˜åˆ¶é›·è¾¾å›¾
    ax.plot(angles_plot, values_plot, 'o-', linewidth=2, color='#FF6B6B', label='æƒ…ç»ª/éœ€æ±‚å æ¯”')
    ax.fill(angles_plot, values_plot, alpha=0.25, color='#FF6B6B')
    
    # è®¾ç½®æ ‡ç­¾
    ax.set_xticks(angles)
    ax.set_xticklabels(categories, fontsize=11, fontweight='bold')
    
    # è®¾ç½®èŒƒå›´
    ax.set_ylim(0, max(values) * 1.2 if max(values) > 0 else 100)
    ax.set_yticks([0, 25, 50, 75, 100])
    ax.set_yticklabels(['0%', '25%', '50%', '75%', '100%'], fontsize=9)
    ax.grid(True, linestyle='--', alpha=0.5)
    
    plt.title('ç²‰ä¸æƒ…ç»ªé›·è¾¾å›¾\nï¼ˆåæ˜ ç”¨æˆ·æƒ…ç»ªåˆ†å¸ƒå’Œå¿ƒç†éœ€æ±‚ï¼‰', 
             fontsize=14, fontweight='bold', pad=30)
    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨
    for angle, value, category in zip(angles, values, categories):
        if value > 0:
            ax.text(angle, value + 5, f'{value:.1f}%', 
                   ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å·²ä¿å­˜æƒ…ç»ªé›·è¾¾å›¾: {save_path}")
    plt.show()

def create_enhanced_visualization(scores, content_metrics=None, comm_metrics=None, 
                                 psych_metrics=None, data_dict=None,
                                 save_path="blogger_enhanced_assessment.png"):
    """åˆ›å»ºå¢å¼ºç‰ˆå¯è§†åŒ–å›¾è¡¨ï¼ˆåŒ…å«ä¸‰ç»´è¯„ä¼°ã€ä¸»é¢˜å æ¯”ã€ä¼ æ’­ç½‘ç»œã€æƒ…ç»ªé›·è¾¾ï¼‰"""
    
    # åˆ›å»ºç»¼åˆå¯è§†åŒ–
    fig = plt.figure(figsize=(20, 12))
    
    dimensions = ['å†…å®¹ç»´åº¦', 'ä¼ æ’­ç»´åº¦', 'å¿ƒç†ç»´åº¦']
    values = [scores.get(dim, 0) for dim in dimensions]
    total_score = scores.get('ç»¼åˆè¯„åˆ†', 0)
    
    # 1. ä¸‰ç»´è¯„ä¼°é›·è¾¾å›¾
    ax1 = plt.subplot(2, 3, 1, projection='polar')
    angles = np.linspace(0, 2 * np.pi, len(dimensions), endpoint=False).tolist()
    values_plot = values + values[:1]
    angles_plot = angles + angles[:1]
    
    ax1.plot(angles_plot, values_plot, 'o-', linewidth=2, color='#4ECDC4')
    ax1.fill(angles_plot, values_plot, alpha=0.25, color='#4ECDC4')
    ax1.set_xticks(angles)
    ax1.set_xticklabels(dimensions, fontsize=10)
    ax1.set_ylim(0, 100)
    ax1.set_yticks([25, 50, 75, 100])
    ax1.set_title('ä¸‰ç»´è¯„ä¼°é›·è¾¾å›¾', fontsize=12, fontweight='bold')
    
    # 2. å†…å®¹ä¸»é¢˜å æ¯”
    ax2 = plt.subplot(2, 3, 2)
    if content_metrics and 'themes' in content_metrics:
        themes = content_metrics['themes']
        theme_names = list(themes.keys())
        theme_ratios = [themes[theme]['post_ratio'] * 100 for theme in theme_names]
        
        # åªæ˜¾ç¤ºå æ¯”>5%çš„ä¸»é¢˜
        significant_themes = [(name, ratio) for name, ratio in zip(theme_names, theme_ratios) if ratio > 5]
        if significant_themes:
            names, ratios = zip(*sorted(significant_themes, key=lambda x: x[1], reverse=True))
            colors = plt.cm.Set3(np.linspace(0, 1, len(names)))
            bars = ax2.barh(names, ratios, color=colors, alpha=0.8)
            ax2.set_xlabel('å æ¯” (%)', fontsize=10)
            ax2.set_title('å†…å®¹ä¸»é¢˜å æ¯”', fontsize=12, fontweight='bold')
            ax2.grid(True, alpha=0.3, axis='x')
            for bar, ratio in zip(bars, ratios):
                ax2.text(ratio + 0.5, bar.get_y() + bar.get_height()/2, 
                        f'{ratio:.1f}%', va='center', fontsize=9, fontweight='bold')
        else:
            ax2.text(0.5, 0.5, 'æ— æ˜¾è‘—ä¸»é¢˜æ•°æ®', ha='center', va='center', 
                    transform=ax2.transAxes, fontsize=12)
            ax2.set_title('å†…å®¹ä¸»é¢˜å æ¯”', fontsize=12, fontweight='bold')
            ax2.axis('off')
    else:
        ax2.text(0.5, 0.5, 'ä¸»é¢˜æ•°æ®æœªæä¾›', ha='center', va='center', 
                transform=ax2.transAxes, fontsize=12)
        ax2.set_title('å†…å®¹ä¸»é¢˜å æ¯”', fontsize=12, fontweight='bold')
        ax2.axis('off')
    
    # 3. ç²‰ä¸æƒ…ç»ªé›·è¾¾å›¾
    ax3 = plt.subplot(2, 3, 3, projection='polar')
    if psych_metrics and 'emotion_analysis' in psych_metrics:
        emotion_analysis = psych_metrics['emotion_analysis']
        categories = ['ç§¯æ', 'æ¶ˆæ', 'ä¸­æ€§']
        values_emotion = [
            emotion_analysis.get('positive', {}).get('ratio', 0) * 100,
            emotion_analysis.get('negative', {}).get('ratio', 0) * 100,
            emotion_analysis.get('neutral', {}).get('ratio', 0) * 100
        ]
        
        angles_emo = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        values_emo_plot = values_emotion + values_emotion[:1]
        angles_emo_plot = angles_emo + angles_emo[:1]
        
        ax3.plot(angles_emo_plot, values_emo_plot, 'o-', linewidth=2, color='#FF6B6B')
        ax3.fill(angles_emo_plot, values_emo_plot, alpha=0.25, color='#FF6B6B')
        ax3.set_xticks(angles_emo)
        ax3.set_xticklabels(categories, fontsize=10)
        max_val = max(values_emotion) * 1.2 if max(values_emotion) > 0 else 100
        ax3.set_ylim(0, max_val)
        ax3.set_yticks([0, 25, 50, 75, 100])
        ax3.set_title('ç²‰ä¸æƒ…ç»ªé›·è¾¾å›¾', fontsize=12, fontweight='bold')
        ax3.grid(True, linestyle='--', alpha=0.5)
    else:
        ax3.text(0.5, 0.5, 'æƒ…ç»ªæ•°æ®æœªæä¾›', ha='center', va='center', 
                transform=ax3.transAxes, fontsize=12)
        ax3.set_title('ç²‰ä¸æƒ…ç»ªé›·è¾¾å›¾', fontsize=12, fontweight='bold')
        ax3.axis('off')
    
    # 4. ç»¼åˆè¯„åˆ†ä»ªè¡¨ç›˜
    ax4 = plt.subplot(2, 3, 4)
    ax4.set_xlim(-1.5, 1.5)
    ax4.set_ylim(-1.5, 1.5)
    ax4.set_aspect('equal')
    ax4.axis('off')
    
    # ç»˜åˆ¶èƒŒæ™¯åœ†ç¯
    circle = plt.Circle((0, 0), 1.0, color='lightgray', alpha=0.3)
    ax4.add_patch(circle)
    
    # ç»˜åˆ¶è¯„åˆ†å¼§
    score_angle = total_score / 100 * 180
    if total_score < 60:
        color = 'red'
    elif total_score < 75:
        color = 'orange'
    elif total_score < 85:
        color = 'yellowgreen'
    else:
        color = 'green'
    
    ax4.plot([0, 0.8 * np.sin(np.deg2rad(score_angle))], 
             [0, 0.8 * np.cos(np.deg2rad(score_angle))], 
             color=color, linewidth=4)
    
    ax4.text(0, 0, f'{total_score:.1f}', ha='center', va='center', 
             fontsize=24, fontweight='bold', color=color)
    ax4.text(0, -0.3, scores.get('è¯„ä¼°ç­‰çº§', 'æœªçŸ¥'), ha='center', va='center',
             fontsize=14, fontweight='bold', color=color)
    ax4.text(0, -0.5, 'ç»¼åˆè¯„åˆ†', ha='center', va='center',
             fontsize=10, color='gray')
    
    # 5. ä¼ æ’­ç½‘ç»œå›¾ï¼ˆæ˜¾ç¤ºçƒ­é—¨è¯é¢˜æ ‡ç­¾ï¼‰
    ax5 = plt.subplot(2, 3, 5)
    if comm_metrics and 'hashtags' in comm_metrics:
        hashtags_data = comm_metrics['hashtags']
        top_hashtags = hashtags_data.get('top_hashtags', {})
        if top_hashtags:
            tags = list(top_hashtags.keys())[:8]
            counts = list(top_hashtags.values())[:8]
            colors_network = plt.cm.viridis(np.linspace(0, 1, len(tags)))
            bars = ax5.barh(tags, counts, color=colors_network, alpha=0.8)
            ax5.set_xlabel('ä½¿ç”¨æ¬¡æ•°', fontsize=10)
            ax5.set_title('ä¼ æ’­ç½‘ç»œï¼ˆçƒ­é—¨è¯é¢˜ï¼‰', fontsize=12, fontweight='bold')
            ax5.grid(True, alpha=0.3, axis='x')
            for bar, count in zip(bars, counts):
                ax5.text(count + 0.1, bar.get_y() + bar.get_height()/2, 
                        f'{int(count)}', va='center', fontsize=9)
        else:
            ax5.text(0.5, 0.5, 'æ— è¯é¢˜æ ‡ç­¾æ•°æ®', ha='center', va='center', 
                    transform=ax5.transAxes, fontsize=12)
            ax5.set_title('ä¼ æ’­ç½‘ç»œ', fontsize=12, fontweight='bold')
            ax5.axis('off')
    else:
        ax5.text(0.5, 0.5, 'ä¼ æ’­æ•°æ®æœªæä¾›', ha='center', va='center', 
                transform=ax5.transAxes, fontsize=12)
        ax5.set_title('ä¼ æ’­ç½‘ç»œ', fontsize=12, fontweight='bold')
        ax5.axis('off')
    
    # 6. å»ºè®®åŒºåŸŸ
    ax6 = plt.subplot(2, 3, 6)
    ax6.axis('off')
    
    suggestion = scores.get('æ²»ç†å»ºè®®', 'æš‚æ— å…·ä½“å»ºè®®')
    strengths = []
    weaknesses = []
    
    for dim, score in zip(dimensions, values):
        if score >= 75:
            strengths.append(f"{dim} ({score:.1f}åˆ†)")
        elif score < 60:
            weaknesses.append(f"{dim} ({score:.1f}åˆ†)")
    
    suggestion_text = f"ğŸ’¡ æ²»ç†å»ºè®®:\n\n{suggestion}\n\n"
    
    if strengths:
        suggestion_text += f"âœ… ä¼˜åŠ¿ç»´åº¦:\n" + "\n".join([f"  â€¢ {s}" for s in strengths]) + "\n\n"
    
    if weaknesses:
        suggestion_text += f"âš ï¸ å¾…æ”¹è¿›ç»´åº¦:\n" + "\n".join([f"  â€¢ {w}" for w in weaknesses])
    else:
        suggestion_text += f"âœ… å„ç»´åº¦è¡¨ç°å‡è¡¡ï¼Œæ— æ˜æ˜¾çŸ­æ¿"
    
    suggestion_text += f"\n\nğŸ“Š ç»¼åˆè¯„åˆ†: {total_score:.1f}åˆ† ({scores.get('è¯„ä¼°ç­‰çº§', 'æœªçŸ¥')})"
    suggestion_text += f"\nğŸ” è¯„ä¼°åŸºäºå¢å¼ºåˆ†ææ–¹æ³•"
    
    ax6.text(0.05, 0.95, suggestion_text, fontsize=10, va='top', ha='left',
             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7),
             transform=ax6.transAxes)
    
    plt.suptitle('åšä¸»ä¸‰ç»´è¯„ä¼°æŠ¥å‘Š\nï¼ˆå†…å®¹â€”ä¼ æ’­â€”å¿ƒç†ï¼‰', fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å·²ä¿å­˜ç»¼åˆå¯è§†åŒ–å›¾è¡¨: {save_path}")
    plt.show()
    
    # ç”Ÿæˆå•ç‹¬çš„è¯¦ç»†å›¾è¡¨
    print("\nğŸ“Š ç”Ÿæˆè¯¦ç»†å¯è§†åŒ–å›¾è¡¨...")
    if content_metrics:
        create_content_theme_chart(content_metrics)
    
    if data_dict:
        create_communication_network(data_dict)
    
    if psych_metrics:
        create_emotion_radar(psych_metrics)

# ======================================
# ä¸»ç¨‹åº
# ======================================

def main():
    print("=" * 70)
    print("åšä¸»ä¸‰ç»´è¯„ä¼°ï¼ˆå¢å¼ºç‰ˆï¼‰")
    print("é’ˆå¯¹æ•°æ®æœ‰é™åœºæ™¯çš„ä¼˜åŒ–åˆ†æ")
    print("=" * 70)
    print()
    
    # é…ç½®å‚æ•°
    BLOGGER_NAME = "é™¶ç™½ç™½"
    
    # ä¼˜å…ˆä½¿ç”¨åšä¸»æœ¬äººçš„å¾®åšæ–‡ä»¶ï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼‰
    import glob
    import os
    # å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    blogger_weibo_files = glob.glob(f"{BLOGGER_NAME}_weibo_*.json")
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆåŒ…å«åšä¸»åç§°çš„æ‰€æœ‰weiboæ–‡ä»¶ï¼‰
    if not blogger_weibo_files:
        all_weibo_files = glob.glob("*_weibo_*.json")
        blogger_weibo_files = [f for f in all_weibo_files if BLOGGER_NAME in f]
    # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…ï¼ˆåŒ…å«"é™¶"å’Œ"ç™½"çš„æ–‡ä»¶ï¼Œé€‚ç”¨äº"é™¶ç™½ç™½Sensei"ï¼‰
    if not blogger_weibo_files and "é™¶" in BLOGGER_NAME and "ç™½" in BLOGGER_NAME:
        all_weibo_files = glob.glob("*_weibo_*.json")
        blogger_weibo_files = [f for f in all_weibo_files if "é™¶" in f and "ç™½" in f and "_weibo_" in f]
    
    if blogger_weibo_files:
        # ä½¿ç”¨æœ€æ–°çš„åšä¸»å¾®åšæ–‡ä»¶
        latest_blogger_file = max(blogger_weibo_files, key=os.path.getmtime)
        DATA_FILE = latest_blogger_file
        print(f"ğŸ“ æ‰¾åˆ°åšä¸»å¾®åšæ–‡ä»¶: {DATA_FILE}")
        print(f"   å¦‚æœæ•°æ®ä¸è¶³ï¼Œå°†åˆå¹¶ä½¿ç”¨é€šç”¨æ•°æ®æ–‡ä»¶")
    else:
        # ä½¿ç”¨é€šç”¨æ•°æ®æ–‡ä»¶
        DATA_FILE = "weibo_data_20251218_012526.json"
        print(f"âš ï¸ æœªæ‰¾åˆ°åšä¸»ä¸“é—¨å¾®åšæ–‡ä»¶ï¼Œä½¿ç”¨é€šç”¨æ•°æ®æ–‡ä»¶: {DATA_FILE}")
        print(f"   ğŸ’¡ æç¤ºï¼šè¿è¡Œ collect_taobaibai_weibo.py å¯æ”¶é›†åšä¸»æœ¬äººçš„å¾®åš")
    
    # 1. åŠ è½½æ•°æ®
    print(f"ğŸ“¥ åŠ è½½åšä¸» '{BLOGGER_NAME}' ç›¸å…³æ•°æ®...")
    data_dict = load_blogger_data(DATA_FILE, BLOGGER_NAME)
    
    if data_dict is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
        return
    
    data_summary = data_dict.get('data_summary', {})
    print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
    print(f"   æ€»æ•°æ®é‡: {data_summary.get('total_posts', 0)}æ¡")
    print(f"   åˆ†ææ•°æ®: {data_summary.get('analysis_posts', 0)}æ¡")
    print(f"   äº’åŠ¨æ•°æ®å¯ç”¨: {data_summary.get('interaction_data_available', False)}")
    
    analysis_posts_count = data_summary.get('analysis_posts', 0)
    total_posts_count = data_summary.get('total_posts', 0)
    
    if analysis_posts_count < 50:
        print(f"\nâš ï¸ è­¦å‘Š: åˆ†ææ•°æ®è¾ƒå°‘ ({analysis_posts_count}æ¡)")
        print("   è¯„ä¼°ç»“æœä»…ä¾›å‚è€ƒï¼Œå»ºè®®æ”¶é›†æ›´å¤šæ•°æ®")
        print(f"\nğŸ’¡ æ•°æ®æ”¶é›†å»ºè®®:")
        print(f"   1. å½“å‰æ•°æ®é›†: æ€»æ•°æ®{total_posts_count}æ¡ï¼Œç›¸å…³æ•°æ®{analysis_posts_count}æ¡")
        print(f"   2. æ‰©å¤§å…³é”®è¯èŒƒå›´: åœ¨weibo_data.pyä¸­å¢åŠ æ›´å¤šå…³é”®è¯ï¼ˆæ˜Ÿåº§ã€æƒ…æ„Ÿã€å¿ƒç†ç­‰ï¼‰")
        print(f"   3. å¢åŠ ç¿»é¡µæ•°é‡: åœ¨weibo_data.pyä¸­å¢åŠ MAX_PAGESå‚æ•°ï¼ˆå½“å‰å¯èƒ½ä¸º20ï¼‰")
        print(f"   4. ç›®æ ‡æ•°æ®é‡: å»ºè®®è‡³å°‘1000-2000æ¡æ€»æ•°æ®ï¼Œ500æ¡ä»¥ä¸Šç›¸å…³æ•°æ®")
    elif analysis_posts_count < 200:
        print(f"\nâš ï¸ æç¤º: åˆ†ææ•°æ®é‡ä¸­ç­‰ ({analysis_posts_count}æ¡)")
        print("   å»ºè®®æ”¶é›†æ›´å¤šæ•°æ®ä»¥æé«˜åˆ†æå‡†ç¡®æ€§å’Œå¯é æ€§")
        print(f"   ç›®æ ‡: è‡³å°‘500æ¡ä»¥ä¸Šç›¸å…³æ•°æ®å¯è·å¾—æ›´å¯é çš„ç»“æœ")
    
    # 2. å¢å¼ºä¸‰ç»´åˆ†æ
    print(f"\n{'='*40}")
    print(f"å¼€å§‹å¢å¼ºä¸‰ç»´åˆ†æ")
    print(f"{'='*40}")
    
    # å†…å®¹ç»´åº¦åˆ†æ
    content_metrics = enhanced_content_analysis(data_dict['analysis_posts'], BLOGGER_NAME)
    
    # ä¼ æ’­ç»´åº¦åˆ†æ
    comm_metrics = enhanced_communication_analysis(data_dict, BLOGGER_NAME)
    
    # å¿ƒç†ç»´åº¦åˆ†æ
    psych_metrics = enhanced_psychological_analysis(data_dict, BLOGGER_NAME)
    
    # 3. è®¡ç®—å¢å¼ºè¯„åˆ†
    print(f"\n{'='*40}")
    print(f"è®¡ç®—å¢å¼ºç‰ˆè¯„åˆ†")
    print(f"{'='*40}")
    
    scores = calculate_enhanced_scores(content_metrics, comm_metrics, psych_metrics)
    
    # 4. å¯è§†åŒ–
    print(f"\n{'='*40}")
    print(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    print(f"{'='*40}")
    
    create_enhanced_visualization(scores, content_metrics, comm_metrics, 
                                 psych_metrics, data_dict)
    
    # 5. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print(f"\n{'='*40}")
    print(f"ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
    print(f"{'='*40}")
    
    report = generate_enhanced_report(content_metrics, comm_metrics, psych_metrics, scores, data_summary)
    
    print(f"\n{'='*70}")
    print(f"âœ… åšä¸»ä¸‰ç»´å¢å¼ºè¯„ä¼°å®Œæˆ!")
    print(f"{'='*70}")
    
    # 6. è¾“å‡ºå…³é”®å‘ç°
    print(f"\nğŸ” å…³é”®å‘ç°æ€»ç»“:")
    print(f"   1. ç»¼åˆè¯„åˆ†: {scores.get('ç»¼åˆè¯„åˆ†', 0):.1f}åˆ† ({scores.get('è¯„ä¼°ç­‰çº§', 'æœªçŸ¥')})")
    print(f"   2. ä¸‰ç»´è¡¨ç°:")
    print(f"      â€¢ å†…å®¹ç»´åº¦: {scores.get('å†…å®¹ç»´åº¦', 0):.1f}åˆ†")
    print(f"      â€¢ ä¼ æ’­ç»´åº¦: {scores.get('ä¼ æ’­ç»´åº¦', 0):.1f}åˆ†")
    print(f"      â€¢ å¿ƒç†ç»´åº¦: {scores.get('å¿ƒç†ç»´åº¦', 0):.1f}åˆ†")
    print(f"   3. ä¸»è¦ä¼˜åŠ¿: ", end="")
    
    strengths = []
    if scores.get('å†…å®¹ç»´åº¦', 0) >= 70:
        strengths.append("å†…å®¹ä¸“ä¸šæ€§")
    if scores.get('ä¼ æ’­ç»´åº¦', 0) >= 70:
        strengths.append("ä¼ æ’­è¦†ç›–")
    if scores.get('å¿ƒç†ç»´åº¦', 0) >= 70:
        strengths.append("å¿ƒç†æ”¯æŒ")
    
    if strengths:
        print(", ".join(strengths))
    else:
        print("æ— æ˜æ˜¾çªå‡ºä¼˜åŠ¿")
    
    print(f"   4. æ”¹è¿›é‡ç‚¹: ", end="")
    weaknesses = []
    if scores.get('å†…å®¹ç»´åº¦', 0) < 60:
        weaknesses.append("å†…å®¹è´¨é‡")
    if scores.get('ä¼ æ’­ç»´åº¦', 0) < 60:
        weaknesses.append("ç”¨æˆ·å‚ä¸")
    if scores.get('å¿ƒç†ç»´åº¦', 0) < 60:
        weaknesses.append("å¿ƒç†æ”¯æŒ")
    
    if weaknesses:
        print(", ".join(weaknesses))
    else:
        print("æš‚æ— æ˜ç¡®çŸ­æ¿")
    
    # 7. ä¿å­˜ç»“æœ
    results = {
        'åšä¸»åç§°': BLOGGER_NAME,
        'è¯„ä¼°æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'è¯„ä¼°ç‰ˆæœ¬': 'å¢å¼ºç‰ˆï¼ˆé’ˆå¯¹æœ‰é™æ•°æ®ï¼‰',
        'æ•°æ®æ¦‚å†µ': data_summary,
        'å†…å®¹ç»´åº¦å¾—åˆ†': scores.get('å†…å®¹ç»´åº¦', 0),
        'ä¼ æ’­ç»´åº¦å¾—åˆ†': scores.get('ä¼ æ’­ç»´åº¦', 0),
        'å¿ƒç†ç»´åº¦å¾—åˆ†': scores.get('å¿ƒç†ç»´åº¦', 0),
        'ç»¼åˆè¯„åˆ†': scores.get('ç»¼åˆè¯„åˆ†', 0),
        'è¯„ä¼°ç­‰çº§': scores.get('è¯„ä¼°ç­‰çº§', 'æœªçŸ¥'),
        'æ²»ç†å»ºè®®': scores.get('æ²»ç†å»ºè®®', ''),
        'å†…å®¹ç»´åº¦è¯¦æƒ…': content_metrics,
        'ä¼ æ’­ç»´åº¦è¯¦æƒ…': comm_metrics,
        'å¿ƒç†ç»´åº¦è¯¦æƒ…': psych_metrics
    }
    
    import json
    results_file = f"blogger_enhanced_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    try:
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {results_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}")
    
    # 8. æ•°æ®æ”¶é›†å»ºè®®
    print(f"\nğŸ“‹ æ•°æ®æ”¶é›†å»ºè®®:")
    blogger_posts_count = len(data_dict.get('blogger_posts', pd.DataFrame()))
    
    if blogger_posts_count < 20:
        print(f"   âš ï¸ å½“å‰åšä¸»æœ¬äººå¾®åšä»…{blogger_posts_count}æ¡ï¼Œå»ºè®®æ”¶é›†æ›´å¤šåšä¸»å¾®åš:")
        print(f"      è¿è¡Œå‘½ä»¤: python collect_taobaibai_weibo.py")
        print(f"      è¿™å°†ä¸“é—¨æ”¶é›†åšä¸» '{BLOGGER_NAME}' çš„å¾®åšå†…å®¹")
    else:
        print(f"   âœ… åšä¸»æœ¬äººå¾®åšæ•°æ®å……è¶³ï¼ˆ{blogger_posts_count}æ¡ï¼‰")
    
    print(f"   1. å¦‚éœ€æ›´å¤šæ•°æ®ï¼Œè¿è¡Œ collect_taobaibai_weibo.py æ”¶é›†åšä¸»æœ¬äººå¾®åš")
    print(f"   2. ç¡®ä¿æŠ“å–å®Œæ•´çš„äº’åŠ¨æ•°æ®ï¼ˆè½¬å‘ã€è¯„è®ºã€ç‚¹èµï¼‰")
    print(f"   3. å¯ä»¥è°ƒæ•´ collect_taobaibai_weibo.py ä¸­çš„ max_pages å‚æ•°ä»¥è·å–æ›´å¤šå¾®åš")
    print(f"   4. å»ºè®®æ”¶é›†æ—¶é—´è·¨åº¦æ›´é•¿çš„å¾®åšï¼Œäº†è§£å†…å®¹è¶‹åŠ¿å˜åŒ–")
    
    return results

if __name__ == "__main__":
    main()