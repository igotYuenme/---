# ======================================
# èšç±»ç»“æœè¯Šæ–­åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…å†…å­˜é—®é¢˜ï¼‰
# ======================================

import sys
import os

# åˆ†æ­¥å¯¼å…¥ï¼Œé¿å…å†…å­˜é—®é¢˜
try:
    import json
    print("âœ… json å¯¼å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ json å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

try:
    import pandas as pd
    print("âœ… pandas å¯¼å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ pandas å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å°è¯•: pip install --upgrade pandas numpy")
    sys.exit(1)

try:
    import numpy as np
    print("âœ… numpy å¯¼å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ numpy å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å°è¯•: pip install --upgrade numpy")
    sys.exit(1)

# å»¶è¿Ÿå¯¼å…¥ä¸»æ¨¡å—
print("\næ­£åœ¨å¯¼å…¥åˆ†ææ¨¡å—...")
try:
    from user_portrait_analysis import (
        load_data, standardize_columns, extract_time_features,
        extract_content_features, calculate_interaction_features,
        calculate_user_engagement_features, extract_sentiment_features,
        perform_clustering, identify_user_types
    )
    print("âœ… åˆ†ææ¨¡å—å¯¼å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ åˆ†ææ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# åŠ è½½æ•°æ®
print("\n" + "=" * 60)
print("èšç±»ç»“æœè¯Šæ–­åˆ†æ")
print("=" * 60)

df = load_data()
if df is None or len(df) == 0:
    print("âŒ æ•°æ®ä¸ºç©º")
    sys.exit(1)

print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•")

# åˆ†æ­¥å¤„ç†ï¼Œé¿å…å†…å­˜é—®é¢˜
print("\nğŸ”§ æ­¥éª¤1: æ•°æ®æ ‡å‡†åŒ–...")
df = standardize_columns(df)

print("ğŸ”§ æ­¥éª¤2: æå–æ—¶é—´ç‰¹å¾...")
df = extract_time_features(df)

print("ğŸ”§ æ­¥éª¤3: æå–å†…å®¹ç‰¹å¾...")
df = extract_content_features(df)

print("ğŸ”§ æ­¥éª¤4: è®¡ç®—äº’åŠ¨ç‰¹å¾...")
df = calculate_interaction_features(df)

print("ğŸ”§ æ­¥éª¤5: è®¡ç®—ç”¨æˆ·å‚ä¸åº¦ç‰¹å¾...")
df = calculate_user_engagement_features(df)

print("ğŸ”§ æ­¥éª¤6: æå–æƒ…æ„Ÿç‰¹å¾...")
df = extract_sentiment_features(df)

# æ£€æŸ¥æ—¶é—´ç‰¹å¾
print("\nğŸ“Š æ—¶é—´ç‰¹å¾æ£€æŸ¥:")
try:
    time_success = df['created_datetime'].notna().sum()
    print(f"  æ—¶é—´è§£ææˆåŠŸæ•°: {time_success} / {len(df)} ({time_success/len(df)*100:.1f}%)")
    print(f"  è€ƒè¯•å‘¨æ¯”ä¾‹: {df['is_exam_season'].mean():.3f}")
    print(f"  æ‹›è˜å­£æ¯”ä¾‹: {df['is_recruitment_season'].mean():.3f}")
    print(f"  ä¼‘é—²æ—¶æ®µæ¯”ä¾‹: {df['is_leisure_time'].mean():.3f}")
except Exception as e:
    print(f"  âš ï¸ æ—¶é—´ç‰¹å¾æ£€æŸ¥å¤±è´¥: {e}")

# æ£€æŸ¥å†…å®¹ç‰¹å¾
print("\nğŸ“ å†…å®¹ç‰¹å¾æ£€æŸ¥:")
try:
    print(f"  å­¦ä¸š/èŒä¸šç±»æ¯”ä¾‹: {df['is_academic_career'].mean():.3f}")
    print(f"  æƒ…æ„Ÿç±»æ¯”ä¾‹: {df['is_emotional'].mean():.3f}")
    print(f"  å¨±ä¹ç±»æ¯”ä¾‹: {df['is_entertainment'].mean():.3f}")
    print(f"  å¹³å‡å­¦ä¸šå¾—åˆ†: {df['academic_score'].mean():.2f}")
    print(f"  å¹³å‡èŒä¸šå¾—åˆ†: {df['career_score'].mean():.2f}")
    print(f"  å¹³å‡æ…°è—‰å¾—åˆ†: {df['comfort_score'].mean():.2f}")
    print(f"  å¹³å‡æ…°è—‰éœ€æ±‚: {df['comfort_need'].mean():.3f}")
except Exception as e:
    print(f"  âš ï¸ å†…å®¹ç‰¹å¾æ£€æŸ¥å¤±è´¥: {e}")

# æ‰§è¡Œèšç±»
print("\nğŸ” æ‰§è¡Œèšç±»åˆ†æ...")
try:
    df, kmeans, scaler, cluster_centers = perform_clustering(df, n_clusters=3)
except Exception as e:
    print(f"  âŒ èšç±»å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# åˆ†ææ¯ä¸ªç°‡çš„ç‰¹å¾
print("\nğŸ” å„ç°‡ç‰¹å¾åˆ†æ:")
for cluster_id in sorted(df['cluster'].unique()):
    try:
        cluster_data = df[df['cluster'] == cluster_id]
        print(f"\nç°‡ {cluster_id} (æ ·æœ¬æ•°: {len(cluster_data)}):")
        print(f"  å­¦ä¸š/èŒä¸šç±»æ¯”ä¾‹: {cluster_data['is_academic_career'].mean():.3f}")
        print(f"  æƒ…æ„Ÿç±»æ¯”ä¾‹: {cluster_data['is_emotional'].mean():.3f}")
        print(f"  å¨±ä¹ç±»æ¯”ä¾‹: {cluster_data['is_entertainment'].mean():.3f}")
        print(f"  å¹³å‡äº’åŠ¨åˆ†æ•°: {cluster_data['interaction_score'].mean():.2f}")
        print(f"  å¹³å‡å‚ä¸åº¦: {cluster_data['engagement_level'].mean():.3f}")
        print(f"  å¹³å‡æ´»è·ƒåº¦: {cluster_data['activity_level'].mean():.3f}")
        print(f"  å¹³å‡æ…°è—‰éœ€æ±‚: {cluster_data['comfort_need'].mean():.3f}")
        print(f"  å¹³å‡æ·±åº¦å¾—åˆ†: {cluster_data['deep_score'].mean():.2f}")
        print(f"  è€ƒè¯•å‘¨æ¯”ä¾‹: {cluster_data['is_exam_season'].mean():.3f}")
        print(f"  æ‹›è˜å­£æ¯”ä¾‹: {cluster_data['is_recruitment_season'].mean():.3f}")
        print(f"  ä¼‘é—²æ—¶æ®µæ¯”ä¾‹: {cluster_data['is_leisure_time'].mean():.3f}")
    except Exception as e:
        print(f"  âš ï¸ ç°‡ {cluster_id} åˆ†æå¤±è´¥: {e}")

# è¯†åˆ«ç”¨æˆ·ç±»å‹
print("\nğŸ‘¥ è¯†åˆ«ç”¨æˆ·ç±»å‹...")
try:
    df, user_type_map = identify_user_types(df)
    
    print("\nğŸ‘¥ ç”¨æˆ·ç±»å‹è¯†åˆ«ç»“æœ:")
    print(f"  ç±»å‹æ˜ å°„: {user_type_map}")
    print(f"\n  å„ç±»å‹æ•°é‡:")
    type_counts = df['user_type'].value_counts()
    for user_type, count in type_counts.items():
        ratio = count / len(df) * 100
        print(f"    {user_type}: {count} ({ratio:.1f}%)")
except Exception as e:
    print(f"  âŒ ç”¨æˆ·ç±»å‹è¯†åˆ«å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

print("\nâœ… è¯Šæ–­åˆ†æå®Œæˆï¼")

# æ·»åŠ å¯è§†åŒ–
print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. èšç±»æ•£ç‚¹å›¾ï¼ˆPCAé™ç»´ï¼‰
    ax1 = axes[0, 0]
    feature_cols = ['is_academic_career', 'is_emotional', 'is_entertainment', 
                   'log_interaction', 'interaction_diversity', 'engagement_level',
                   'comfort_score', 'deep_score']
    X = df[feature_cols].fillna(0)
    
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(StandardScaler().fit_transform(X))
    
    # æŒ‰ç”¨æˆ·ç±»å‹ç€è‰²
    type_colors = {'å¿ƒç†æ…°è—‰å‹': '#FF6B6B', 'å¨±ä¹å‹': '#4ECDC4', 'æ·±åº¦å‚ä¸å‹': '#45B7D1'}
    colors = df['user_type'].map(type_colors).fillna('#999999')
    
    scatter = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.6, s=50)
    ax1.set_xlabel('ä¸»æˆåˆ†1', fontsize=12)
    ax1.set_ylabel('ä¸»æˆåˆ†2', fontsize=12)
    ax1.set_title('ç”¨æˆ·èšç±»ç»“æœï¼ˆPCAé™ç»´ï¼‰', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ å›¾ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor=color, label=label) 
                      for label, color in type_colors.items() 
                      if label in df['user_type'].values]
    if legend_elements:
        ax1.legend(handles=legend_elements, loc='best')
    
    # 2. ç”¨æˆ·ç±»å‹åˆ†å¸ƒ
    ax2 = axes[0, 1]
    user_type_counts = df['user_type'].value_counts()
    colors_list = [type_colors.get(ut, '#999999') for ut in user_type_counts.index]
    bars = ax2.bar(user_type_counts.index, user_type_counts.values, color=colors_list)
    ax2.set_xlabel('ç”¨æˆ·ç±»å‹', fontsize=12)
    ax2.set_ylabel('æ•°é‡', fontsize=12)
    ax2.set_title('ä¸‰ç±»ç”¨æˆ·ç¾¤ä½“åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax2.tick_params(axis='x', rotation=15)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}\n({height/len(df)*100:.1f}%)',
                ha='center', va='bottom', fontsize=10)
    
    # 3. å„ç±»å‹ç‰¹å¾å¯¹æ¯”
    ax3 = axes[1, 0]
    type_features = df.groupby('user_type').agg({
        'is_academic_career': 'mean',
        'is_emotional': 'mean',
        'is_entertainment': 'mean',
        'log_interaction': 'mean',
        'engagement_level': 'mean'
    }).T
    
    x = np.arange(len(type_features.index))
    width = 0.25
    for i, user_type in enumerate(type_features.columns):
        offset = (i - len(type_features.columns)/2 + 0.5) * width
        ax3.bar(x + offset, type_features[user_type], width, 
               label=user_type, alpha=0.8, color=type_colors.get(user_type, '#999999'))
    
    ax3.set_xlabel('ç‰¹å¾', fontsize=12)
    ax3.set_ylabel('å¹³å‡å€¼', fontsize=12)
    ax3.set_title('å„ç”¨æˆ·ç±»å‹ç‰¹å¾å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax3.set_xticks(x)
    ax3.set_xticklabels(type_features.index, rotation=45, ha='right')
    ax3.legend()
    ax3.grid(True, alpha=0.3, axis='y')
    
    # 4. äº’åŠ¨è¡Œä¸ºå¯¹æ¯”
    ax4 = axes[1, 1]
    interaction_by_type = df.groupby('user_type').agg({
        'interaction_score': 'mean',
        'reposts_count': 'mean',
        'comments_count': 'mean',
        'attitudes_count': 'mean'
    })
    
    x = np.arange(len(interaction_by_type.index))
    width = 0.2
    metrics = ['interaction_score', 'reposts_count', 'comments_count', 'attitudes_count']
    metric_labels = ['äº’åŠ¨æ€»åˆ†', 'è½¬å‘', 'è¯„è®º', 'ç‚¹èµ']
    
    for i, (metric, label) in enumerate(zip(metrics, metric_labels)):
        offset = (i - len(metrics)/2 + 0.5) * width
        values = interaction_by_type[metric].values
        # å½’ä¸€åŒ–æ˜¾ç¤º
        if metric == 'interaction_score':
            values_norm = values / (values.max() + 1) * 100
        else:
            values_norm = values / (values.max() + 1) * 100
        ax4.bar(x + offset, values_norm, width, label=label, alpha=0.8)
    
    ax4.set_xlabel('ç”¨æˆ·ç±»å‹', fontsize=12)
    ax4.set_ylabel('å½’ä¸€åŒ–å€¼', fontsize=12)
    ax4.set_title('å„ç±»å‹ç”¨æˆ·äº’åŠ¨è¡Œä¸ºå¯¹æ¯”', fontsize=14, fontweight='bold')
    ax4.set_xticks(x)
    ax4.set_xticklabels(interaction_by_type.index, rotation=15)
    ax4.legend()
    ax4.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    save_path = "weibo_clustering_diagnosis.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ å·²ä¿å­˜å¯è§†åŒ–ç»“æœ: {save_path}")
    plt.show()
    
except Exception as e:
    print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()



